{"pages":[{"title":"ABOUT","date":"2023-10-04T06:07:04.840Z","path":"about/index.html","text":"👩‍💻: A CODER WITH GREEN HANDS 💡: THINK TWICE, CODE ONCE! 🎯: GOTO NIPS, MSRA AND BE REASSURED 💌: BEST WISHES TO LEE XIAOMAO 12345// life mottoif (sad() == true) &#123; sad().stop(); beAwesome();&#125;"},{"title":"TAGS","date":"2023-07-13T06:48:03.646Z","path":"tags/index.html","text":""},{"title":"CATEGORIES","date":"2023-07-13T06:47:46.496Z","path":"categories/index.html","text":""}],"posts":[{"title":"","date":"2024-05-03T03:48:39.342Z","path":"jottings/interview/memo_cpp_notes/","text":"","tags":[],"categories":[{"name":"interview","slug":"interview","permalink":"https://stu-yue.github.io/categories/interview/"}]},{"title":"","date":"2024-05-03T03:48:39.332Z","path":"jottings/interview/Hot100/表达式求和/","text":"","tags":[],"categories":[{"name":"interview","slug":"interview","permalink":"https://stu-yue.github.io/categories/interview/"},{"name":"Hot100","slug":"interview/Hot100","permalink":"https://stu-yue.github.io/categories/interview/Hot100/"}]},{"title":"Memo | Law of Large Number","date":"2023-10-21T05:56:53.000Z","path":"jottings/mathematics/memo_law_of_probability/","text":"1 Law of Large Number1.1 Folk UnderstandingIn simple terms, the large numbers theorem refers to that a random event may or may not occur in a single experiment, but in a large number of repeated experiments, it often shows obvious regularity, that is, the frequency of the random event will converge to a constant value, which is the probability of the event. Another way to express it is that when the sample data is infinite, the sample mean tends to population mean. Because in real life, we can not run an infinite number of experiments, and it is difficult to estimate the parameters of the population. The law of large numbers connects mean values, which belong to mathematical statistics, with expectations, which belong to probability theory. 1.2 Convergence in Probability Weak Law: convergence in probability Strong Law: almost sure convergence (outlier can be negligible in measure) 1.3 Bernoulli’s LawFrom the perspective of defining probability, reveals the relationship between probability and frequency.$$\\lim\\limits_{n\\rightarrow \\infin} P{ |\\frac{f_A}{n} - p| &lt; \\epsilon } &#x3D; 1$$ 1.4 Khinchin’s LawPriori Condition: Independent Identically Distributed, $\\lim\\limits_{n\\rightarrow \\infin} P{ |\\frac{1}{n}\\sum\\limits_{i&#x3D;1}^nX_i - \\frac{1}{n}\\sum\\limits_{i&#x3D;1}^{n}E(X_i)| &lt; \\epsilon } &#x3D; 1$ 1.5 Chebyshev’s LawPriori Condition: Independent Distributed, $\\lim\\limits_{n\\rightarrow \\infin} P{ |\\frac{1}{n}\\sum\\limits_{i&#x3D;1}^nX_i - \\frac{1}{n}\\sum\\limits_{i&#x3D;1}^{n}\\mu_i| &lt; \\epsilon } &#x3D; 1$ 2 Heavy-tailed Distribution Ref: 1, 2 Pareto Distribution:$$P(X&gt;x) &#x3D; \\left{\\begin{align}(\\frac{x_{min}}{x})^\\alpha,&amp;\\quad x\\ge x_{min} \\1, &amp;\\quad x&lt;x_{min}\\end{align}\\right.$$ $$f(x) &#x3D; \\left{\\begin{align}\\frac{1}{x^\\alpha}\\cdot\\frac{\\alpha x_{min}^\\alpha}{x},&amp;\\quad x\\ge x_{min} \\0, &amp;\\quad x&lt;x_{min}\\end{align}\\right.$$ Pareto Principle: states that for many outcomes, roughly 80% of consequences com from 20% of causes (the “vital few”). Other names —— 80&#x2F;20 rule, the law of the vital few (states the imbalance phenomenon); Mathematically, the 80&#x2F;20 rule is roughly described by a power law distribution (also known as a Pareto distribution) for a particular set of parameters. Zipf Distribution:$$f(x) &#x3D; \\frac{1}{x^\\alpha\\sum_{i&#x3D;1}^{n}(1&#x2F;i)^\\alpha}, \\ x &#x3D; 1,2,\\cdots,n$$Zipf’s law states that the value of the nth entry is inversely proportional to n, when a list of measured values is sorted in decreasing order. Zeta Distribution: when $n\\rightarrow \\infty$, $\\text{Zipf}(\\alpha, n)\\rightarrow\\text{Zeta}(\\alpha)$ ; Zeta is regraded as a type of pareto distribution in the discrete distribution.$$f(x) &#x3D; \\frac{1}{x^\\alpha\\sum_{i&#x3D;1}^{\\infty}(1&#x2F;i)^\\alpha}, \\ x &#x3D; 1,2,\\cdots,n,\\text{and}\\ \\alpha &gt; 1$$","tags":[{"name":"math","slug":"math","permalink":"https://stu-yue.github.io/tags/math/"}],"categories":[{"name":"mathematics","slug":"mathematics","permalink":"https://stu-yue.github.io/categories/mathematics/"}]},{"title":"Memo | CTC Introduction","date":"2023-10-17T07:56:53.000Z","path":"jottings/statistics_ml/memo_ctc/","text":"1 Problem DescriptionIn seq2seq structure, given input sequence $X&#x3D;[x_1,\\cdots,x_T]$ with corresponding label $Y &#x3D; [y_1,\\cdots,y_N]$, such as speech recognition. Our job is to find a map, and this algorithm for classifying time series data is called Temporal Classification. Compared with traditional classification, temporal classification has the following difficulties: The lengths of $X$ and $Y$ are variable. The lengths of $X$ and $Y$ are not equal. For an end-to-end model, we don’t want manual design the alignment between $X$ and $Y$. The CTC provides the solution, that for a given input sequence $X$, CTC gives the output distribution of all possible $Y$. Based on this distribution, we can output the most likely outcome or give the probability of a certain output. Loss Function: Given the input sequence $X$, we want to maximize the posterior probability $P(Y|X)$ of $Y$, and $P(Y|X)$ should be derivable so that we can perform the gradient-descent algorithm; Test: Given a trained model and input sequence $X$, we want to output $Y$ with the highest probability:$$Y^* &#x3D; \\arg\\max_YP(Y|X)$$Of course, when testing, we want Y to be searched as soon as possible (greedy, beam, prefix-beam, LM). CTC Procedure REF: https://zhuanlan.zhihu.com/p/42719047 CTC Traits Conditional independence: A very unreasonable assumption of the CTC is its assumption that each time slice is independent of each other, which is a very bad assumption. In OCR or speech recognition, there is some semantic information between each time slice, so the effect should be improved if the language model can be added to the CTC. Monotonic alignment: Another constraint of CTC is the monotonic alignment between input $X$ and output $Y$, which holds true in OCR and speech recognition. However, in some scenarios, such as machine translation, this constraint is not valid. Many-to-one mapping: Another constraint of CTC is that the length of the input sequence $X$ is greater than the length of the label data $Y$, but for scenarios where the length of $Y$ is greater than the length of $X$, CTC is invalid.","tags":[{"name":"ml","slug":"ml","permalink":"https://stu-yue.github.io/tags/ml/"}],"categories":[{"name":"statistics_ml","slug":"statistics-ml","permalink":"https://stu-yue.github.io/categories/statistics-ml/"}]},{"title":"Memo | LM and Word Representation","date":"2023-10-17T07:56:53.000Z","path":"jottings/statistics_ml/memo_lm_and_word_vector/","text":"Language Model 语言模型是衡量一句话出现在自然语言中的概率的模型； 数学形式上，给定一句话 $s &#x3D; { w_1,\\cdots,w_n }$，它对应的概率为：$$\\begin{align*}P(s) &amp;&#x3D; P(w_1,\\cdots,w_n) \\&amp;&#x3D; P(w_1)\\times P(w_2|w_1) \\times \\cdots \\times P(w_n|w_1,\\cdots,w_{n-1})\\&amp;&#x3D; \\prod\\limits_{i&#x3D;1}^{n}P(w_i|w_1,\\cdots,w_{i-1})\\end{align*}$$ 语言模型的核心在于根据前文预测下一个词出现的概率； $P(w_i|w_1,\\cdots,w_{i-1}),\\ w_i \\in V,\\ V&#x3D;{ w_1,\\cdots,w_{|V|} }$ 马尔可夫假设 (Markov Assumption)：当前词出现的概率只和它前面的k个词相关；$$\\begin{align*}P(w_i | w_1,\\cdots,w_{i-1}) &amp;&#x3D; P(w_i | w_{i-k},\\cdots,w_{i-1}) \\&amp; &#x3D; P(w_i) \\qquad\\qquad\\qquad \\rightarrow\\quad\\text{k&#x3D;0, Unigram Model} \\&amp; &#x3D; P(w_i | w_{i-1}) \\qquad\\qquad \\rightarrow\\quad\\text{k&#x3D;1, Bigram Model} \\&amp; &#x3D; P(w_i | w_{i-2}, w_{i-1})\\qquad\\rightarrow\\quad\\text{k&#x3D;2, Trigram Model}\\\\end{align*}$$ 用频率估计概率（大数定理——伯努利）$$\\begin{align*}P(w_i | w_1,\\cdots,w_{i-1}) &amp;&#x3D; P(w_i | w_{i-k},\\cdots,w_{i-1}) \\&amp; &#x3D; \\frac{P(w_{i-k},\\cdots,w_{i-1},w_i)}{P(w_{i-k},\\cdots,w_{i-1})} \\&amp; \\approx \\frac{\\text{count}(w_{i-k},\\cdots,w_{i-1},w_i)}{\\text{count}(w_{i-k},\\cdots,w_{i-1})} \\\\end{align*}$$ Zipf Law, also known as the rank-size rule or Zipf distribution, is an empirical observation about the frequency distribution of words or other items in a given corpus of natural language. It states that the frequency of any word is inversely proportional to its rank in the frequency table. 隐藏信息，排位靠后的词的频率非常低，甚至未出现在语料中； 数据稀疏，对于未出现在语料中的词或n-gram，无法估计其概率； 平滑技术 （拉普拉斯平滑、古德-图灵平滑、插值平滑、Katz平滑） $$\\begin{align*}P(w_i | w_1,\\cdots,w_{i-1}) &amp;&#x3D; P(w_i | w_{i-k},\\cdots,w_{i-1}) \\&amp; &#x3D; \\frac{P(w_{i-k},\\cdots,w_{i-1},w_i)}{P(w_{i-k},\\cdots,w_{i-1})} \\&amp; \\approx \\frac{\\text{count}(w_{i-k},\\cdots,w_{i-1},w_i)+1}{\\text{count}(w_{i-k},\\cdots,w_{i-1})+|V|} \\\\end{align*}$$ 回退策略$$\\begin{align*}P(w_i | w_1,\\cdots,w_{i-1}) &amp;&#x3D; P(w_i | w_{i-k},\\cdots,w_{i-1}) \\&amp; &#x3D; \\frac{P(w_{i-k},\\cdots,w_{i-1},w_i)}{P(w_{i-k},\\cdots,w_{i-1})} \\&amp; \\approx \\frac{\\text{count}(w_{i-k},\\cdots,w_{i-1},w_i)}{\\text{count}(w_{i-k},\\cdots,w_{i-1})}\\qquad \\rightarrow\\quad\\text{students opened their} \\&amp; \\approx \\frac{\\text{count}(w_{i-k+j},\\cdots,w_{i-1},w_i)}{\\text{count}(w_{i-k+j},\\cdots,w_{i-1})}\\quad \\rightarrow\\quad\\text{opened their} \\\\end{align*}$$ 参数规模问题：随着k的增大，参数数目呈指数增长，无法存储； k&#x3D;1，参数量&#x3D;$|V|^2$；k&#x3D;2，参数量&#x3D;$|V|^3$；k&#x3D;n-1，参数量&#x3D;$|V|^n$； 困惑度（Perplexity） 用来衡量一个概率分布或概率模型预测样本的好坏程度； 可以用来比较两个概率模型，低困惑度的概率模型能更好地预测样本；$$\\text{Perplexity}(s) &#x3D; 2^{H(s)} &#x3D; \\sqrt[n]{1&#x2F;P(w_1,\\cdots,w_n)}$$ Word Representation 词库： WordNet：一个包含同义词（ synonym ）和上位词（ hypernyms ）的知识库； 词库的问题： 缺少差异性 (proficient也被视为good的同义词)，不够精确； 缺少新词，无法及时更新 主观性，人工标注； 离散词表示： One-hot表示： 123456单词表示 motel = [0 0 0 0 0 1 0] hotel = [0 0 0 1 0 0 0]文本表示 The students opened their books [0 0 1 1 0 0 1 0] ⬆ 次数、频率、逆文档频率、TF-IDF、... 词袋模型（Bag of Word）：词袋模型用于文本表示，如果每个词为One-hot表示，那么把每个词的One-hot向量相加，得到的向量就是该文本基于BOW得到的表示； 词频（Term Frequency，TF）：在文档中出现频率越高的词对当前文档可能越重要；$$f_{ij} &#x3D; \\frac{\\text{count}(\\text{term}\\ i)\\text{in doc} \\ j}{\\text{count}(\\text{all term})\\text{in doc} \\ j}, \\tf_{ij} &#x3D; \\frac{f_{ij}}{\\max_k(f_{kj})}$$ 逆文档频率（Inverse Document Frequency，IDF）：在很多文档中都出现的词可能不重要（如虚词）；$$df_i &#x3D; \\text{doc frequency of term}\\ i &#x3D; \\text{numbers of doc containing term} \\ i, \\idf_i &#x3D; \\log_2\\frac{N}{df_i} \\ \\text{（N为文档总数）}$$ TF-IDF：综合一个词在当前文档中的频率和所有文档中出现的次数来度量这个词对当前文档重要性；$$tf_{ij}-idf_i &#x3D; tf_{ij}*idf_i &#x3D; tf_{ij}*\\log_2\\frac{N}{df_i}$$ N-gram：N元组提取局部的上下文你信息； 离散词表示问题：语义鸿沟、维度爆炸； 分布式词表示（词嵌入） 用一个低维稠密的向量表示单词的整体含义； 核心思想：一个词的含义能被该词所在的上下文反映； Co-occurrence： 基于窗口的共现矩阵： 统计窗口内单词之间的共现信息； 类似于word2vec； 能够捕获一些句法和语义信息（局部信息）； 基于文档的共现矩阵： 统计文档和单词之间的共现信息； Latent Semantics Analysis (LSA)； 能够捕获话题信息（全局信息）； Word2vec[Mikolov et al. 2013] 是一套学习词向量的算法框架 算法思想：大量的自然语言文本（训练语料） 为词表中的每个词随机初始化一个向量表示 遍历文本中的每个单词 $c$，其上下文单词为 $o$ 使用单词 $c$ 的上下文 $o$ 预测单词 $c$ 的概率分布（核心思想） 更新词向量的表示使得单词 $c$ 的预测概率最大化 连续词袋模型（CBOW，Continuous Bag of Words） 目标：通过局部语言模型的优化，获得词向量 优化目标：固定上下文词向量表示，计算中心词的似然函数，最大化其似然（负对数） 计算优化：负采样（Negative Sampling） 为避免softmax计算整个庞大的词表，通常采用负采样的方法，将多分类问题转换为二分类问题；大大减少了训练时间和计算成本； Skip-gram：中心词 $c$ 预测上下文 $o$； 优点 缺点 代表方法 共现矩阵法 速度快，有效利用统计数据 过分依赖单词共现性和数据量 LSA, HAL 直接学习法 能够捕获语法和语义信息 速度和数据规模相关，未有效利用统计数据 Skip-gram, CBOW 基于计数的和基于预测的都探究了语料库的潜在共现统计 GloVe：集两家之长 共现概率矩阵$X_{ij}$； 单词 $w_i$，$w_j$ 的词向量 $v_i$，$v_j$； 以学习的方式，用词向量之间的语义关系来拟合共现概率矩阵；$$J &#x3D; \\sum\\limits_{i,j&#x3D;1}^{|V|}f(X_{ij})(v_i^Tv_j+b_i+b_j-\\log X_{ij})^2 \\v_i^Tv_j \\quad\\rightarrow\\quad\\text{局部信息} \\\\log X_{ij}\\quad\\rightarrow\\quad\\text{全局统计信息} \\$$ 训练快，适应于大规模数据，在小规模数据上性能优秀；","tags":[{"name":"ml","slug":"ml","permalink":"https://stu-yue.github.io/tags/ml/"}],"categories":[{"name":"statistics_ml","slug":"statistics-ml","permalink":"https://stu-yue.github.io/categories/statistics-ml/"}]},{"title":"Memo | ISA and Micro-architecture","date":"2023-10-14T10:21:53.000Z","path":"jottings/architecture/memo_isa_and_micro_architecture/","text":"What is CPU? CPUs are a general purpose, flexible architecture that take in a stream of instructions from all types of workloads, and compute or process information based on those instructions. Simply put, CPUs do what we tell them or program them to do. This ability to continue shrinking transistors is based on a famous law&#x2F;observation that we in the industry refer to as Moore’s Law, that is, we can double the number of transistors per unit area about every two year. Bug Aside: Operators traced an error on the computers to a moth trapped in a relay, recoining the term “bug”. CPU Architecture: **ENIAC: ** In early period, computer programs are hardware-based. Computers with data in memory and programs embedded in the hardware are computationally inefficient and less flexible. Von Neumann Machine: Programs are encoded as data and stored in memory (Principle of Stored Program Control). Harvard Machine: A memory structure that separates program instruction storage from data storage. CPU can access instructions and read&#x2F;write data at the same time. Use two independent memory modules to store instructions and data respectively, and each storage module does not allow instructions and data to coexist; Use two independent buses as a dedicated communication path between the CPU and each memory, and these two buses are unrelated. In fact, the vast majority of modern computers use “Modified Harvard Architecture,” where instructions and data share the same address space, but the cache is separate. As it stands, von Neumann for large-scale processing, and Harvard for small-scale processing. CPU workflow architecture: Instruction Set ArchitectureThe ISA is the dictionary of instructions, data types, and the formats that the CPU adhering to that ISA must execute. The ISA is used as a design spec (specification) that tells the engineer what operations it needs to execute. Because of this layer of abstraction, the instructions in the ISA are implementation independent. Micro-architecture is the concrete implementation of ISA in the hardware. CISC (Complex Instruction Set Computers): Early CPUs all used CISC, which was designed to perform the required computational tasks with minimal machine language instructions. In order to achieve complex operations, microprocessors provide programmers with functions similar to various registers and machine instructions, but also through microprograms stored in read-only memory (ROM) to achieve its extremely powerful functions. RISC (Reduced Instruction Set Computers): In CISC, many complex instructions require extremely complex operations, and most of these instructions are direct copies of some high-level language, so the universality is poor. Because of the secondary microcode execution, it also slows down the operation of simple instruction systems that are frequently invoked. Summary: The complex instructions are converted into a microprogram, which is stored in the microservice memory when the CPU is manufactured. A microprogram contains several microinstructions (also known as microcode), and when executing complex instructions, it is actually executing a microprogram. This also brings a difference between the two instruction sets, the execution of microprograms cannot be interrupted, while RISC instructions can be interrupted between each other, so in theory RISC can respond faster to interrupts. Command Capability: The instruction capability of CISC is strong, but the usage rate of most instructions is low, which increases the complexity of CPU. Instructions are variable length format, which must be divided into different length instructions, so more processing work is needed when executing a single instruction. Most RISC instructions are single-cycle instructions, the length of instructions is fixed, and the CPU is fast and stable when executing instructions. Addressing Mode: CISC supports a variety of addressing methods. RISC supports few addressing methods. Implementation Mode: CISC is implemented through microprogrammed control technology (microcode). RISC adds a general register, hard-wired logic control is the main, suitable for pipelined execution. RISC can optimize compilation and effectively support high-level languages. R&amp;D Cycle: CISC has a long development cycle. RISC hardware is simple, so its manufacturing process is simple and low cost.","tags":[{"name":"instruction set arch","slug":"instruction-set-arch","permalink":"https://stu-yue.github.io/tags/instruction-set-arch/"}],"categories":[{"name":"architecture","slug":"architecture","permalink":"https://stu-yue.github.io/categories/architecture/"}]},{"title":"Memo | Installation","date":"2023-10-04T07:56:53.000Z","path":"jottings/tools/docker/memo_install/","text":"Docker 分为 stable test 和 nightly 三个更新频道。 官方网站上有各种环境下的 安装指南，这里主要介绍 Docker 在 Linux 、Windows 10 和 macOS 上的安装。 Ubuntu 警告：切勿在没有配置 Docker APT 源的情况下直接使用 apt 命令安装 Docker. 准备工作系统要求Docker 支持以下版本的 Ubuntu 操作系统： Ubuntu Hirsute 21.04 Ubuntu Groovy 20.10 Ubuntu Focal 20.04 (LTS) Ubuntu Bionic 18.04 (LTS) Docker 可以安装在 64 位的 x86 平台或 ARM 平台上。Ubuntu 发行版中，LTS（Long-Term-Support）长期支持版本，会获得 5 年的升级维护支持，这样的版本会更稳定，因此在生产环境中推荐使用 LTS 版本。 卸载旧版本旧版本的 Docker 称为 docker 或者 docker-engine，使用以下命令卸载旧版本： 1sudo apt-get remove docker docker-engine docker.io 使用 APT 安装由于 apt 源使用 HTTPS 以确保软件下载过程中不被篡改。因此，我们首先需要添加使用 HTTPS 传输的软件包以及 CA 证书。 1234sudo apt-get updatesudo apt-get install apt-transport-https \\ ca-certificates curl gnupg lsb-release 鉴于国内网络问题，强烈建议使用国内源，官方源请在注释中查看。 为了确认所下载软件包的合法性，需要添加软件源的 GPG 密钥。 12345curl -fsSL https://mirrors.aliyun.com/docker-ce/linux/ubuntu/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg# 官方源# curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg 然后，我们需要向 sources.list 中添加 Docker 软件源 12345echo &quot;deb [arch=amd64 signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://mirrors.aliyun.com/docker-ce/linux/ubuntu $(lsb_release -cs) stable&quot; | sudo tee /etc/apt/sources.list.d/docker.list &gt; /dev/null# 官方源# echo &quot;deb [arch=amd64 signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable&quot; | sudo tee /etc/apt/sources.list.d/docker.list &gt; /dev/null 以上命令会添加稳定版本的 Docker APT 镜像源，如果需要测试版本的 Docker 请将 stable 改为 test。 安装 Docker更新 apt 软件包缓存，并安装 docker-ce： 123sudo apt-get updatesudo apt-get install docker-ce docker-ce-cli containerd.io 使用脚本自动安装在测试或开发环境中 Docker 官方为了简化安装流程，提供了一套便捷的安装脚本，Ubuntu 系统上可以使用这套脚本安装，另外可以通过 --mirror 选项使用国内源进行安装： 若你想安装测试版的 Docker, 请从 test.docker.com 获取脚本 1234# curl -fsSL test.docker.com -o get-docker.shcurl -fsSL get.docker.com -o get-docker.shsudo sh get-docker.sh --mirror Aliyun# sudo sh get-docker.sh --mirror AzureChinaCloud 执行这个命令后，脚本就会自动的将一切准备工作做好，并且把 Docker 的稳定(stable)版本安装在系统中。 启动 Docker12sudo systemctl enable dockersudo systemctl start docker 建立 docker 用户组默认情况下，docker 命令会使用 Unix socket 与 Docker 引擎通讯。而只有 root 用户和 docker 组的用户才可以访问 Docker 引擎的 Unix socket。出于安全考虑，一般 Linux 系统上不会直接使用 root 用户。因此，更好地做法是将需要使用 docker 的用户加入 docker 用户组。 建立 docker 组： 1sudo groupadd docker 将当前用户加入 docker 组： 1sudo usermod -aG docker $USER 退出当前终端并重新登录，进行如下测试。 测试 Docker 是否安装正确12345678910111213141516171819202122232425262728docker run --rm hello-worldUnable to find image &#x27;hello-world:latest&#x27; locallylatest: Pulling from library/hello-worldb8dfde127a29: Pull completeDigest: sha256:308866a43596e83578c7dfa15e27a73011bdd402185a84c5cd7f32a88b501a24Status: Downloaded newer image for hello-world:latestHello from Docker!This message shows that your installation appears to be working correctly.To generate this message, Docker took the following steps: 1. The Docker client contacted the Docker daemon. 2. The Docker daemon pulled the &quot;hello-world&quot; image from the Docker Hub. (amd64) 3. The Docker daemon created a new container from that image which runs the executable that produces the output you are currently reading. 4. The Docker daemon streamed that output to the Docker client, which sent it to your terminal.To try something more ambitious, you can run an Ubuntu container with: $ docker run -it ubuntu bashShare images, automate workflows, and more with a free Docker ID: https://hub.docker.com/For more examples and ideas, visit: https://docs.docker.com/get-started/ 若能正常输出以上信息，则说明安装成功。 镜像加速如果在使用过程中发现拉取 Docker 镜像十分缓慢，可以配置 Docker 国内镜像加速。","tags":[{"name":"docker","slug":"docker","permalink":"https://stu-yue.github.io/tags/docker/"},{"name":"linux","slug":"linux","permalink":"https://stu-yue.github.io/tags/linux/"}],"categories":[{"name":"tools","slug":"tools","permalink":"https://stu-yue.github.io/categories/tools/"},{"name":"docker","slug":"tools/docker","permalink":"https://stu-yue.github.io/categories/tools/docker/"}]},{"title":"Memo | Network","date":"2023-10-04T07:56:53.000Z","path":"jottings/tools/docker/memo_network/","text":"1 外部访问容器容器中可以运行一些网络应用，要让外部也可以访问这些应用，可以通过 -P 或 -p 参数来指定端口映射。 当使用 -P 标记时，Docker 会随机映射一个端口到内部容器开放的网络端口。 使用 docker container ls 可以看到，本地主机的 32768 被映射到了容器的 80 端口。此时访问本机的 32768 端口即可访问容器内 NGINX 默认页面。 12345$ docker run -d -P nginx:alpine$ docker container ls -lCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESfae320d08268 nginx:alpine &quot;/docker-entrypoint.…&quot; 24 seconds ago Up 20 seconds 0.0.0.0:32768-&gt;80/tcp bold_mcnulty 同样的，可以通过 docker logs 命令来查看访问记录。 12$ docker logs fa172.17.0.1 - - [25/Aug/2020:08:34:04 +0000] &quot;GET / HTTP/1.1&quot; 200 612 &quot;-&quot; &quot;Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:80.0) Gecko/20100101 Firefox/80.0&quot; &quot;-&quot; -p 则可以指定要映射的端口，并且，在一个指定端口上只可以绑定一个容器。支持的格式有 ip:hostPort:containerPort | ip::containerPort | hostPort:containerPort。 映射所有接口地址使用 hostPort:containerPort 格式本地的 80 端口映射到容器的 80 端口，可以执行 1$ docker run -d -p 80:80 nginx:alpine 此时默认会绑定本地所有接口上的所有地址。 映射到指定地址的指定端口可以使用 ip:hostPort:containerPort 格式指定映射使用一个特定地址，比如 localhost 地址 127.0.0.1 1$ docker run -d -p 127.0.0.1:80:80 nginx:alpine 映射到指定地址的任意端口使用 ip::containerPort 绑定 localhost 的任意端口到容器的 80 端口，本地主机会自动分配一个端口。 1$ docker run -d -p 127.0.0.1::80 nginx:alpine 还可以使用 udp 标记来指定 udp 端口 1$ docker run -d -p 127.0.0.1:80:80/udp nginx:alpine 查看映射端口配置使用 docker port 来查看当前映射的端口配置，也可以查看到绑定的地址 12$ docker port fa 800.0.0.0:32768 注意： 容器有自己的内部网络和 ip 地址（使用 docker inspect 查看，Docker 还可以有一个可变的网络配置。） -p 标记可以多次使用来绑定多个端口 例如 1234$ docker run -d \\ -p 80:80 \\ -p 443:443 \\ nginx:alpine 2 容器互联如果你之前有 Docker 使用经验，你可能已经习惯了使用 --link 参数来使容器互联。 随着 Docker 网络的完善，强烈建议大家将容器加入自定义的 Docker 网络来连接多个容器，而不是使用 --link 参数。 新建网络下面先创建一个新的 Docker 网络。 1$ docker network create -d bridge my-net -d 参数指定 Docker 网络类型，有 bridge overlay。其中 overlay 网络类型用于 Swarm mode，在本小节中你可以忽略它。 连接容器运行一个容器并连接到新建的 my-net 网络 1$ docker run -it --rm --name busybox1 --network my-net busybox sh 打开新的终端，再运行一个容器并加入到 my-net 网络 1$ docker run -it --rm --name busybox2 --network my-net busybox sh 再打开一个新的终端查看容器信息 12345$ docker container lsCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESb47060aca56b busybox &quot;sh&quot; 11 minutes ago Up 11 minutes busybox28720575823ec busybox &quot;sh&quot; 16 minutes ago Up 16 minutes busybox1 下面通过 ping 来证明 busybox1 容器和 busybox2 容器建立了互联关系。 在 busybox1 容器输入以下命令 1234/ # ping busybox2PING busybox2 (172.19.0.3): 56 data bytes64 bytes from 172.19.0.3: seq=0 ttl=64 time=0.072 ms64 bytes from 172.19.0.3: seq=1 ttl=64 time=0.118 ms 用 ping 来测试连接 busybox2 容器，它会解析成 172.19.0.3。 同理在 busybox2 容器执行 ping busybox1，也会成功连接到。 1234/ # ping busybox1PING busybox1 (172.19.0.2): 56 data bytes64 bytes from 172.19.0.2: seq=0 ttl=64 time=0.064 ms64 bytes from 172.19.0.2: seq=1 ttl=64 time=0.143 ms 这样，busybox1 容器和 busybox2 容器建立了互联关系。 3 配置 DNS如何自定义配置容器的主机名和 DNS 呢？秘诀就是 Docker 利用虚拟文件来挂载容器的 3 个相关配置文件。 在容器中使用 mount 命令可以看到挂载信息： 1234$ mount/dev/disk/by-uuid/1fec...ebdf on /etc/hostname type ext4 .../dev/disk/by-uuid/1fec...ebdf on /etc/hosts type ext4 ...tmpfs on /etc/resolv.conf type tmpfs ... 这种机制可以让宿主主机 DNS 信息发生更新后，所有 Docker 容器的 DNS 配置通过 /etc/resolv.conf 文件立刻得到更新。 配置全部容器的 DNS ，也可以在 /etc/docker/daemon.json 文件中增加以下内容来设置。 123456&#123; &quot;dns&quot; : [ &quot;114.114.114.114&quot;, &quot;8.8.8.8&quot; ]&#125; 这样每次启动的容器 DNS 自动配置为 114.114.114.114 和 8.8.8.8。使用以下命令来证明其已经生效。 1234$ docker run -it --rm ubuntu:18.04 cat etc/resolv.confnameserver 114.114.114.114nameserver 8.8.8.8 如果用户想要手动指定容器的配置，可以在使用 docker run 命令启动容器时加入如下参数： -h HOSTNAME 或者 --hostname=HOSTNAME 设定容器的主机名，它会被写到容器内的 /etc/hostname 和 /etc/hosts。但它在容器外部看不到，既不会在 docker container ls 中显示，也不会在其他的容器的 /etc/hosts 看到。 --dns=IP_ADDRESS 添加 DNS 服务器到容器的 /etc/resolv.conf 中，让容器用这个服务器来解析所有不在 /etc/hosts 中的主机名。 --dns-search=DOMAIN 设定容器的搜索域，当设定搜索域为 .example.com 时，在搜索一个名为 host 的主机时，DNS 不仅搜索 host，还会搜索 host.example.com。 注意：如果在容器启动时没有指定最后两个参数，Docker 会默认用主机上的 /etc/resolv.conf 来配置容器。","tags":[{"name":"docker","slug":"docker","permalink":"https://stu-yue.github.io/tags/docker/"},{"name":"linux","slug":"linux","permalink":"https://stu-yue.github.io/tags/linux/"}],"categories":[{"name":"tools","slug":"tools","permalink":"https://stu-yue.github.io/categories/tools/"},{"name":"docker","slug":"tools/docker","permalink":"https://stu-yue.github.io/categories/tools/docker/"}]},{"title":"Memo | Operation","date":"2023-10-04T07:56:53.000Z","path":"jottings/tools/docker/memo_operation/","text":"1 启动启动容器有两种方式，一种是基于镜像新建一个容器并启动，另外一个是将在终止状态（exited）的容器重新启动。 因为 Docker 的容器实在太轻量级了，很多时候用户都是随时删除和新创建容器。 新建并启动所需要的命令主要为 docker run。 例如，下面的命令输出一个 “Hello World”，之后终止容器。 12$ docker run ubuntu:18.04 /bin/echo &#x27;Hello world&#x27;Hello world 这跟在本地直接执行 /bin/echo &#39;hello world&#39; 几乎感觉不出任何区别。 下面的命令则启动一个 bash 终端，允许用户进行交互。 12$ docker run -t -i ubuntu:18.04 /bin/bashroot@af8bae53bdd3:/# 其中，-t 选项让Docker分配一个伪终端（pseudo-tty）并绑定到容器的标准输入上， -i 则让容器的标准输入保持打开。 在交互模式下，用户可以通过所创建的终端来输入命令，例如 1234root@af8bae53bdd3:/# pwd/root@af8bae53bdd3:/# lsbin boot dev etc home lib lib64 media mnt opt proc root run sbin srv sys tmp usr var 当利用 docker run 来创建容器时，Docker 在后台运行的标准操作包括： 检查本地是否存在指定的镜像，不存在就从 registry 下载 利用镜像创建并启动一个容器 分配一个文件系统，并在只读的镜像层外面挂载一层可读写层 从宿主主机配置的网桥接口中桥接一个虚拟接口到容器中去 从地址池配置一个 ip 地址给容器 执行用户指定的应用程序 执行完毕后容器被终止 启动已终止容器可以利用 docker container start 命令，直接将一个已经终止（exited）的容器启动运行。 容器的核心为所执行的应用程序，所需要的资源都是应用程序运行所必需的。除此之外，并没有其它的资源。可以在伪终端中利用 ps 或 top 来查看进程信息。 1234root@ba267838cc1b:/# ps PID TTY TIME CMD 1 ? 00:00:00 bash 11 ? 00:00:00 ps 可见，容器中仅运行了指定的 bash 应用。这种特点使得 Docker 对资源的利用率极高，是货真价实的轻量级虚拟化。 2 守护态运行更多的时候，需要让 Docker 在后台运行而不是直接把执行命令的结果输出在当前宿主机下。此时，可以通过添加 **-d 参数 (Detach)**来实现。 下面举两个例子来说明一下。 如果不使用 -d 参数运行容器。 12345$ docker run ubuntu:18.04 /bin/sh -c &quot;while true; do echo hello world; sleep 1; done&quot;hello worldhello worldhello worldhello world 容器会把输出的结果 (STDOUT) 打印到宿主机上面 如果使用了 -d 参数运行容器。 12$ docker run -d ubuntu:18.04 /bin/sh -c &quot;while true; do echo hello world; sleep 1; done&quot;77b2dc01fe0f3f1265df143181e7b9af5e05279a884f4776ee75350ea9d8017a 此时容器会在后台运行并不会把输出的结果 (STDOUT) 打印到宿主机上面(输出结果可以用 docker logs 查看)。 注： 容器是否会长久运行，是和 docker run 指定的命令有关，和 -d 参数无关。 使用 -d 参数启动后会返回一个唯一的 id，也可以通过 docker container ls 命令来查看容器信息。 123$ docker container lsCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES77b2dc01fe0f ubuntu:18.04 /bin/sh -c &#x27;while tr 2 minutes ago Up 1 minute agitated_wright 要获取容器的输出信息，可以通过 docker container logs 命令。 12345$ docker container logs [container ID or NAMES]hello worldhello worldhello world. . . 3 终止可以使用 docker container stop 来终止一个运行中的容器。 此外，当 Docker 容器中指定的应用终结时，容器也自动终止。 例如对于上一章节中只启动了一个终端的容器，用户通过 exit 命令或 Ctrl+d 来退出终端时，所创建的容器立刻终止。 终止状态的容器可以用 docker container ls -a 命令看到。例如 123$ docker container ls -aCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESba267838cc1b ubuntu:18.04 &quot;/bin/bash&quot; 30 minutes ago Exited (0) About a minute ago trusting_newton 处于终止状态的容器，可以通过 docker container start 命令来重新启动。 此外，docker container restart 命令会将一个运行态的容器终止，然后再重新启动它。 4 进入容器在使用 -d 参数时，容器启动后会进入后台。 某些时候需要进入容器进行操作，包括使用 docker attach 命令或 docker exec 命令，推荐大家使用 docker exec 命令，原因会在下面说明。 attach 命令下面示例如何使用 docker attach 命令。 123456789$ docker run -dit ubuntu243c32535da7d142fb0e6df616a3c3ada0b8ab417937c853a9e1c251f499f550$ docker container lsCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES243c32535da7 ubuntu:latest &quot;/bin/bash&quot; 18 seconds ago Up 17 seconds nostalgic_hypatia$ docker attach 243croot@243c32535da7:/# 注意： 如果从这个 stdin 中 exit，会导致容器的停止。 exec 命令-i -t 参数docker exec 后边可以跟多个参数，这里主要说明 -i -t 参数。 只用 -i 参数时，由于没有分配伪终端，界面没有我们熟悉的 Linux 命令提示符，但命令执行结果仍然可以返回。 当 -i -t 参数一起使用时，则可以看到我们熟悉的 Linux 命令提示符。 12345678910111213141516$ docker run -dit ubuntu69d137adef7a8a689cbcb059e94da5489d3cddd240ff675c640c8d96e84fe1f6$ docker container lsCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES69d137adef7a ubuntu:latest &quot;/bin/bash&quot; 18 seconds ago Up 17 seconds zealous_swirles$ docker exec -i 69d1 bashlsbinbootdev...$ docker exec -it 69d1 bashroot@69d137adef7a:/# 如果从这个 stdin 中 exit，不会导致容器的停止。这就是为什么推荐大家使用 docker exec 的原因。 4 导出和导入导出容器如果要导出本地某个容器，可以使用 docker export 命令。 1234$ docker container ls -aCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES7691a814370e ubuntu:18.04 &quot;/bin/bash&quot; 36 hours ago Exited (0) 21 hours ago test$ docker export 7691a814370e &gt; ubuntu.tar 这样将导出容器快照到本地文件。 导入容器快照可以使用 docker import 从容器快照文件中再导入为镜像，例如 1234$ cat ubuntu.tar | docker import - test/ubuntu:v1.0$ docker image lsREPOSITORY TAG IMAGE ID CREATED VIRTUAL SIZEtest/ubuntu v1.0 9d37a6082e97 About a minute ago 171.3 MB 此外，也可以通过指定 URL 或者某个目录来导入，例如 1$ docker import http://example.com/exampleimage.tgz example/imagerepo 注：用户既可以使用 docker load 来导入镜像存储文件到本地镜像库，也可以使用 docker import来导入一个容器快照到本地镜像库。这两者的区别在于容器快照文件将丢弃所有的历史记录和元数据信息（即仅保存容器当时的快照状态），而镜像存储文件将保存完整记录，体积也要大。此外，从容器快照文件导入时可以重新指定标签等元数据信息。 保存镜像归档文件 将一个镜像保存到一个 tar 文件 1docker save -o myimage_v1.tar myimage:v1 将多个镜像保存到一个 tar 文件 1docker save -o myimages.tar myimage:v1 myimage:v2 通过重定向来保存镜像 （不用-o参数） 1docker save myimage:v1 &gt; myimage_v1.tar 加载镜像归档文件 使用 --input 选项 1docker load --input[-i] myimage.tar 使用管道 1cat myimage.tar | docker load [-] 其中 - 代表标准输入&#x2F;输出。 5 删除删除容器可以使用 docker container rm 来删除一个处于终止状态的容器。例如 12$ docker container rm trusting_newtontrusting_newton 如果要删除一个运行中的容器，可以添加 -f 参数。Docker 会发送 SIGKILL 信号给容器。 清理所有处于终止状态的容器用 docker container ls -a 命令可以查看所有已经创建的包括终止状态的容器，如果数量太多要一个个删除可能会很麻烦，用下面的命令可以清理掉所有处于终止状态的容器。 1$ docker container prune","tags":[{"name":"docker","slug":"docker","permalink":"https://stu-yue.github.io/tags/docker/"},{"name":"linux","slug":"linux","permalink":"https://stu-yue.github.io/tags/linux/"}],"categories":[{"name":"tools","slug":"tools","permalink":"https://stu-yue.github.io/categories/tools/"},{"name":"docker","slug":"tools/docker","permalink":"https://stu-yue.github.io/categories/tools/docker/"}]},{"title":"Memo | Regitstry and Data","date":"2023-10-04T07:56:53.000Z","path":"jottings/tools/docker/memo_registry_and_data/","text":"1 私有仓库有时候使用 Docker Hub 这样的公共仓库可能不方便，用户可以创建一个本地仓库供私人使用。 docker-registry 是官方提供的工具，可以用于构建私有的镜像仓库。本文内容基于 docker-registry v2.x 版本。 安装运行 docker-registry容器运行你可以使用官方 registry 镜像来运行。 1$ docker run -d -p 5000:5000 --restart=always --name registry registry 这将使用官方的 registry 镜像来启动私有仓库。默认情况下，仓库会被创建在容器的 /var/lib/registry 目录下。你可以通过 -v 参数来将镜像文件存放在本地的指定路径。例如下面的例子将上传的镜像放到本地的 /opt/data/registry 目录。 1234$ docker run -d \\ -p 5000:5000 \\ -v /opt/data/registry:/var/lib/registry \\ registry 在私有仓库上传、搜索、下载镜像创建好私有仓库之后，就可以使用 docker tag 来标记一个镜像，然后推送它到仓库。例如私有仓库地址为 127.0.0.1:5000。 先在本机查看已有的镜像。 123$ docker image lsREPOSITORY TAG IMAGE ID CREATED VIRTUAL SIZEubuntu latest ba5877dc9bec 6 weeks ago 192.7 MB 使用 docker tag 将 ubuntu:latest 这个镜像标记为 127.0.0.1:5000/ubuntu:latest。 格式为 docker tag IMAGE[:TAG] [REGISTRY_HOST[:REGISTRY_PORT]/]REPOSITORY[:TAG]。 12345$ docker tag ubuntu:latest 127.0.0.1:5000/ubuntu:latest$ docker image lsREPOSITORY TAG IMAGE ID CREATED VIRTUAL SIZEubuntu latest ba5877dc9bec 6 weeks ago 192.7 MB127.0.0.1:5000/ubuntu:latest latest ba5877dc9bec 6 weeks ago 192.7 MB 使用 docker push 上传标记的镜像。 123456789$ docker push 127.0.0.1:5000/ubuntu:latestThe push refers to repository [127.0.0.1:5000/ubuntu]373a30c24545: Pusheda9148f5200b0: Pushedcdd3de0940ab: Pushedfc56279bbb33: Pushedb38367233d37: Pushed2aebd096e0e2: Pushedlatest: digest: sha256:fe4277621f10b5026266932ddf760f5a756d2facd505a94d2da12f4f52f71f5a size: 1568 用 curl 查看仓库中的镜像。 12$ curl 127.0.0.1:5000/v2/_catalog&#123;&quot;repositories&quot;:[&quot;ubuntu&quot;]&#125; 这里可以看到 &#123;&quot;repositories&quot;:[&quot;ubuntu&quot;]&#125;，表明镜像已经被成功上传了。 先删除已有镜像，再尝试从私有仓库中下载这个镜像。 1234567891011121314$ docker image rm 127.0.0.1:5000/ubuntu:latest$ docker pull 127.0.0.1:5000/ubuntu:latestPulling repository 127.0.0.1:5000/ubuntu:latestba5877dc9bec: Download complete511136ea3c5a: Download complete9bad880da3d2: Download complete25f11f5fb0cb: Download completeebc34468f71d: Download complete2318d26665ef: Download complete$ docker image lsREPOSITORY TAG IMAGE ID CREATED VIRTUAL SIZE127.0.0.1:5000/ubuntu:latest latest ba5877dc9bec 6 weeks ago 192.7 MB 配置非 https 仓库地址如果你不想使用 127.0.0.1:5000 作为仓库地址，比如想让本网段的其他主机也能把镜像推送到私有仓库。你就得把例如 192.168.199.100:5000 这样的内网地址作为私有仓库地址，这时你会发现无法成功推送镜像。 这是因为 Docker 默认不允许非 HTTPS 方式推送镜像。我们可以通过 Docker 的配置选项来取消这个限制，或者查看下一节配置能够通过 HTTPS 访问的私有仓库。 Ubuntu 16.04+, Debian 8+, centos 7对于使用 systemd 的系统，请在 /etc/docker/daemon.json 中写入如下内容（如果文件不存在请新建该文件） 123456789&#123; &quot;registry-mirrors&quot;: [ &quot;https://hub-mirror.c.163.com&quot;, &quot;https://mirror.baidubce.com&quot; ], &quot;insecure-registries&quot;: [ &quot;192.168.199.100:5000&quot; ]&#125; 注意：该文件必须符合 json 规范，否则 Docker 将不能启动。 其他对于 Docker Desktop for Windows 、 Docker Desktop for Mac 在设置中的 Docker Engine 中进行编辑 ，增加和上边一样的字符串即可。 2 数据卷数据卷 是一个可供一个或多个容器使用的特殊目录，它绕过 UnionFS，可以提供很多有用的特性： 数据卷 可以在容器之间共享和重用 对 数据卷 的修改会立马生效 对 数据卷 的更新，不会影响镜像 数据卷 默认会一直存在，即使容器被删除 注意：数据卷 的使用，类似于 Linux 下对目录或文件进行 mount，镜像中的被指定为挂载点的目录中的文件会复制到数据卷中（仅数据卷为空时会复制）。 创建一个数据卷1$ docker volume create my-vol 查看所有的 数据卷 1234$ docker volume lsDRIVER VOLUME NAMElocal my-vol 在主机里使用以下命令可以查看指定 数据卷 的信息 复制 1234567891011$ docker volume inspect my-vol[ &#123; &quot;Driver&quot;: &quot;local&quot;, &quot;Labels&quot;: &#123;&#125;, &quot;Mountpoint&quot;: &quot;/var/lib/docker/volumes/my-vol/_data&quot;, &quot;Name&quot;: &quot;my-vol&quot;, &quot;Options&quot;: &#123;&#125;, &quot;Scope&quot;: &quot;local&quot; &#125;] 启动一个挂载数据卷的容器在用 docker run 命令的时候，使用 --mount 标记来将 数据卷 挂载到容器里。在一次 docker run 中可以挂载多个 数据卷。 下面创建一个名为 web 的容器，并加载一个 数据卷 到容器的 /usr/share/nginx/html 目录。 12345$ docker run -d -P \\ --name web \\ # -v my-vol:/usr/share/nginx/html \\ --mount source=my-vol,target=/usr/share/nginx/html \\ nginx:alpine 查看数据卷的具体信息在主机里使用以下命令可以查看 web 容器的信息 1$ docker inspect web 数据卷 信息在 “Mounts” Key 下面 123456789101112&quot;Mounts&quot;: [ &#123; &quot;Type&quot;: &quot;volume&quot;, &quot;Name&quot;: &quot;my-vol&quot;, &quot;Source&quot;: &quot;/var/lib/docker/volumes/my-vol/_data&quot;, &quot;Destination&quot;: &quot;/usr/share/nginx/html&quot;, &quot;Driver&quot;: &quot;local&quot;, &quot;Mode&quot;: &quot;&quot;, &quot;RW&quot;: true, &quot;Propagation&quot;: &quot;&quot; &#125;], 删除数据卷1$ docker volume rm my-vol 数据卷 是被设计用来持久化数据的，它的生命周期独立于容器，Docker 不会在容器被删除后自动删除 数据卷，并且也不存在垃圾回收这样的机制来处理没有任何容器引用的 数据卷。如果需要在删除容器的同时移除数据卷。可以在删除容器的时候使用 docker rm -v 这个命令。 无主的数据卷可能会占据很多空间，要清理请使用以下命令 1$ docker volume prune 3 挂载主机目录挂载一个主机目录作为数据卷使用 --mount 标记可以指定挂载一个本地主机的目录到容器中去。 12345$ docker run -d -P \\ --name web \\ # -v /src/webapp:/usr/share/nginx/html \\ --mount type=bind,source=/src/webapp,target=/usr/share/nginx/html \\ nginx:alpine 上面的命令加载主机的 /src/webapp 目录到容器的 /usr/share/nginx/html目录。这个功能在进行测试的时候十分方便，比如用户可以放置一些程序到本地目录中，来查看容器是否正常工作。本地目录的路径必须是绝对路径，以前使用 -v 参数时如果本地目录不存在 Docker 会自动为你创建一个文件夹，现在使用 --mount 参数时如果本地目录不存在，Docker 会报错。 **Docker 挂载主机目录的默认权限是 读写，用户也可以通过增加 readonly 指定为 只读**。 12345$ docker run -d -P \\ --name web \\ # -v /src/webapp:/usr/share/nginx/html:ro \\ --mount type=bind,source=/src/webapp,target=/usr/share/nginx/html,readonly \\ nginx:alpine 加了 readonly 之后，就挂载为 只读 了。如果你在容器内 /usr/share/nginx/html 目录新建文件，会显示如下错误 12/usr/share/nginx/html # touch new.txttouch: new.txt: Read-only file system 查看数据卷的具体信息在主机里使用以下命令可以查看 web 容器的信息 1$ docker inspect web 挂载主机目录 的配置信息在 “Mounts” Key 下面 12345678910&quot;Mounts&quot;: [ &#123; &quot;Type&quot;: &quot;bind&quot;, &quot;Source&quot;: &quot;/src/webapp&quot;, &quot;Destination&quot;: &quot;/usr/share/nginx/html&quot;, &quot;Mode&quot;: &quot;&quot;, &quot;RW&quot;: true, &quot;Propagation&quot;: &quot;rprivate&quot; &#125;], 挂载一个本地主机文件作为数据卷--mount 标记也可以从主机挂载单个文件到容器中 123456789$ docker run --rm -it \\ # -v $HOME/.bash_history:/root/.bash_history \\ --mount type=bind,source=$HOME/.bash_history,target=/root/.bash_history \\ ubuntu:18.04 \\ bashroot@2affd44b4667:/# history1 ls2 diskutil list 这样就可以记录在容器输入过的命令了。","tags":[{"name":"docker","slug":"docker","permalink":"https://stu-yue.github.io/tags/docker/"},{"name":"linux","slug":"linux","permalink":"https://stu-yue.github.io/tags/linux/"}],"categories":[{"name":"tools","slug":"tools","permalink":"https://stu-yue.github.io/categories/tools/"},{"name":"docker","slug":"tools/docker","permalink":"https://stu-yue.github.io/categories/tools/docker/"}]},{"title":"Memo | Usage","date":"2023-10-04T07:56:53.000Z","path":"jottings/tools/docker/memo_usage/","text":"0 Docker命令简介 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485Usage: docker [OPTIONS] COMMANDA self-sufficient runtime for containersCommon Commands: run Create and run a new container from an image exec Execute a command in a running container ps List containers build Build an image from a Dockerfile pull Download an image from a registry push Upload an image to a registry images List images login Log in to a registry logout Log out from a registry search Search Docker Hub for images version Show the Docker version information info Display system-wide informationManagement Commands: builder Manage builds buildx* Docker Buildx checkpoint Manage checkpoints compose* Docker Compose container Manage containers context Manage contexts image Manage images manifest Manage Docker image manifests and manifest lists network Manage networks plugin Manage plugins system Manage Docker trust Manage trust on Docker images volume Manage volumesSwarm Commands: swarm Manage SwarmCommands: attach Attach local standard input, output, and error streams to a running container commit Create a new image from a container&#x27;s changes cp Copy files/folders between a container and the local filesystem create Create a new container diff Inspect changes to files or directories on a container&#x27;s filesystem events Get real time events from the server export Export a container&#x27;s filesystem as a tar archive history Show the history of an image import Import the contents from a tarball to create a filesystem image inspect Return low-level information on Docker objects kill Kill one or more running containers load Load an image from a tar archive or STDIN logs Fetch the logs of a container pause Pause all processes within one or more containers port List port mappings or a specific mapping for the container rename Rename a container restart Restart one or more containers rm Remove one or more containers rmi Remove one or more images save Save one or more images to a tar archive (streamed to STDOUT by default) start Start one or more stopped containers stats Display a live stream of container(s) resource usage statistics stop Stop one or more running containers tag Create a tag TARGET_IMAGE that refers to SOURCE_IMAGE top Display the running processes of a container unpause Unpause all processes within one or more containers update Update configuration of one or more containers wait Block until one or more containers stop, then print their exit codesGlobal Options: --config string Location of client config files (default &quot;/home/wangy/.docker&quot;) -c, --context string Name of the context to use to connect to the daemon (overrides DOCKER_HOST env var and default context set with &quot;docker context use&quot;) -D, --debug Enable debug mode -H, --host list Daemon socket to connect to -l, --log-level string Set the logging level (&quot;debug&quot;, &quot;info&quot;, &quot;warn&quot;, &quot;error&quot;, &quot;fatal&quot;) (default &quot;info&quot;) --tls Use TLS; implied by --tlsverify --tlscacert string Trust certs signed only by this CA (default &quot;/home/wangy/.docker/ca.pem&quot;) --tlscert string Path to TLS certificate file (default &quot;/home/wangy/.docker/cert.pem&quot;) --tlskey string Path to TLS key file (default &quot;/home/wangy/.docker/key.pem&quot;) --tlsverify Use TLS and verify the remote -v, --version Print version information and quitRun &#x27;docker COMMAND --help&#x27; for more information on a command. 1 获取镜像之前提到过，Docker Hub 上有大量的高质量的镜像可以用，这里我们就说一下怎么获取这些镜像。 从 Docker 镜像仓库获取镜像的命令是 docker pull。其命令格式为： 1$ docker pull [选项] [Docker Registry 地址[:端口号]/]仓库名[:标签] 具体的选项可以通过 docker pull --help 命令看到，这里我们说一下镜像名称的格式。 Docker 镜像仓库地址：地址的格式一般是 &lt;域名/IP&gt;[:端口号]。默认地址是 Docker Hub(docker.io)。 仓库名：如之前所说，这里的仓库名是两段式名称，即 &lt;用户名&gt;/&lt;软件名&gt;。对于 Docker Hub，如果不给出用户名，则默认为 library，也就是官方镜像。 比如： 12345678$ docker pull ubuntu:18.0418.04: Pulling from library/ubuntu92dc2a97ff99: Pull completebe13a9d27eb8: Pull completec8299583700a: Pull completeDigest: sha256:4bc3ae6596938cb0d9e5ac51a1152ec9dcac2a1c50829c74abd9c4361e321b26Status: Downloaded newer image for ubuntu:18.04docker.io/library/ubuntu:18.04 上面的命令中没有给出 Docker 镜像仓库地址，因此将会从 Docker Hub （docker.io）获取镜像。而镜像名称是 ubuntu:18.04，因此将会获取官方镜像 library/ubuntu 仓库中标签为 18.04 的镜像。docker pull **命令的输出结果最后一行给出了镜像的完整名称，即： docker.io/library/ubuntu:18.04**。 从下载过程中可以看到我们之前提及的分层存储的概念，镜像是由多层存储所构成。下载也是一层层的去下载，并非单一文件。下载过程中给出了每一层的 ID 的前 12 位。并且下载结束后，给出该镜像完整的 sha256 的摘要，以确保下载一致性。 在使用上面命令的时候，你可能会发现，你所看到的层 ID 以及 sha256 的摘要和这里的不一样。这是因为官方镜像是一直在维护的，有任何新的 bug，或者版本更新，都会进行修复再以原来的标签发布，这样可以确保任何使用这个标签的用户可以获得更安全、更稳定的镜像。 如果从 Docker Hub 下载镜像非常缓慢，可以参照 镜像加速器 一节配置加速器。 运行有了镜像后，我们就能够以这个镜像为基础启动并运行一个容器。以上面的 ubuntu:18.04 为例，如果我们打算启动里面的 bash 并且进行交互式操作的话，可以执行下面的命令。 复制 123456789101112131415$ docker run -it --rm ubuntu:18.04 bashroot@e7009c6ce357:/# cat /etc/os-releaseNAME=&quot;Ubuntu&quot;VERSION=&quot;18.04.1 LTS (Bionic Beaver)&quot;ID=ubuntuID_LIKE=debianPRETTY_NAME=&quot;Ubuntu 18.04.1 LTS&quot;VERSION_ID=&quot;18.04&quot;HOME_URL=&quot;https://www.ubuntu.com/&quot;SUPPORT_URL=&quot;https://help.ubuntu.com/&quot;BUG_REPORT_URL=&quot;https://bugs.launchpad.net/ubuntu/&quot;PRIVACY_POLICY_URL=&quot;https://www.ubuntu.com/legal/terms-and-policies/privacy-policy&quot;VERSION_CODENAME=bionicUBUNTU_CODENAME=bionic docker run 就是运行容器的命令，具体格式我们会在 容器 一节进行详细讲解，我们这里简要的说明一下上面用到的参数。 -it：这是两个参数，一个是 -i：交互式操作，一个是 -t 终端。我们这里打算进入 bash 执行一些命令并查看返回结果，因此我们需要交互式终端。 --rm：这个参数是说容器退出后随之将其删除。默认情况下，为了排障需求，退出的容器并不会立即删除，除非手动 docker rm。我们这里只是随便执行个命令，看看结果，不需要排障和保留结果，因此使用 --rm 可以避免浪费空间。 ubuntu:18.04：这是指用 ubuntu:18.04 镜像为基础来启动容器。 bash：放在镜像名后的是 命令，这里我们希望有个交互式 Shell，因此用的是 bash。 进入容器后，我们可以在 Shell 下操作，执行任何所需的命令。这里，我们执行了 cat /etc/os-release，这是 Linux 常用的查看当前系统版本的命令，从返回的结果可以看到容器内是 Ubuntu 18.04.1 LTS 系统。 最后我们通过 exit 退出了这个容器。 2 列出镜像要想列出已经下载下来的镜像，可以使用 docker image ls 命令。 12345678$ docker image lsREPOSITORY TAG IMAGE ID CREATED SIZEredis latest 5f515359c7f8 5 days ago 183 MBnginx latest 05a60462f8ba 5 days ago 181 MBmongo 3.2 fe9198c04d62 5 days ago 342 MB&lt;none&gt; &lt;none&gt; 00285df0df87 5 days ago 342 MBubuntu 18.04 329ed837d508 3 days ago 63.3MBubuntu bionic 329ed837d508 3 days ago 63.3MB 列表包含了 仓库名、标签、镜像 ID、创建时间 以及 所占用的空间。 其中仓库名、标签在之前的基础概念章节已经介绍过了。镜像 ID 则是镜像的唯一标识，一个镜像可以对应多个 标签。因此，在上面的例子中，我们可以看到 ubuntu:18.04 和 ubuntu:bionic 拥有相同的 ID，因为它们对应的是同一个镜像。 镜像体积如果仔细观察，会注意到，这里标识的所占用空间和在 Docker Hub 上看到的镜像大小不同。比如，ubuntu:18.04 镜像大小，在这里是 63.3MB，但是在 Docker Hub 显示的却是 25.47 MB。这是因为 Docker Hub 中显示的体积是压缩后的体积。在镜像下载和上传过程中镜像是保持着压缩状态的，因此 Docker Hub 所显示的大小是网络传输中更关心的流量大小。而 docker image ls 显示的是镜像下载到本地后，展开的大小，准确说，是展开后的各层所占空间的总和，因为镜像到本地后，查看空间的时候，更关心的是本地磁盘空间占用的大小。 另外一个需要注意的问题是，**docker image ls 列表中的镜像体积总和并非是所有镜像实际硬盘消耗**。由于 Docker 镜像是多层存储结构，并且可以继承、复用，因此不同镜像可能会因为使用相同的基础镜像，从而拥有共同的层。由于 Docker 使用 Union FS，相同的层只需要保存一份即可，因此实际镜像硬盘占用空间很可能要比这个列表镜像大小的总和要小的多。 你可以通过 docker system df 命令来便捷的查看镜像、容器、数据卷所占用的空间。 1234567$ docker system dfTYPE TOTAL ACTIVE SIZE RECLAIMABLEImages 24 0 1.992GB 1.992GB (100%)Containers 1 0 62.82MB 62.82MB (100%)Local Volumes 9 0 652.2MB 652.2MB (100%)Build Cache 0B 0B 虚悬镜像上面的镜像列表中，还可以看到一个特殊的镜像，这个镜像既没有仓库名，也没有标签，均为 &lt;none&gt;。： 1&lt;none&gt; &lt;none&gt; 00285df0df87 5 days ago 342 MB 这个镜像原本是有镜像名和标签的，原来为 mongo:3.2，随着官方镜像维护，发布了新版本后，重新 docker pull mongo:3.2 时，mongo:3.2 这个镜像名被转移到了新下载的镜像身上，而旧的镜像上的这个名称则被取消，从而成为了 &lt;none&gt;。除了 docker pull 可能导致这种情况，**docker build 也同样可以导致这种现象。由于新旧镜像同名，旧镜像名称被取消，从而出现仓库名、标签均为 &lt;none&gt; 的镜像**。这类无标签镜像也被称为 虚悬镜像(dangling image) ，可以用下面的命令专门显示这类镜像： 123$ docker image ls -f dangling=trueREPOSITORY TAG IMAGE ID CREATED SIZE&lt;none&gt; &lt;none&gt; 00285df0df87 5 days ago 342 MB 一般来说，虚悬镜像已经失去了存在的价值，是可以随意删除的，可以用下面的命令删除。 1$ docker image prune 中间层镜像为了加速镜像构建、重复利用资源，Docker 会利用 中间层镜像。所以在使用一段时间后，可能会看到一些依赖的中间层镜像。默认的 docker image ls 列表中只会显示顶层镜像，如果希望显示包括中间层镜像在内的所有镜像的话，需要加 -a 参数。 复制 1$ docker image ls -a 这样会看到很多无标签的镜像，与之前的虚悬镜像不同，这些无标签的镜像很多都是中间层镜像，是其它镜像所依赖的镜像。这些无标签镜像不应该删除，否则会导致上层镜像因为依赖丢失而出错。实际上，这些镜像也没必要删除，因为之前说过，相同的层只会存一遍，而这些镜像是别的镜像的依赖，因此并不会因为它们被列出来而多存了一份，无论如何你也会需要它们。只要删除那些依赖它们的镜像后，这些依赖的中间层镜像也会被连带删除。 列出部分镜像不加任何参数的情况下，docker image ls 会列出所有顶层镜像，但是有时候我们只希望列出部分镜像。docker image ls 有好几个参数可以帮助做到这个事情。 根据仓库名列出镜像 1234$ docker image ls ubuntuREPOSITORY TAG IMAGE ID CREATED SIZEubuntu 18.04 329ed837d508 3 days ago 63.3MBubuntu bionic 329ed837d508 3 days ago 63.3MB 列出特定的某个镜像，也就是说指定仓库名和标签 123$ docker image ls ubuntu:18.04REPOSITORY TAG IMAGE ID CREATED SIZEubuntu 18.04 329ed837d508 3 days ago 63.3MB 除此以外，**docker image ls 还支持强大的过滤器参数 --filter，或者简写 -f**。之前我们已经看到了使用过滤器来列出虚悬镜像的用法，它还有更多的用法。比如，我们希望看到在 mongo:3.2 之后建立的镜像，可以用下面的命令： 1234$ docker image ls -f since=mongo:3.2REPOSITORY TAG IMAGE ID CREATED SIZEredis latest 5f515359c7f8 5 days ago 183 MBnginx latest 05a60462f8ba 5 days ago 181 MB 想查看某个位置之前的镜像也可以，只需要把 since 换成 before 即可。 此外，如果镜像构建时，定义了 LABEL，还可以通过 LABEL 来过滤。 12$ docker image ls -f label=com.example.version=0.1... 以特定格式显示默认情况下，docker image ls 会输出一个完整的表格，但是我们并非所有时候都会需要这些内容。比如，刚才删除虚悬镜像的时候，我们需要利用 docker image ls 把所有的虚悬镜像的 ID 列出来，然后才可以交给 docker image rm 命令作为参数来删除指定的这些镜像，这个时候就用到了 -q 参数。 1234567$ docker image ls -q5f515359c7f805a60462f8bafe9198c04d6200285df0df87329ed837d508329ed837d508 --filter 配合 -q 产生出指定范围的 ID 列表，然后送给另一个 docker 命令作为参数，从而针对这组实体成批的进行某种操作的做法在 Docker 命令行使用过程中非常常见，不仅仅是镜像，将来我们会在各个命令中看到这类搭配以完成很强大的功能。因此每次在文档看到过滤器后，可以多注意一下它们的用法。 另外一些时候，我们可能只是对表格的结构不满意，希望自己组织列；或者不希望有标题，这样方便其它程序解析结果等，这就用到了 Go 的模板语法。 比如，下面的命令会直接列出镜像结果，并且只包含镜像ID和仓库名： 1234567$ docker image ls --format &quot;&#123;&#123;.ID&#125;&#125;: &#123;&#123;.Repository&#125;&#125;&quot;5f515359c7f8: redis05a60462f8ba: nginxfe9198c04d62: mongo00285df0df87: &lt;none&gt;329ed837d508: ubuntu329ed837d508: ubuntu 或者打算以表格等距显示，并且有标题行，和默认一样，不过自己定义列： 12345678$ docker image ls --format &quot;table &#123;&#123;.ID&#125;&#125;\\t&#123;&#123;.Repository&#125;&#125;\\t&#123;&#123;.Tag&#125;&#125;&quot;IMAGE ID REPOSITORY TAG5f515359c7f8 redis latest05a60462f8ba nginx latestfe9198c04d62 mongo 3.200285df0df87 &lt;none&gt; &lt;none&gt;329ed837d508 ubuntu 18.04329ed837d508 ubuntu bionic 3 删除本地镜像如果要删除本地的镜像，可以使用 docker image rm 命令，其格式为： 1$ docker image rm [选项] &lt;镜像1&gt; [&lt;镜像2&gt; ...] 用 ID、镜像名、摘要删除镜像其中，&lt;镜像&gt; 可以是 镜像短 ID、镜像长 ID、镜像名 或者 镜像摘要。 比如我们有这么一些镜像： 123456$ docker image lsREPOSITORY TAG IMAGE ID CREATED SIZEcentos latest 0584b3d2cf6d 3 weeks ago 196.5 MBredis alpine 501ad78535f0 3 weeks ago 21.03 MBdocker latest cf693ec9b5c7 3 weeks ago 105.1 MBnginx latest e43d811ce2f4 5 weeks ago 181.5 MB 我们可以用镜像的完整 ID，也称为 长 ID，来删除镜像。使用脚本的时候可能会用长 ID，但是人工输入就太累了，所以更多的时候是用 短 ID 来删除镜像。docker image ls 默认列出的就已经是短 ID 了，一般取前3个字符以上，只要足够区分于别的镜像就可以了。 比如这里，如果我们要删除 redis:alpine 镜像，可以执行： 123456789$ docker image rm 501Untagged: redis:alpineUntagged: redis@sha256:f1ed3708f538b537eb9c2a7dd50dc90a706f7debd7e1196c9264edeea521a86dDeleted: sha256:501ad78535f015d88872e13fa87a828425117e3d28075d0c117932b05bf189b7Deleted: sha256:96167737e29ca8e9d74982ef2a0dda76ed7b430da55e321c071f0dbff8c2899bDeleted: sha256:32770d1dcf835f192cafd6b9263b7b597a1778a403a109e2cc2ee866f74adf23Deleted: sha256:127227698ad74a5846ff5153475e03439d96d4b1c7f2a449c7a826ef74a2d2faDeleted: sha256:1333ecc582459bac54e1437335c0816bc17634e131ea0cc48daa27d32c75eab3Deleted: sha256:4fc455b921edf9c4aea207c51ab39b10b06540c8b4825ba57b3feed1668fa7c7 我们也可以用镜像名，也就是 &lt;仓库名&gt;:&lt;标签&gt;，来删除镜像。 12345$ docker image rm centosUntagged: centos:latestUntagged: centos@sha256:b2f9d1c0ff5f87a4743104d099a3d561002ac500db1b9bfa02a783a46e0d366cDeleted: sha256:0584b3d2cf6d235ee310cf14b54667d889887b838d3f3d3033acd70fc3c48b8aDeleted: sha256:97ca462ad9eeae25941546209454496e1d66749d53dfa2ee32bf1faabd239d38 当然，更精确的是使用 镜像摘要 删除镜像。 123456$ docker image ls --digestsREPOSITORY TAG DIGEST IMAGE ID CREATED SIZEnode slim sha256:b4f0e0bdeb578043c1ea6862f0d40cc4afe32a4a582f3be235a3b164422be228 6e0c4c8e3913 3 weeks ago 214 MB$ docker image rm node@sha256:b4f0e0bdeb578043c1ea6862f0d40cc4afe32a4a582f3be235a3b164422be228Untagged: node@sha256:b4f0e0bdeb578043c1ea6862f0d40cc4afe32a4a582f3be235a3b164422be228 Untagged 和 Deleted如果观察上面这几个命令的运行输出信息的话，你会注意到删除行为分为两类，一类是 Untagged，另一类是 Deleted。我们之前介绍过，镜像的唯一标识是其 ID 和摘要，而一个镜像可以有多个标签。 因此当我们使用上面命令删除镜像的时候，实际上是在要求删除某个标签的镜像。所以首先需要做的是将满足我们要求的所有镜像标签都取消，这就是我们看到的 Untagged 的信息。因为一个镜像可以对应多个标签，因此当我们删除了所指定的标签后，可能还有别的标签指向了这个镜像，如果是这种情况，那么 Delete 行为就不会发生。所以并非所有的 docker image rm 都会产生删除镜像的行为，有可能仅仅是取消了某个标签而已。 当该镜像所有的标签都被取消了，该镜像很可能会失去了存在的意义，因此会触发删除行为。镜像是多层存储结构，因此在删除的时候也是从上层向基础层方向依次进行判断删除。镜像的多层结构让镜像复用变得非常容易，因此很有可能某个其它镜像正依赖于当前镜像的某一层。这种情况，依旧不会触发删除该层的行为。直到没有任何层依赖当前层时，才会真实的删除当前层。这就是为什么，有时候会奇怪，为什么明明没有别的标签指向这个镜像，但是它还是存在的原因，也是为什么有时候会发现所删除的层数和自己 docker pull 看到的层数不一样的原因。 除了镜像依赖以外，还需要注意的是容器对镜像的依赖。如果有用这个镜像启动的容器存在（即使容器没有运行），那么同样不可以删除这个镜像。之前讲过，容器是以镜像为基础，再加一层容器存储层，组成这样的多层存储结构去运行的。因此该镜像如果被这个容器所依赖的，那么删除必然会导致故障。如果这些容器是不需要的，应该先将它们删除，然后再来删除镜像。 用 docker image ls 命令来配合像其它可以承接多个实体的命令一样，可以使用 docker image ls -q 来配合使用 docker image rm，这样可以成批的删除希望删除的镜像。我们在“镜像列表”章节介绍过很多过滤镜像列表的方式都可以拿过来使用。 比如，我们需要删除所有仓库名为 redis 的镜像： 1$ docker image rm $(docker image ls -q redis) 或者删除所有在 mongo:3.2 之前的镜像： 1$ docker image rm $(docker image ls -q -f before=mongo:3.2) 充分利用你的想象力和 Linux 命令行的强大，你可以完成很多非常赞的功能。 4 利用 commit 理解镜像构成 注意：如果您是初学者，您可以暂时跳过后面的内容，直接学习 容器 一节。 注意： docker commit 命令除了学习之外，还有一些特殊的应用场合，比如被入侵后保存现场等。但是，不要使用 docker commit 定制镜像，定制镜像应该使用 Dockerfile 来完成。如果你想要定制镜像请查看下一小节。 镜像是容器的基础，每次执行 docker run 的时候都会指定哪个镜像作为容器运行的基础。在之前的例子中，我们所使用的都是来自于 Docker Hub 的镜像。直接使用这些镜像是可以满足一定的需求，而当这些镜像无法直接满足需求时，我们就需要定制这些镜像。接下来的几节就将讲解如何定制镜像。 回顾一下之前我们学到的知识，镜像是多层存储，每一层是在前一层的基础上进行的修改；而容器同样也是多层存储，是在以镜像为基础层，在其基础上加一层作为容器运行时的存储层。 现在让我们以定制一个 Web 服务器为例子，来讲解镜像是如何构建的。 1$ docker run --name webserver -d -p 80:80 nginx 这条命令会用 nginx 镜像启动一个容器，命名为 webserver，并且映射了 80 端口，这样我们可以用浏览器去访问这个 nginx 服务器。 如果是在本机运行的 Docker，那么可以直接访问：http://localhost ，如果是在虚拟机、云服务器上安装的 Docker，则需要将 localhost 换为虚拟机地址或者实际云服务器地址。 直接用浏览器访问的话，我们会看到默认的 Nginx 欢迎页面。 现在，假设我们非常不喜欢这个欢迎页面，我们希望改成欢迎 Docker 的文字，我们可以使用 docker exec 命令进入容器，修改其内容。 1234$ docker exec -it webserver bashroot@3729b97e8226:/# echo &#x27;&lt;h1&gt;Hello, Docker!&lt;/h1&gt;&#x27; &gt; /usr/share/nginx/html/index.htmlroot@3729b97e8226:/# exitexit 我们以交互式终端方式进入 webserver 容器，并执行了 bash 命令，也就是获得一个可操作的 Shell。 然后，我们用 &lt;h1&gt;Hello, Docker!&lt;/h1&gt; 覆盖了 /usr/share/nginx/html/index.html 的内容。 现在我们再刷新浏览器的话，会发现内容被改变了。 我们修改了容器的文件，也就是改动了容器的存储层。我们可以通过 docker diff 命令看到具体的改动。 1234567891011121314151617$ docker diff webserverC /rootA /root/.bash_historyC /runC /usrC /usr/shareC /usr/share/nginxC /usr/share/nginx/htmlC /usr/share/nginx/html/index.htmlC /varC /var/cacheC /var/cache/nginxA /var/cache/nginx/client_tempA /var/cache/nginx/fastcgi_tempA /var/cache/nginx/proxy_tempA /var/cache/nginx/scgi_tempA /var/cache/nginx/uwsgi_temp 现在我们定制好了变化，我们希望能将其保存下来形成镜像。 要知道，当我们运行一个容器的时候（如果不使用卷的话），我们做的任何文件修改都会被记录于容器存储层里。而 Docker 提供了一个 docker commit 命令，可以将容器的存储层保存下来成为镜像。换句话说，就是在原有镜像的基础上，再叠加上容器的存储层，并构成新的镜像。以后我们运行这个新镜像的时候，就会拥有原有容器最后的文件变化。 docker commit 的语法格式为： 1$ docker commit [选项] &lt;容器ID或容器名&gt; [&lt;仓库名&gt;[:&lt;标签&gt;]] 我们可以用下面的命令将容器保存为镜像： 1234567$ docker commit \\ [-a]--author &quot;Tao Wang &lt;twang2218@gmail.com&gt;&quot; \\ [-m]--message &quot;修改了默认网页&quot; \\ # [-c] list # apply dockerfile instruction to the created image webserver \\ nginx:v2sha256:07e33465974800ce65751acc279adc6ed2dc5ed4e0838f8b86f0c87aa1795214 其中 --author 是指定修改的作者，而 --message 则是记录本次修改的内容。这点和 git 版本控制相似，不过这里这些信息可以省略留空。 我们可以在 docker image ls 中看到这个新定制的镜像： 12345$ docker image ls nginxREPOSITORY TAG IMAGE ID CREATED SIZEnginx v2 07e334659748 9 seconds ago 181.5 MBnginx 1.11 05a60462f8ba 12 days ago 181.5 MBnginx latest e43d811ce2f4 4 weeks ago 181.5 MB 我们还可以用 docker history 具体查看镜像内的历史记录，如果比较 nginx:latest 的历史记录，我们会发现新增了我们刚刚提交的这一层。 1234567891011$ docker history nginx:v2IMAGE CREATED CREATED BY SIZE COMMENT07e334659748 54 seconds ago nginx -g daemon off; 95 B 修改了默认网页e43d811ce2f4 4 weeks ago /bin/sh -c #(nop) CMD [&quot;nginx&quot; &quot;-g&quot; &quot;daemon 0 B&lt;missing&gt; 4 weeks ago /bin/sh -c #(nop) EXPOSE 443/tcp 80/tcp 0 B&lt;missing&gt; 4 weeks ago /bin/sh -c ln -sf /dev/stdout /var/log/nginx/ 22 B&lt;missing&gt; 4 weeks ago /bin/sh -c apt-key adv --keyserver hkp://pgp. 58.46 MB&lt;missing&gt; 4 weeks ago /bin/sh -c #(nop) ENV NGINX_VERSION=1.11.5-1 0 B&lt;missing&gt; 4 weeks ago /bin/sh -c #(nop) MAINTAINER NGINX Docker Ma 0 B&lt;missing&gt; 4 weeks ago /bin/sh -c #(nop) CMD [&quot;/bin/bash&quot;] 0 B&lt;missing&gt; 4 weeks ago /bin/sh -c #(nop) ADD file:23aa4f893e3288698c 123 MB 新的镜像定制好后，我们可以来运行这个镜像。 1docker run --name web2 -d -p 81:80 nginx:v2 这里我们命名为新的服务为 web2，并且映射到 81 端口。访问 http://localhost:81 看到结果，其内容应该和之前修改后的 webserver 一样。 至此，我们第一次完成了定制镜像，使用的是 docker commit 命令，手动操作给旧的镜像添加了新的一层，形成新的镜像，对镜像多层存储应该有了更直观的感觉。 慎用 docker commit使用 docker commit 命令虽然可以比较直观的帮助理解镜像分层存储的概念，但是实际环境中并不会这样使用。 首先，如果仔细观察之前的 docker diff webserver 的结果，你会发现除了真正想要修改的 /usr/share/nginx/html/index.html 文件外，由于命令的执行，还有很多文件被改动或添加了。这还仅仅是最简单的操作，如果是安装软件包、编译构建，那会有大量的无关内容被添加进来，将会导致镜像极为臃肿。 此外，使用 docker commit 意味着所有对镜像的操作都是黑箱操作，生成的镜像也被称为 黑箱镜像，换句话说，就是除了制作镜像的人知道执行过什么命令、怎么生成的镜像，别人根本无从得知。而且，即使是这个制作镜像的人，过一段时间后也无法记清具体的操作。这种黑箱镜像的维护工作是非常痛苦的。 而且，回顾之前提及的镜像所使用的分层存储的概念，除当前层外，之前的每一层都是不会发生改变的，换句话说，任何修改的结果仅仅是在当前层进行标记、添加、修改，而不会改动上一层。如果使用 docker commit 制作镜像，以及后期修改的话，每一次修改都会让镜像更加臃肿一次，所删除的上一层的东西并不会丢失，会一直如影随形的跟着这个镜像，即使根本无法访问到。这会让镜像更加臃肿。 5 使用 Dockerfile 定制镜像从刚才的 docker commit 的学习中，我们可以了解到，镜像的定制实际上就是定制每一层所添加的配置、文件。如果我们可以把每一层修改、安装、构建、操作的命令都写入一个脚本，用这个脚本来构建、定制镜像，那么之前提及的无法重复的问题、镜像构建透明性的问题、体积的问题就都会解决。这个脚本就是 Dockerfile。 Dockerfile 是一个文本文件，其内包含了一条条的 指令(Instruction)，每一条指令构建一层，因此每一条指令的内容，就是描述该层应当如何构建。 还以之前定制 nginx 镜像为例，这次我们使用 Dockerfile 来定制。 在一个空白目录中，建立一个文本文件，并命名为 Dockerfile： 123$ mkdir mynginx$ cd mynginx$ touch Dockerfile 其内容为： 12FROM nginxRUN echo &#x27;&lt;h1&gt;Hello, Docker!&lt;/h1&gt;&#x27; &gt; /usr/share/nginx/html/index.html 这个 Dockerfile 很简单，一共就两行。涉及到了两条指令，FROM 和 RUN。 FROM 指定基础镜像所谓定制镜像，那一定是以一个镜像为基础，在其上进行定制。就像我们之前运行了一个 nginx 镜像的容器，再进行修改一样，基础镜像是必须指定的。而 FROM 就是指定 基础镜像，因此一个 Dockerfile 中 FROM 是必备的指令，并且必须是第一条指令。 在 Docker Hub 上有非常多的高质量的官方镜像，有可以直接拿来使用的服务类的镜像，如 nginx、redis、mongo、mysql、httpd、php、tomcat 等；也有一些方便开发、构建、运行各种语言应用的镜像，如 node、openjdk、python、ruby、golang 等。可以在其中寻找一个最符合我们最终目标的镜像为基础镜像进行定制。 如果没有找到对应服务的镜像，官方镜像中还提供了一些更为基础的操作系统镜像，如 ubuntu、debian、centos、fedora、alpine 等，这些操作系统的软件库为我们提供了更广阔的扩展空间。 除了选择现有镜像为基础镜像外，Docker 还存在一个特殊的镜像，名为 scratch。这个镜像是虚拟的概念，并不实际存在，它表示一个空白的镜像。 12FROM scratch... 如果你以 scratch 为基础镜像的话，意味着你不以任何镜像为基础，接下来所写的指令将作为镜像第一层开始存在。 不以任何系统为基础，直接将可执行文件复制进镜像的做法并不罕见，对于 Linux 下静态编译的程序来说，并不需要有操作系统提供运行时支持，所需的一切库都已经在可执行文件里了，因此直接 FROM scratch 会让镜像体积更加小巧。使用 Go 语言 开发的应用很多会使用这种方式来制作镜像，这也是有人认为 Go 是特别适合容器微服务架构的语言的原因之一。 RUN 执行命令RUN 指令是用来执行命令行命令的。由于命令行的强大能力，RUN 指令在定制镜像时是最常用的指令之一。其格式有两种： shell 格式：RUN &lt;命令&gt;，就像直接在命令行中输入的命令一样。刚才写的 Dockerfile 中的 RUN 指令就是这种格式。 1RUN echo &#x27;&lt;h1&gt;Hello, Docker!&lt;/h1&gt;&#x27; &gt; /usr/share/nginx/html/index.html exec 格式：RUN [&quot;可执行文件&quot;, &quot;参数1&quot;, &quot;参数2&quot;]，这更像是函数调用中的格式。 既然 RUN 就像 Shell 脚本一样可以执行命令，那么我们是否就可以像 Shell 脚本一样把每个命令对应一个 RUN 呢？比如这样： 123456789FROM debian:stretchRUN apt-get updateRUN apt-get install -y gcc libc6-dev make wgetRUN wget -O redis.tar.gz &quot;http://download.redis.io/releases/redis-5.0.3.tar.gz&quot;RUN mkdir -p /usr/src/redisRUN tar -xzf redis.tar.gz -C /usr/src/redis --strip-components=1RUN make -C /usr/src/redisRUN make -C /usr/src/redis install 之前说过，Dockerfile 中每一个指令都会建立一层，RUN 也不例外。每一个 RUN 的行为，就和刚才我们手工建立镜像的过程一样：新建立一层，在其上执行这些命令，执行结束后，commit 这一层的修改，构成新的镜像。 而上面的这种写法，创建了 7 层镜像。这是完全没有意义的，而且很多运行时不需要的东西，都被装进了镜像里，比如编译环境、更新的软件包等等。结果就是产生非常臃肿、非常多层的镜像，不仅仅增加了构建部署的时间，也很容易出错。 这是很多初学 Docker 的人常犯的一个错误。 Union FS 是有最大层数限制的，比如 AUFS，曾经是最大不得超过 42 层，现在是不得超过 127 层。 上面的 Dockerfile 正确的写法应该是这样： 1234567891011121314FROM debian:stretchRUN set -x; buildDeps=&#x27;gcc libc6-dev make wget&#x27; \\ &amp;&amp; apt-get update \\ &amp;&amp; apt-get install -y $buildDeps \\ &amp;&amp; wget -O redis.tar.gz &quot;http://download.redis.io/releases/redis-5.0.3.tar.gz&quot; \\ &amp;&amp; mkdir -p /usr/src/redis \\ &amp;&amp; tar -xzf redis.tar.gz -C /usr/src/redis --strip-components=1 \\ &amp;&amp; make -C /usr/src/redis \\ &amp;&amp; make -C /usr/src/redis install \\ &amp;&amp; rm -rf /var/lib/apt/lists/* \\ &amp;&amp; rm redis.tar.gz \\ &amp;&amp; rm -r /usr/src/redis \\ &amp;&amp; apt-get purge -y --auto-remove $buildDeps 首先，之前所有的命令只有一个目的，就是编译、安装 redis 可执行文件。因此没有必要建立很多层，这只是一层的事情。因此，这里没有使用很多个 RUN 一一对应不同的命令，而是仅仅使用一个 RUN 指令，并使用 &amp;&amp; 将各个所需命令串联起来。将之前的 7 层，简化为了 1 层。在撰写 Dockerfile 的时候，要经常提醒自己，这并不是在写 Shell 脚本，而是在定义每一层该如何构建。 并且，这里为了格式化还进行了换行。Dockerfile 支持 Shell 类的行尾添加 \\ 的命令换行方式，以及行首 # 进行注释的格式。良好的格式，比如换行、缩进、注释等，会让维护、排障更为容易，这是一个比较好的习惯。 此外，还可以看到这一组命令的最后添加了清理工作的命令，删除了为了编译构建所需要的软件，清理了所有下载、展开的文件，并且还清理了 apt 缓存文件。这是很重要的一步，我们之前说过，镜像是多层存储，每一层的东西并不会在下一层被删除，会一直跟随着镜像。因此镜像构建时，一定要确保每一层只添加真正需要添加的东西，任何无关的东西都应该清理掉。 很多人初学 Docker 制作出了很臃肿的镜像的原因之一，就是忘记了每一层构建的最后一定要清理掉无关文件。 构建镜像好了，让我们再回到之前定制的 nginx 镜像的 Dockerfile 来。现在我们明白了这个 Dockerfile 的内容，那么让我们来构建这个镜像吧。 在 Dockerfile 文件所在目录执行： 123456789$ docker build -t nginx:v3 .Sending build context to Docker daemon 2.048 kBStep 1 : FROM nginx ---&gt; e43d811ce2f4Step 2 : RUN echo &#x27;&lt;h1&gt;Hello, Docker!&lt;/h1&gt;&#x27; &gt; /usr/share/nginx/html/index.html ---&gt; Running in 9cdc27646c7b ---&gt; 44aa4490ce2cRemoving intermediate container 9cdc27646c7bSuccessfully built 44aa4490ce2c 从命令的输出结果中，我们可以清晰的看到镜像的构建过程。在 Step 2 中，如同我们之前所说的那样，RUN 指令启动了一个容器 9cdc27646c7b，执行了所要求的命令，并最后提交了这一层 44aa4490ce2c，随后删除了所用到的这个容器 9cdc27646c7b。 这里我们使用了 docker build 命令进行镜像构建。其格式为： 1docker build [选项] &lt;上下文路径/URL/-&gt; 在这里我们指定了最终镜像的名称 -t nginx:v3，构建成功后，我们可以像之前运行 nginx:v2 那样来运行这个镜像，其结果会和 nginx:v2 一样。 镜像构建上下文（Context）如果注意，会看到 docker build 命令最后有一个 .。. 表示当前目录，而 Dockerfile 就在当前目录，因此不少初学者以为这个路径是在指定 Dockerfile 所在路径，这么理解其实是不准确的。如果对应上面的命令格式，你可能会发现，这是在指定 上下文路径。那么什么是上下文呢？ 首先我们要理解 docker build 的工作原理。Docker 在运行时分为 Docker 引擎（也就是服务端守护进程）和客户端工具。Docker 的引擎提供了一组 REST API，被称为 Docker Remote API，而如 docker 命令这样的客户端工具，则是通过这组 API 与 Docker 引擎交互，从而完成各种功能。因此，虽然表面上我们好像是在本机执行各种 docker 功能，但实际上，一切都是使用的远程调用形式在服务端（Docker 引擎）完成。也因为这种 C&#x2F;S 设计，让我们操作远程服务器的 Docker 引擎变得轻而易举。 当我们进行镜像构建的时候，并非所有定制都会通过 RUN 指令完成，经常会需要将一些本地文件复制进镜像，比如通过 COPY 指令、ADD 指令等。而 docker build 命令构建镜像，其实并非在本地构建，而是在服务端，也就是 Docker 引擎中构建的。那么在这种客户端&#x2F;服务端的架构中，如何才能让服务端获得本地文件呢？ 这就引入了上下文的概念。当构建的时候，用户会指定构建镜像上下文的路径，docker build 命令得知这个路径后，会将路径下的所有内容打包，然后上传给 Docker 引擎。这样 Docker 引擎收到这个上下文包后，展开就会获得构建镜像所需的一切文件。 如果在 Dockerfile 中这么写： 1COPY ./package.json /app/ 这并不是要复制执行 docker build 命令所在的目录下的 package.json，也不是复制 Dockerfile 所在目录下的 package.json，而是复制 上下文（context） 目录下的 package.json。 因此，COPY 这类指令中的源文件的路径都是相对路径。这也是初学者经常会问的为什么 COPY ../package.json /app 或者 COPY /opt/xxxx /app 无法工作的原因，因为这些路径已经超出了上下文的范围，Docker 引擎无法获得这些位置的文件。如果真的需要那些文件，应该将它们复制到上下文目录中去。 现在就可以理解刚才的命令 docker build -t nginx:v3 . 中的这个 .，实际上是在指定上下文的目录，docker build 命令会将该目录下的内容打包交给 Docker 引擎以帮助构建镜像。 如果观察 docker build 输出，我们其实已经看到了这个发送上下文的过程： 123$ docker build -t nginx:v3 .Sending build context to Docker daemon 2.048 kB... 理解构建上下文对于镜像构建是很重要的，避免犯一些不应该的错误。比如有些初学者在发现 COPY /opt/xxxx /app 不工作后，于是干脆将 Dockerfile 放到了硬盘根目录去构建，结果发现 docker build 执行后，在发送一个几十 GB 的东西，极为缓慢而且很容易构建失败。那是因为这种做法是在让 docker build 打包整个硬盘，这显然是使用错误。 一般来说，应该会将 Dockerfile 置于一个空目录下，或者项目根目录下。如果该目录下没有所需文件，那么应该把所需文件复制一份过来。如果目录下有些东西确实不希望构建时传给 Docker 引擎，那么可以用 .gitignore 一样的语法写一个 .dockerignore，该文件是用于剔除不需要作为上下文传递给 Docker 引擎的。 那么为什么会有人误以为 . 是指定 Dockerfile 所在目录呢？这是因为在默认情况下，如果不额外指定 Dockerfile 的话，会将上下文目录下的名为 Dockerfile 的文件作为 Dockerfile。 这只是默认行为，实际上 Dockerfile 的文件名并不要求必须为 Dockerfile，而且并不要求必须位于上下文目录中，比如**可以用 -f ../Dockerfile.php 参数指定某个文件作为 Dockerfile**。 当然，一般大家习惯性的会使用默认的文件名 Dockerfile，以及会将其置于镜像构建上下文目录中。 其它 docker build 的用法直接用 Git repo 进行构建或许你已经注意到了，docker build 还支持从 URL 构建，比如可以直接从 Git repo 中构建： 1234567891011121314# $env:DOCKER_BUILDKIT=0# export DOCKER_BUILDKIT=0$ docker build -t hello-world https://github.com/docker-library/hello-world.git#master:amd64/hello-worldStep 1/3 : FROM scratch ---&gt;Step 2/3 : COPY hello / ---&gt; ac779757d46eStep 3/3 : CMD [&quot;/hello&quot;] ---&gt; Running in d2a513a760edRemoving intermediate container d2a513a760ed ---&gt; 038ad4142d2bSuccessfully built 038ad4142d2b 这行命令指定了构建所需的 Git repo，并且指定分支为 master，构建目录为 /amd64/hello-world/，然后 Docker 就会自己去 git clone 这个项目、切换到指定分支、并进入到指定目录后开始构建。 用给定的 tar 压缩包构建1$ docker build http://server/context.tar.gz 如果所给出的 URL 不是个 Git repo，而是个 tar 压缩包，那么 Docker 引擎会下载这个包，并自动解压缩，以其作为上下文，开始构建。 从标准输入中读取 Dockerfile 进行构建1docker build - &lt; Dockerfile 或 1cat Dockerfile | docker build - 如果标准输入传入的是文本文件，则将其视为 Dockerfile，并开始构建。这种形式由于直接从标准输入中读取 Dockerfile 的内容，它没有上下文，因此不可以像其他方法那样可以将本地文件 COPY 进镜像之类的事情。 从标准输入中读取上下文压缩包进行构建1$ docker build - &lt; context.tar.gz 如果发现标准输入的文件格式是 gzip、bzip2 以及 xz 的话，将会使其为上下文压缩包，直接将其展开，将里面视为上下文，并开始构建。 6 Dockerfile 指令详解我们已经介绍了 FROM，RUN，还提及了 COPY, ADD，其实 Dockerfile 功能很强大，它提供了十多个指令。下面我们继续讲解其他的指令。 Docker 镜像由多个只读层组成，每个层都对应 Dockerfile 中的一个指令。以下这些 Dockerfile 指令将会创建新的层： RUN：每个 RUN 命令都会在镜像中创建一个新的层。由于这让镜像变得较大，因此常常会看到多个 RUN 命令链在一起以减少创建的层数。 COPY：COPY 指令会将文件或目录从 Docker 构建上下文复制到新的一层上，然后将它们添加到 Dockerfile 中所指定的路径。 ADD：就像 COPY 一样, ADD 指令会在一个新的层上复制文件，并对文件（如果是压缩的话，会进行自动解压缩）进行提取。另一方面，由于 ADD 命令有些额外的功能（比如能下载远程的 URLs），通常建议在文件复制时尽量使用 COPY。 WORKDIR：如果指定的工作目录不存在，这个命令将会创建该目录，并在新的层上创建。 VOLUME: 此命令也会在 Docker 镜像中创建新的层。 USER：此命令用于设定下一条 RUN, CMD, ENTRYPOINT 指令运行时的 UID。 LABEL：此命令添加元数据到镜像，创建一个新的层。 虽然 ENTRYPOINT 和 CMD 指定了容器的默认执行命令，但它们不会创建新的层，因为它们不会改变镜像的文件系统。同样，ENV 指令用来设置环境变量，也不会创建新的镜像层。 这就是 Dockerfile 中用来创建新层的主要指令, 使用合理可有效改进创建的 Docker 镜像的大小和构建时间。 6.1 COPY 复制文件格式： COPY [--chown=&lt;user&gt;:&lt;group&gt;] &lt;源路径&gt;... &lt;目标路径&gt; COPY [--chown=&lt;user&gt;:&lt;group&gt;] [&quot;&lt;源路径1&gt;&quot;,... &quot;&lt;目标路径&gt;&quot;] 和 RUN 指令一样，也有两种格式，一种类似于命令行，一种类似于函数调用。 COPY 指令将从构建上下文目录中 &lt;源路径&gt; 的文件&#x2F;目录复制到新的一层的镜像内的 &lt;目标路径&gt; 位置。比如： 1COPY package.json /usr/src/app/ &lt;源路径&gt; 可以是多个，甚至可以是通配符，其通配符规则要满足 Go 的 filepath.Match 规则，如： 12COPY hom* /mydir/COPY hom?.txt /mydir/ &lt;目标路径&gt; 可以是容器内的绝对路径，也可以是相对于工作目录的相对路径（工作目录可以用 WORKDIR 指令来指定）。目标路径不需要事先创建，如果目录不存在会在复制文件前先行创建缺失目录。 此外，还需要注意一点，使用 COPY 指令，源文件的各种元数据都会保留。比如读、写、执行权限、文件变更时间等。这个特性对于镜像定制很有用。特别是构建相关文件都在使用 Git 进行管理的时候。 在使用该指令的时候还可以加上 --chown=&lt;user&gt;:&lt;group&gt; 选项来改变文件的所属用户及所属组。 1234COPY --chown=55:mygroup files* /mydir/COPY --chown=bin files* /mydir/COPY --chown=1 files* /mydir/COPY --chown=10:11 files* /mydir/ 如果源路径为文件夹，复制的时候不是直接复制该文件夹，而是将文件夹中的内容复制到目标路径。 6.2 ADD 更高级的复制文件ADD 指令和 COPY 的格式和性质基本一致。但是在 COPY 基础上增加了一些功能。 比如 &lt;源路径&gt; 可以是一个 URL，这种情况下，Docker 引擎会试图去下载这个链接的文件放到 &lt;目标路径&gt; 去。下载后的文件权限自动设置为 600，如果这并不是想要的权限，那么还需要增加额外的一层 RUN 进行权限调整，另外，如果下载的是个压缩包，需要解压缩，也一样还需要额外的一层 RUN 指令进行解压缩。所以不如直接使用 RUN 指令，然后使用 wget 或者 curl 工具下载，处理权限、解压缩、然后清理无用文件更合理。因此，这个功能其实并不实用，而且不推荐使用。 如果 &lt;源路径&gt; 为一个 tar 压缩文件的话，压缩格式为 gzip, bzip2 以及 xz 的情况下，ADD 指令将会自动解压缩这个压缩文件到 &lt;目标路径&gt; 去。 在某些情况下，这个自动解压缩的功能非常有用，比如官方镜像 ubuntu 中： 123FROM scratchADD ubuntu-xenial-core-cloudimg-amd64-root.tar.gz /... 但在某些情况下，如果我们真的是希望复制个压缩文件进去，而不解压缩，这时就不可以使用 ADD 命令了。 在 Docker 官方的 Dockerfile 最佳实践文档 中要求，尽可能的使用 COPY，因为 COPY 的语义很明确，就是复制文件而已，而 ADD 则包含了更复杂的功能，其行为也不一定很清晰。最适合使用 ADD 的场合，就是所提及的需要自动解压缩的场合。 另外需要注意的是，ADD 指令会令镜像构建缓存失效，从而可能会令镜像构建变得比较缓慢。 因此在 COPY 和 ADD 指令中选择的时候，可以遵循这样的原则，所有的文件复制均使用 COPY 指令，仅在需要自动解压缩的场合使用 ADD。 在使用该指令的时候还可以加上 --chown=&lt;user&gt;:&lt;group&gt; 选项来改变文件的所属用户及所属组。 1234ADD --chown=55:mygroup files* /mydir/ADD --chown=bin files* /mydir/ADD --chown=1 files* /mydir/ADD --chown=10:11 files* /mydir/ 6.3 CMD 容器启动命令CMD 指令的格式和 RUN 相似，也是两种格式： shell 格式：CMD &lt;命令&gt; exec 格式：CMD [&quot;可执行文件&quot;, &quot;参数1&quot;, &quot;参数2&quot;...] 参数列表格式：CMD [&quot;参数1&quot;, &quot;参数2&quot;...]。在指定了 ENTRYPOINT 指令后，用 CMD 指定具体的参数。 之前介绍容器的时候曾经说过，Docker 不是虚拟机，容器就是进程。既然是进程，那么在启动容器的时候，需要指定所运行的程序及参数。**CMD 指令就是用于指定默认的容器主进程的启动命令的**。 在运行时可以指定新的命令来替代镜像设置中的这个默认命令，比如，ubuntu 镜像默认的 CMD 是 /bin/bash，如果我们直接 docker run -it ubuntu 的话，会直接进入 bash。我们也可以在运行时指定运行别的命令，如 docker run -it ubuntu cat /etc/os-release。这就是用 cat /etc/os-release 命令替换了默认的 /bin/bash 命令了，输出了系统版本信息。 在指令格式上，一般推荐使用 exec 格式，这类格式在解析时会被解析为 JSON 数组，因此一定要使用双引号 &quot;，而不要使用单引号。 如果使用 shell 格式的话，实际的命令会被包装为 sh -c 的参数的形式进行执行。比如： 1CMD echo $HOME 在实际执行中，会将其变更为： 1CMD [ &quot;sh&quot;, &quot;-c&quot;, &quot;echo $HOME&quot; ] 这就是为什么我们可以使用环境变量的原因，因为这些环境变量会被 shell 进行解析处理。 提到 CMD 就不得不提容器中应用在前台执行和后台执行的问题。这是初学者常出现的一个混淆。 Docker 不是虚拟机，容器中的应用都应该以前台执行，而不是像虚拟机、物理机里面那样，用 systemd 去启动后台服务，容器内没有后台服务的概念。 一些初学者将 CMD 写为： 1CMD service nginx start 然后发现容器执行后就立即退出了。甚至在容器内去使用 systemctl 命令结果却发现根本执行不了。这就是因为没有搞明白前台、后台的概念，没有区分容器和虚拟机的差异，依旧在以传统虚拟机的角度去理解容器。 对于容器而言，其启动程序就是容器应用进程，容器就是为了主进程而存在的，主进程退出，容器就失去了存在的意义，从而退出，其它辅助进程不是它需要关心的东西。 而使用 service nginx start 命令，则是希望 init 系统以后台守护进程的形式启动 nginx 服务。而刚才说了 CMD service nginx start 会被理解为 CMD [ &quot;sh&quot;, &quot;-c&quot;, &quot;service nginx start&quot;]，因此主进程实际上是 sh。那么当 service nginx start 命令结束后，sh 也就结束了，sh 作为主进程退出了，自然就会令容器退出。 正确的做法是直接执行 nginx 可执行文件，并且要求以前台形式运行。比如： 1CMD [&quot;nginx&quot;, &quot;-g&quot;, &quot;daemon off;&quot;] 6.4 ENTRYPOINT 入口点ENTRYPOINT 的格式和 RUN 指令格式一样，分为 exec 格式和 shell 格式。 ENTRYPOINT 的目的和 CMD 一样，都是在指定容器启动程序及参数。**ENTRYPOINT 在运行时也可以替代，不过比 CMD 要略显繁琐，需要通过 docker run 的参数 --entrypoint 来指定**。 当指定了 ENTRYPOINT 后，CMD 的含义就发生了改变，不再是直接的运行其命令，而是将 CMD 的内容作为参数传给 ENTRYPOINT 指令，换句话说实际执行时，将变为： 1&lt;ENTRYPOINT&gt; &quot;&lt;CMD&gt;&quot; 那么有了 CMD 后，为什么还要有 ENTRYPOINT 呢？这种 &lt;ENTRYPOINT&gt; &quot;&lt;CMD&gt;&quot; 有什么好处么？让我们来看几个场景。 场景一：让镜像变成像命令一样使用假设我们需要一个得知自己当前公网 IP 的镜像，那么可以先用 CMD 来实现： 12345FROM ubuntu:18.04RUN apt-get update \\ &amp;&amp; apt-get install -y curl \\ &amp;&amp; rm -rf /var/lib/apt/lists/*CMD [ &quot;curl&quot;, &quot;-s&quot;, &quot;http://myip.ipip.net&quot; ] 假如我们使用 docker build -t myip . 来构建镜像的话，如果我们需要查询当前公网 IP，只需要执行： 12$ docker run myip当前 IP：61.148.226.66 来自：北京市 联通 嗯，这么看起来好像可以直接把镜像当做命令使用了，不过命令总有参数，如果我们希望加参数呢？比如从上面的 CMD 中可以看到实质的命令是 curl，那么如果我们希望显示 HTTP 头信息，就需要加上 -i 参数。那么我们可以直接加 -i 参数给 docker run myip 么？ 12$ docker run myip -idocker: Error response from daemon: invalid header field value &quot;oci runtime error: container_linux.go:247: starting container process caused \\&quot;exec: \\\\\\&quot;-i\\\\\\&quot;: executable file not found in $PATH\\&quot;\\n&quot;. 我们可以看到可执行文件找不到的报错，executable file not found。之前我们说过，跟在镜像名后面的是 command，运行时会替换 CMD 的默认值。因此这里的 -i 替换了原来的 CMD，而不是添加在原来的 curl -s http://myip.ipip.net 后面。而 -i 根本不是命令，所以自然找不到。 那么如果我们希望加入 -i 这参数，我们就必须重新完整的输入这个命令： 1$ docker run myip curl -s http://myip.ipip.net -i 这显然不是很好的解决方案，而使用 ENTRYPOINT 就可以解决这个问题。现在我们重新用 ENTRYPOINT 来实现这个镜像： 12345FROM ubuntu:18.04RUN apt-get update \\ &amp;&amp; apt-get install -y curl \\ &amp;&amp; rm -rf /var/lib/apt/lists/*ENTRYPOINT [ &quot;curl&quot;, &quot;-s&quot;, &quot;http://myip.ipip.net&quot; ] 这次我们再来尝试直接使用 docker run myip -i： 123456789101112131415161718$ docker run myip当前 IP：61.148.226.66 来自：北京市 联通$ docker run myip -iHTTP/1.1 200 OKServer: nginx/1.8.0Date: Tue, 22 Nov 2016 05:12:40 GMTContent-Type: text/html; charset=UTF-8Vary: Accept-EncodingX-Powered-By: PHP/5.6.24-1~dotdeb+7.1X-Cache: MISS from cache-2X-Cache-Lookup: MISS from cache-2:80X-Cache: MISS from proxy-2_6Transfer-Encoding: chunkedVia: 1.1 cache-2:80, 1.1 proxy-2_6:8006Connection: keep-alive当前 IP：61.148.226.66 来自：北京市 联通 可以看到，这次成功了。这是因为当存在 ENTRYPOINT 后，CMD 的内容将会作为参数传给 ENTRYPOINT，而这里 -i 就是新的 CMD，因此会作为参数传给 curl，从而达到了我们预期的效果。 场景二：应用运行前的准备工作启动容器就是启动主进程，但有些时候，启动主进程前，需要一些准备工作。 比如 mysql 类的数据库，可能需要一些数据库配置、初始化的工作，这些工作要在最终的 mysql 服务器运行之前解决。 此外，可能希望避免使用 root 用户去启动服务，从而提高安全性，而在启动服务前还需要以 root 身份执行一些必要的准备工作，最后切换到服务用户身份启动服务。或者除了服务外，其它命令依旧可以使用 root 身份执行，方便调试等。 这些准备工作是和容器 CMD 无关的，无论 CMD 为什么，都需要事先进行一个预处理的工作。这种情况下，可以写一个脚本，然后放入 ENTRYPOINT 中去执行，而这个脚本会将接到的参数（也就是 &lt;CMD&gt;）作为命令，在脚本最后执行。比如官方镜像 redis 中就是这么做的： 12345678FROM alpine:3.4...RUN addgroup -S redis &amp;&amp; adduser -S -G redis redis...ENTRYPOINT [&quot;docker-entrypoint.sh&quot;]EXPOSE 6379CMD [ &quot;redis-server&quot; ] 可以看到其中为了 redis 服务创建了 redis 用户，并在最后指定了 ENTRYPOINT 为 docker-entrypoint.sh 脚本。 123456789#!/bin/sh...# allow the container to be started with `--user`if [ &quot;$1&quot; = &#x27;redis-server&#x27; -a &quot;$(id -u)&quot; = &#x27;0&#x27; ]; then find . \\! -user redis -exec chown redis &#x27;&#123;&#125;&#x27; + exec gosu redis &quot;$0&quot; &quot;$@&quot;fiexec &quot;$@&quot; 该脚本的内容就是根据 CMD 的内容来判断，如果是 redis-server 的话，则切换到 redis 用户身份启动服务器，否则依旧使用 root 身份执行。比如： 12$ docker run -it redis iduid=0(root) gid=0(root) groups=0(root) 6.5 ENV 设置环境变量格式有两种： ENV &lt;key&gt; &lt;value&gt; ENV &lt;key1&gt;=&lt;value1&gt; &lt;key2&gt;=&lt;value2&gt;... 这个指令很简单，就是设置环境变量而已，无论是后面的其它指令，如 RUN，还是运行时的应用，都可以直接使用这里定义的环境变量。 12ENV VERSION=1.0 DEBUG=on \\ NAME=&quot;Happy Feet&quot; 这个例子中演示了如何换行，以及对含有空格的值用双引号括起来的办法，这和 Shell 下的行为是一致的。 定义了环境变量，那么在后续的指令中，就可以使用这个环境变量。比如在官方 node 镜像 Dockerfile 中，就有类似这样的代码： 123456789ENV NODE_VERSION 7.2.0RUN curl -SLO &quot;https://nodejs.org/dist/v$NODE_VERSION/node-v$NODE_VERSION-linux-x64.tar.xz&quot; \\ &amp;&amp; curl -SLO &quot;https://nodejs.org/dist/v$NODE_VERSION/SHASUMS256.txt.asc&quot; \\ &amp;&amp; gpg --batch --decrypt --output SHASUMS256.txt SHASUMS256.txt.asc \\ &amp;&amp; grep &quot; node-v$NODE_VERSION-linux-x64.tar.xz\\$&quot; SHASUMS256.txt | sha256sum -c - \\ &amp;&amp; tar -xJf &quot;node-v$NODE_VERSION-linux-x64.tar.xz&quot; -C /usr/local --strip-components=1 \\ &amp;&amp; rm &quot;node-v$NODE_VERSION-linux-x64.tar.xz&quot; SHASUMS256.txt.asc SHASUMS256.txt \\ &amp;&amp; ln -s /usr/local/bin/node /usr/local/bin/nodejs 在这里先定义了环境变量 NODE_VERSION，其后的 RUN 这层里，多次使用 $NODE_VERSION 来进行操作定制。可以看到，将来升级镜像构建版本的时候，只需要更新 7.2.0 即可，Dockerfile 构建维护变得更轻松了。 下列指令可以支持环境变量展开： ADD、COPY、ENV、EXPOSE、FROM、LABEL、USER、WORKDIR、VOLUME、STOPSIGNAL、ONBUILD、RUN。 可以从这个指令列表里感觉到，环境变量可以使用的地方很多，很强大。通过环境变量，我们可以让一份 Dockerfile 制作更多的镜像，只需使用不同的环境变量即可。 6.6 ARG 构建参数格式：ARG &lt;参数名&gt;[=&lt;默认值&gt;] 构建参数和 ENV 的效果一样，都是设置环境变量。所不同的是，**ARG 所设置的构建环境的环境变量，在将来容器运行时是不会存在这些环境变量的。但是不要因此就使用 ARG 保存密码之类的信息，因为 docker history 还是可以看到所有值的**。 Dockerfile 中的 ARG 指令是定义参数名称，以及定义其默认值。该默认值可以在构建命令 docker build 中用 --build-arg &lt;参数名&gt;=&lt;值&gt; 来覆盖。 灵活的使用 ARG 指令，能够在不修改 Dockerfile 的情况下，构建出不同的镜像。 ARG 指令有生效范围，如果在 FROM 指令之前指定，那么只能用于 FROM 指令中。 12345ARG DOCKER_USERNAME=libraryFROM $&#123;DOCKER_USERNAME&#125;/alpineRUN set -x ; echo $&#123;DOCKER_USERNAME&#125; 使用上述 Dockerfile 会发现无法输出 $&#123;DOCKER_USERNAME&#125; 变量的值，要想正常输出，你必须在 FROM 之后再次指定 ARG 123456789# 只在 FROM 中生效ARG DOCKER_USERNAME=libraryFROM $&#123;DOCKER_USERNAME&#125;/alpine# 要想在 FROM 之后使用，必须再次指定ARG DOCKER_USERNAME=libraryRUN set -x ; echo $&#123;DOCKER_USERNAME&#125; 对于多阶段构建，尤其要注意这个问题： 12345678910# 这个变量在每个 FROM 中都生效ARG DOCKER_USERNAME=libraryFROM $&#123;DOCKER_USERNAME&#125;/alpineRUN set -x ; echo 1FROM $&#123;DOCKER_USERNAME&#125;/alpineRUN set -x ; echo 2 对于上述 Dockerfile 两个 FROM 指令都可以使用 $&#123;DOCKER_USERNAME&#125;，对于在各个阶段中使用的变量都必须在每个阶段分别指定： 123456789101112131415ARG DOCKER_USERNAME=libraryFROM $&#123;DOCKER_USERNAME&#125;/alpine# 在FROM 之后使用变量，必须在每个阶段分别指定ARG DOCKER_USERNAME=libraryRUN set -x ; echo $&#123;DOCKER_USERNAME&#125;FROM $&#123;DOCKER_USERNAME&#125;/alpine# 在FROM 之后使用变量，必须在每个阶段分别指定ARG DOCKER_USERNAME=libraryRUN set -x ; echo $&#123;DOCKER_USERNAME&#125; 6.7 VOLUME 定义匿名卷格式为： VOLUME [&quot;&lt;路径1&gt;&quot;, &quot;&lt;路径2&gt;&quot;...] VOLUME &lt;路径&gt; 之前我们说过，容器运行时应该尽量保持容器存储层不发生写操作，对于数据库类需要保存动态数据的应用，其数据库文件应该保存于卷(volume)中，后面的章节我们会进一步介绍 Docker 卷的概念。为了防止运行时用户忘记将动态文件所保存目录挂载为卷，在 Dockerfile 中，我们可以事先指定某些目录挂载为匿名卷，这样在运行时如果用户不指定挂载，其应用也可以正常运行，不会向容器存储层写入大量数据。 1VOLUME /data 这里的 /data 目录就会在容器运行时自动挂载为匿名卷，任何向 /data 中写入的信息都不会记录进容器存储层，从而保证了容器存储层的无状态化。当然，运行容器时可以覆盖这个挂载设置。比如： 1$ docker run -d -v mydata:/data xxxx 在这行命令中，就使用了 mydata 这个命名卷挂载到了 /data 这个位置，替代了 Dockerfile 中定义的匿名卷的挂载配置。 当你在 Dockerfile 中使用 VOLUME 指令或者在运行容器时通过 -v 标记创建一个匿名卷时，Docker 会自动在宿主机上为该卷分配一个存储位置。这个位置是由 Docker 的配置和宿主机操作系统决定的，并且通常不是直接由用户指定的。对于大多数 Linux 安装和 Docker 的默认配置，匿名卷的存储位置是在宿主机的 /var/lib/docker/volumes/ 目录下。 在该目录下，每个卷都会有一个唯一的标识符作为其目录名。在这些卷的目录内部，你会找到两个子目录：_data，其中存储的是卷的数据，以及 mounts.json，其中包含有关该卷的元数据。 例如，若你通过 Docker 命令创建了一个匿名卷，你可能会在 /var/lib/docker/volumes/ 下找到一个类似于 /var/lib/docker/volumes/2f4a8c1d591f396c2b47e6b42dfea9184292ab09e78a0b3e7661e8a3ef4b0c82/ 的目录，_data 文件夹就在这个目录里。 需要注意的是，不同的 Docker 配置和不同的宿主机操作系统可能会改变这个默认的存储位置。此外，出于安全和维护的目的，不鼓励直接手动操作这些文件目录。对于大多数用例，建议使用 Docker 命令来管理和操作卷。 若需要查询卷的具体存储位置，可以使用 Docker 卷命令，如 docker volume inspect [VOLUME_NAME]，它会显示卷的详细信息，包括其在宿主机上的确切存储路径。对于匿名卷，虽然没有友好的名称，但每个都有一个唯一的ID，可以通过列出所有卷的信息 docker volume ls 来找到，然后用这个 ID 进行检查。 6.8 EXPOSE 暴露端口格式为 EXPOSE &lt;端口1&gt; [&lt;端口2&gt;...]。 **EXPOSE 指令是声明容器运行时提供服务的端口，这只是一个声明，在容器运行时并不会因为这个声明应用就会开启这个端口的服务**。在 Dockerfile 中写入这样的声明有两个好处，一个是帮助镜像使用者理解这个镜像服务的守护端口，以方便配置映射；另一个用处则是在运行时使用随机端口映射时，也就是 docker run -P 时，会自动随机映射 EXPOSE 的端口。 要将 EXPOSE 和在运行时使用 -p &lt;宿主端口&gt;:&lt;容器端口&gt; 区分开来。-p，是映射宿主端口和容器端口，换句话说，就是将容器的对应端口服务公开给外界访问，而 EXPOSE 仅仅是声明容器打算使用什么端口而已，并不会自动在宿主进行端口映射。 在 Docker 中，-p 和 -P 标志被用来将容器内部的端口映射到宿主机的端口上，但它们的工作方式存在一些区别： -p 标志: 使用 -p 标志时，你需要明确指定端口映射关系。这包括宿主机的端口和容器内部的端口，格式为 -p 宿主机端口:容器端口。 例如，docker run -p 8080:80 nginx 会将容器内部的 80 端口映射到宿主机的 8080 端口。 你可以精确控制端口映射的过程，包括选择特定的宿主机端口。 -P 标志: 使用 -P 标志时，Docker 会自动将容器内部所有通过 EXPOSE 指令暴露出来的端口映射到宿主机的一个随机高端口（通常在 49153 到 65535 之间）。 这意味着你无法控制端口映射到宿主机的哪个端口上，端口号是由 Docker 动态分配的。 例如，docker run -P nginx 假设你的 nginx 镜像通过 EXPOSE 指令暴露了 80 端口，Docker 将自动选择一个端口将其映射到宿主机。 6.9 WORKDIR 指定工作目录格式为 WORKDIR &lt;工作目录路径&gt;。 使用 WORKDIR 指令可以来指定工作目录（或者称为当前目录），以后各层的当前目录就被改为指定的目录，如该目录不存在，WORKDIR 会帮你建立目录。 之前提到一些初学者常犯的错误是把 Dockerfile 等同于 Shell 脚本来书写，这种错误的理解还可能会导致出现下面这样的错误： 12RUN cd /appRUN echo &quot;hello&quot; &gt; world.txt 如果将这个 Dockerfile 进行构建镜像运行后，会发现找不到 /app/world.txt 文件，或者其内容不是 hello。原因其实很简单，在 Shell 中，连续两行是同一个进程执行环境，因此前一个命令修改的内存状态，会直接影响后一个命令；而在 Dockerfile 中，这两行 RUN 命令的执行环境根本不同，是两个完全不同的容器。这就是对 Dockerfile 构建分层存储的概念不了解所导致的错误。 之前说过每一个 RUN 都是启动一个容器、执行命令、然后提交存储层文件变更。第一层 RUN cd /app 的执行仅仅是当前进程的工作目录变更，一个内存上的变化而已，其结果不会造成任何文件变更。而到第二层的时候，启动的是一个全新的容器，跟第一层的容器更完全没关系，自然不可能继承前一层构建过程中的内存变化。 因此如果需要改变以后各层的工作目录的位置，那么应该使用 WORKDIR 指令。 123WORKDIR /appRUN echo &quot;hello&quot; &gt; world.txt 如果你的 WORKDIR 指令使用的相对路径，那么所切换的路径与之前的 WORKDIR 有关： 12345WORKDIR /aWORKDIR bWORKDIR cRUN pwd RUN pwd 的工作目录为 /a/b/c。 6.10 USER 指定当前用户格式：USER &lt;用户名&gt;[:&lt;用户组&gt;] USER 指令和 WORKDIR 相似，都是改变环境状态并影响以后的层。WORKDIR 是改变工作目录，USER 则是改变之后层的执行 RUN, CMD 以及 ENTRYPOINT 这类命令的身份。 注意，USER 只是帮助你切换到指定用户而已，这个用户必须是事先建立好的，否则无法切换。 123RUN groupadd -r redis &amp;&amp; useradd -r -g redis redisUSER redisRUN [ &quot;redis-server&quot; ] 如果以 root 执行的脚本，在执行期间希望改变身份，比如希望以某个已经建立好的用户来运行某个服务进程，不要使用 su 或者 sudo，这些都需要比较麻烦的配置，而且在 TTY 缺失的环境下经常出错。建议使用 gosu。 12345678# 建立 redis 用户，并使用 gosu 换另一个用户执行命令RUN groupadd -r redis &amp;&amp; useradd -r -g redis redis# 下载 gosuRUN wget -O /usr/local/bin/gosu &quot;https://github.com/tianon/gosu/releases/download/1.12/gosu-amd64&quot; \\ &amp;&amp; chmod +x /usr/local/bin/gosu \\ &amp;&amp; gosu nobody true# 设置 CMD，并以另外的用户执行CMD [ &quot;exec&quot;, &quot;gosu&quot;, &quot;redis&quot;, &quot;redis-server&quot; ] 7 Dockerfile多阶段构建将所有的构建过程编包含在一个 Dockerfile 中，包括项目及其依赖库的编译、测试、打包等流程，这里可能会带来的一些问题： 镜像层次多，镜像体积较大，部署时间变长 源代码存在泄露的风险 事先在一个 Dockerfile 将项目及其依赖库编译测试打包好后，再将其拷贝到运行环境中，这种方式需要我们编写两个 Dockerfile 和一些编译脚本才能将其两个阶段自动整合起来，这种方式虽然可以很好地规避第一种方式存在的风险，但明显部署过程较复杂。 例如，编写 Dockerfile.build 文件 12345678910FROM golang:alpineRUN apk --no-cache add gitWORKDIR /go/src/github.com/go/helloworldCOPY app.go .RUN go get -d -v github.com/go-sql-driver/mysql \\ &amp;&amp; CGO_ENABLED=0 GOOS=linux go build -a -installsuffix cgo -o app . 编写 Dockerfile.copy 文件 123456789FROM alpine:latestRUN apk --no-cache add ca-certificatesWORKDIR /root/COPY app .CMD [&quot;./app&quot;] 新建 build.sh 12345678910111213#!/bin/shecho Building go/helloworld:builddocker build -t go/helloworld:build . -f Dockerfile.builddocker create --name extract go/helloworld:builddocker cp extract:/go/src/github.com/go/helloworld/app ./appdocker rm -f extractecho Building go/helloworld:2docker build --no-cache -t go/helloworld:2 . -f Dockerfile.copyrm ./app 现在运行脚本即可构建镜像 123$ chmod +x build.sh$ ./build.sh 对比两种方式生成的镜像大小 12345$ docker image lsREPOSITORY TAG IMAGE ID CREATED SIZEgo/helloworld 2 f7cf3465432c 22 seconds ago 6.47MBgo/helloworld 1 f55d3e16affc 2 minutes ago 295MB 使用多阶段构建为解决以上问题，Docker v17.05 开始支持多阶段构建 (multistage builds)。使用多阶段构建我们就可以很容易解决前面提到的问题，并且只需要编写一个 Dockerfile： 例如，编写 Dockerfile 文件 123456789101112131415161718192021FROM golang:alpine as builderRUN apk --no-cache add gitWORKDIR /go/src/github.com/go/helloworld/RUN go get -d -v github.com/go-sql-driver/mysqlCOPY app.go .RUN CGO_ENABLED=0 GOOS=linux go build -a -installsuffix cgo -o app .FROM alpine:latest as prodRUN apk --no-cache add ca-certificatesWORKDIR /root/COPY --from=0 /go/src/github.com/go/helloworld/app .CMD [&quot;./app&quot;] 构建镜像 1$ docker build -t go/helloworld:3 . 对比三个镜像大小 123456$ docker image lsREPOSITORY TAG IMAGE ID CREATED SIZEgo/helloworld 3 d6911ed9c846 7 seconds ago 6.47MBgo/helloworld 2 f7cf3465432c 22 seconds ago 6.47MBgo/helloworld 1 f55d3e16affc 2 minutes ago 295MB 很明显使用多阶段构建的镜像体积小，同时也完美解决了上边提到的问题。 只构建某一阶段的镜像我们可以使用 as 来为某一阶段命名，例如 1FROM golang:alpine as builder 例如当我们只想构建 builder 阶段的镜像时，增加 --target=builder 参数即可 1$ docker build --target builder -t username/imagename:tag . 构建时从其他镜像复制文件上面例子中我们使用 COPY --from=0 /go/src/github.com/go/helloworld/app . 从上一阶段的镜像中复制文件，我们也可以复制任意镜像中的文件。 1$ COPY --from=nginx:latest /etc/nginx/nginx.conf /nginx.conf","tags":[{"name":"docker","slug":"docker","permalink":"https://stu-yue.github.io/tags/docker/"},{"name":"linux","slug":"linux","permalink":"https://stu-yue.github.io/tags/linux/"}],"categories":[{"name":"tools","slug":"tools","permalink":"https://stu-yue.github.io/categories/tools/"},{"name":"docker","slug":"tools/docker","permalink":"https://stu-yue.github.io/categories/tools/docker/"}]},{"title":"Memo | Shell Basis","date":"2023-10-04T07:56:53.000Z","path":"jottings/languages/shell/memo_shell_basis/","text":"0 交互式登录 shell 和非登录 shell交互式登录 shell 和非登录 shell 是两种在 Linux 系统中使用 Shell 时的不同环境。它们有以下区别： 交互式登录 shell： 当用户通过登录界面（例如终端登录、SSH 远程登录）成功登录到系统时，系统会为用户启动一个交互式登录 shell。 交互式登录 shell 会读取系统的登录配置文件（如 /etc/profile 和 ~/.bash_profile）来执行初始化操作，设置环境变量、加载别名和执行其他登录时需要的配置。 它还会执行 ~/.bashrc 文件，以便加载用户自定义的 Shell 配置。 用户在交互式登录 shell 中执行的命令会被记录到历史记录文件（如 ~/.bash_history）中。 非登录 shell： 当用户已经登录到系统后，在当前 shell 中打开一个新的终端或运行脚本时，会启动一个非登录 shell。 非登录 shell 不会读取登录配置文件（如 /etc/profile 和 ~/.bash_profile），而是读取 ~/.bashrc 文件进行初始化。 它不会执行登录时需要的配置，只会加载用户自定义的 Shell 配置。 用户在非登录 shell 中执行的命令不会被记录到历史记录文件中，除非用户在 ~/.bashrc 中显式地指定。 大部分Linux发行版用~/.profile替换~/.bash_profile，~/.profile被所有shell读取，~\\.bash_profile仅被Bash读取； .profile在登录shell启动时，被读取和执行；.bashrc在非登录shell启动时执行； profile中会读取各自级别的bashrc； Shell启动时读取配置文件的顺序： /etc/profile /etc/profile.d/*.sh /etc/bash.bashrc ~/.profile 或 ~/.bash_login 或 ~/.bash_profile：个性化环境变量设置 ~/.bashrc ：别名、函数和其他个性化设置； 系统级 用户级 登录shell &#x2F;etc&#x2F;profile ~&#x2F;.profile | ~&#x2F;.bash_profile 非登录shell &#x2F;etc&#x2F;bash.bashrc ~&#x2F;.bashrc 登录式 shell 和非登录式 shell 的运行形式如下： 登录式 shell： 正常通过某终端登录的 shell。 su - username。 su -l username。 非登录式 shell： su username。 图形终端下打开的命令窗口。 自动执行的 shell 脚本。 **echo $0**：-bash登录shell；bash非登录； 1 结合Linux文件描述符理解重定向 输入输出重定向就是通过修改文件指针实现的。发生重定向时，文件描述符并没有改变，改变的是文件描述符对应的文件指针。 Shell对文件描述符的操作 分类 用法 说明 输出 n&gt;filename 以输出的方式打开文件 filename，并绑定到文件描述符 n。n 可以不写，默认为 1，也即标准输出文件。 输出 n&gt;&amp;m 用文件描述符 m 修改文件描述符 n，或者说用文件描述符 m 的内容覆盖文件描述符 n，结果就是 n 和 m 都代表了同一个文件，因为 n 和 m 的文件指针都指向了同一个文件。因为使用的是&gt;，所以 n 和 m 只能用作命令的输出文件。n 可以不写，默认为 1。 输出 n&gt;&amp;- 关闭文件描述符 n 及其代表的文件。n 可以不写，默认为 1。在shell中执行&gt;&amp;-命令会关闭文件描述符。具体来说，它会关闭标准输出（文件描述符1）和标准错误输出（文件描述符2），将它们都重定向到空设备（&#x2F;dev&#x2F;null）。 输出 &amp;&gt;filename 这个操作可以用于临时禁用命令的输出，以防止输出被显示或记录。将正确输出结果和错误信息全部重定向到 filename。 输入 n&lt;filename 以输入的方式打开文件 filename，并绑定到文件描述符 n。n 可以不写，默认为 0，也即标准输入文件。 输入 n&lt;&amp;m 类似于 n&gt;&amp;m，但是因为使用的是&lt;，所以 n 和 m 只能用作命令的输入文件。n 可以不写，默认为 0。 输入 n&lt;&amp;- 关闭文件描述符 n 及其代表的文件。n 可以不写，默认为 0。 输入和输出 n&lt;&gt;filename 同时以输入和输出的方式打开文件 filename，并绑定到文件描述符 n，相当于 n&gt;filename 和 n&lt;filename 的总和。。n 可以不写，默认为 0。 1echo &quot;C语言中文网&quot; 10&gt;log.txt &gt;&amp;10 先执行10&gt;log.txt，打开 log.txt，并给它分配文件描述符 10；接着执行&gt;&amp;10，用文件描述符 10 来修改文件描述符 1（对于&gt;，省略不写的话默认为 1），让 1 和 10 都指向 log.txt 文件，最终的结果是向 log.txt 文件中输出内容。 这条语句其实等价于echo &quot;C语言中文网&quot; &gt;log.txt，我之所以写得这么绕，是为了让大家理解各种操作符的用法。 文件描述符 10 只用了一次，我们在末尾最好将它关闭，这是一个好习惯。 1echo &quot;C语言中文网&quot; 10&gt;log.txt &gt;&amp;10 10&gt;&amp;- 2 管道(pipeline, | )pipeline ( | ) 是 UNIX 系统，基础且重要的观念。连结上个指令的标准输出，做为下个指令的标准输入。 1who | wc -l 善用这个观念，对精简 script 有相当的帮助。 3 后台工作( &amp; )单一个&amp; 符号，且放在完整指令列的最后端，即表示将该指令列放入后台中工作。 nohup重定向到指定文件 1nohup ./run &gt;log 2&gt;&amp;1 &amp; 4 快速入门 #!: Shebang line，告诉系统脚本用什么解释器执行； .sh作为可执行程序时，加上第一行； sh （默认Shell解释器运行）、bash（bash解释器执行）； 5 基本语法（变量） 定义变量：变量名&#x3D;变量值，等号两侧不能有空格，变量名一般习惯用大写。 删除变量：unset 变量名 。 声明静态变量：readonly 变量名，静态变量不能unset。 使用变量：$变量名 **命令替换$()**：将命令返回值赋给变量；等价于反引号； 环境变量： export 变量名&#x3D;变量值，将 Shell 变量输出为环境变量； source 配置文件路径，让修改后的配置信息立即生效； echo $变量名，检查环境变量是否生效； 位置参数： $n ：$0 代表命令本身、$1-$9 代表第1到9个参数，10以上参数用花括号，如 ${10}。 $* ：命令行中所有参数，且把所有参数看成一个整体。 $@ ：命令行中所有参数，且把每个参数区分对待（推荐）。 $# ：所有参数个数。 $ 和 $@，在不被双引号包围时*，行为十分相似——都会将脚本的所有参数展开成一个由空格分隔的列表。在这种情况下，参数中包含的空格或特殊字符可能不会被正确处理，并可能会引起词语分割，例如： 12345678910// study.sh#!/bin/bashecho $@echo $*// run./study.sh one &quot;two two&quot; three// outputone two two three $ 和 $@，在被双引号包围时*， $@ 会将每个参数作为独立且被引号包围的字符串，保持其作为单独的元素，会正确处理包含空格&#x2F;特殊字符的参数； $* 会将所有参数连成一个单独的字符串，其中的参数由空格分隔，这种方式不能保证原有参数的准确界限，尤其是参数内部包含空格时。 预定义变量 $$：当前进程的PID； **$!**：后台运行的最后一个进程的PID； **$?**：最后一次执行命令的返回状态，0为成功； 运算式计算 $(())：$((2+3*2)) $(expr 2 + 3)： 条件判断 [ condition ]：前后都要有空格，非空返回0，0为true；否则为false； 12345if [ condition ]; then execelif [ condition ]; then execfi 测试参数 含义 str 字符串 str 非空 -z str 字符串 str 为空 -n str 字符串 str 的长度非零 str1=str2 str1!=str2 -e file 文件 file 存在 -f file 文件 file 存在且是一个普通文件 -d dir 文件 dir 存在且是一个目录 -r/w/x file 文件 file 存在且可读&#x2F;可写&#x2F;可执行 -s file 文件 file 存在且文件大小不为0 num1 -eq/ne/lt/le/ge/gt num2 [ ! condition ] [ cond1 -a cond2] [[ cond1 &amp;&amp; cond2]] [ cond1 -o cond2 ] [[ cond1 || cond2 ]] case分支 1234567891011case $var inc1) echo &quot;c1&quot; ;;c2) echo &quot;c2&quot; ;;*) echo &quot;default&quot; ;;esac for循环 123456789for var in (...); do echo $vardonefor i in &#123;1..20&#125;; do if [ $((i % 2)) -eq 0 ]; then echo $i; fi; done;# 每隔2个数打印一次for ((i=0; i&lt;=10; i+=2)); do echo $i; done;for i in $(seq 0 2 10); do echo $i; done; while循环 1234i=1while [ $i -le 20 ]; do echo $i; i=$((i + 1));done; 读取控制台输入 (read) -p：指定读取值时的提示符； 1read -p &quot;请输入一个数num=&quot; NUM; echo &quot;num=$NUM&quot; -t：指定读取值时等待秒数，如果没有指定时间内输入，不再等待； 1read -t 5 -p &quot;请5s内输入一个数num=&quot; NUM; echo &quot;num=$NUM&quot; 6 函数 系统函数： 自定义函数： 12345678910111213141516171819202122232425262728function_name() &#123; # Your commands here [return value] &#125;# examplecheck_file_exists() &#123; if [ -e &quot;$1&quot; ]; then echo &quot;File exists.&quot; return 0 else echo &quot;File does not exist.&quot; return 1 fi&#125;# 调用函数并传递文件名check_file_exists &quot;/path/to/your/file.txt&quot;# 获取函数返回值status=$?# 根据返回值判断文件是否存在if [ $status -eq 0 ]; then echo &quot;The check confirmed that the file exists.&quot;else echo &quot;The check confirmed that the file does NOT exist.&quot;fi 可以使用局部变量（声明和赋值应该在不同行），将位置参数赋予更有意义的名字： 123456789101112print_details() &#123; local name=$1 local age=$2 local job_title=$3 echo &quot;Name: $name&quot; echo &quot;Age: $age&quot; echo &quot;Job Title: $job_title&quot;&#125;# 调用函数并传递参数print_details &quot;Alice&quot; &quot;30&quot; &quot;Developer&quot;","tags":[{"name":"linux","slug":"linux","permalink":"https://stu-yue.github.io/tags/linux/"},{"name":"shell","slug":"shell","permalink":"https://stu-yue.github.io/tags/shell/"}],"categories":[{"name":"languages","slug":"languages","permalink":"https://stu-yue.github.io/categories/languages/"},{"name":"shell","slug":"languages/shell","permalink":"https://stu-yue.github.io/categories/languages/shell/"}]},{"title":"Memo | Tmux","date":"2023-10-04T07:56:53.000Z","path":"jottings/languages/shell/memo_tmux/","text":"What is tmux? A typical use of the command line is to open a terminal window (session), whose important feature is that window is connected to the process started in it (window closed, session ends, vice versa). Tmux, Terminal multiplexer, is the session and window “unbind” tool. It allows: simultaneous access to multiple sessions in a single window. (useful for running multiple terminal simultaneously) a new window to access an existing session; each session having multiple connection window (multiple people sharing sessions in real time) arbitrary vertical and horizontal splitting of windows; Basic conception: session: Basic Usage Start —tmux, Quit—exit/Ctrl-d, Prefix Key—Ctrl+b; A status bar is located at the bottom: 12[name/id] [list of ][0] 0:bash 1:test3* 2:test4- &quot;VM-16-17-ubuntu&quot; 15:28 04-Oct-23 Session Management New a session: tmux new -s &lt;session-name&gt;; 1tmux new -s sessionName -n window Split sessions: tmux detach/Ctrl+b d, after the command is executed, the current Tmux window exits, but the session and the processes inside it still run in the background; View all current tmux sessions: tmux ls 1230: 1 windows (created Tue Sep 19 20:42:24 2023)1: 1 windows (created Tue Oct 3 19:57:48 2023)test2: 1 windows (created Wed Oct 4 14:49:04 2023) (attached) Attach a session: tmux attach -t id/&lt;session-name&gt;; Kill a session: tmux kill-session -t id/&lt;session-name&gt;; Switch a session: tmux switch -t id/&lt;session-name&gt;; Rename a session: tmux rename-session -t id/&lt;session-name&gt; &lt;new-name&gt;; Shortcuts: Ctrl+bd: Split current session; Ctrl+b s list all session; Ctrl+b w list all windows; Ctrl+b $: rename current session; Simple workflow of tmux: new a session: tmux new -s my_session; run program in tmux window; Ctrl+b d splits the session; Attach the last session tmux attach-session -t my_session; Pane OperationTmux can split the window into panes, which can execute different commands. tmux splilt-window splits into vertical layout; tmux split-window -h splits into horizontal layout; tmux select-pane moves the cursor in different panes: 1234567891011# 光标切换到上方窗格$ tmux select-pane -U# 光标切换到下方窗格$ tmux select-pane -D# 光标切换到左边窗格$ tmux select-pane -L# 光标切换到右边窗格$ tmux select-pane -R tmux swap-pane exchanges the positions of panes: 12345# 当前窗格上移$ tmux swap-pane -U# 当前窗格下移$ tmux swap-pane -D Shortcuts in pane operations: 1234567891011121314Ctrl+b % ：划分左右两个窗格。Ctrl+b &quot; ：划分上下两个窗格。Ctrl+b &lt;arrow key&gt; ：光标切换到其他窗格。&lt;arrow key&gt;是指向要切换到的窗格的方向键，比如切换到下方窗格，就按方向键↓。Ctrl+b ; ：光标切换到上一个窗格。Ctrl+b o ：光标切换到下一个窗格。Ctrl+b &#123; ：当前窗格与上一个窗格交换位置。Ctrl+b &#125; ：当前窗格与下一个窗格交换位置。Ctrl+b Ctrl+o ：所有窗格向前移动一个位置，第一个窗格变成最后一个窗格。Ctrl+b Alt+o ：所有窗格向后移动一个位置，最后一个窗格变成第一个窗格。Ctrl+b x ：关闭当前窗格。Ctrl+b ! ：将当前窗格拆分为一个独立窗口。Ctrl+b z ：当前窗格全屏显示，再使用一次会变回原来大小。Ctrl+b Ctrl+&lt;arrow key&gt; ：按箭头方向调整窗格大小。Ctrl+b q ：显示窗格编号。 Close window&#x2F;pane 1exit Window Operation tmux new-window -n &lt;window-name&gt;: new a window; tmux select-window -t &lt;window-number/name&gt;: switch window tmux rename-window Shortcuts in window operation: 123456Ctrl+b c：创建一个新窗口，状态栏会显示多个窗口的信息。Ctrl+b p：切换到上一个窗口（按照状态栏上的顺序）。Ctrl+b n：切换到下一个窗口。Ctrl+b &lt;number&gt;：切换到指定编号的窗口，其中的&lt;number&gt;是状态栏上的窗口编号。Ctrl+b w：从列表中选择窗口。Ctrl+b ,：窗口重命名。 Other Commands1234567891011121314151617181920212223242526272829303132333435363738394041# 列出所有快捷键，及其对应的 Tmux 命令$ tmux list-keys# 列出所有 Tmux 命令及其参数$ tmux list-commands# 列出当前所有 Tmux 会话的信息$ tmux info# 常用快捷键Ctrl+b s 列出所有sessions信息Ctrl+b wCtrl+b o 切换窗格Ctrl+b n 切换窗口Ctrl+b d 分离sessiontmux new [-t xx [-n xx]]tmux new-windowtmux split-window [-h]tmux lstmux attach -t xxtmux detachexit打开鼠标模式tmux set mouse on# 重新加载当前的 Tmux 配置$ tmux source-file ~/.tmux.confctrl+b, : //按完前缀ctrl+B后，再按分号：进入命令行模式set -g mouse on //命令行中输入这句命令，回车就行了# 重命名tmux rename-session -t id &lt;new&gt; -t (target)tmux rename-window -t id &lt;new&gt;tmux resize-pane -U/D/L/R 10# 选中复制shift + 左键选中","tags":[{"name":"linux","slug":"linux","permalink":"https://stu-yue.github.io/tags/linux/"},{"name":"shell","slug":"shell","permalink":"https://stu-yue.github.io/tags/shell/"}],"categories":[{"name":"languages","slug":"languages","permalink":"https://stu-yue.github.io/categories/languages/"},{"name":"shell","slug":"languages/shell","permalink":"https://stu-yue.github.io/categories/languages/shell/"}]},{"title":"Memo | Here Document","date":"2023-10-04T07:56:53.000Z","path":"jottings/languages/shell/memo_here_doc/","text":"tee命令 ：tee指令会从标准输入设备读取数据，将其内容输出到标准输出设备，同时保存成文件。 -a：追加文件内容，而非覆盖； -i：忽略中断信号； Here Document（也称heredoc）是一种在shell中输入一个或多个行的标准输入文本的方法。一个Here Document允许用户创建一个有起始标记和终止标记的文本块，该文本块的内容会被当作输入传送给一个命令。 基本语法 1234command &lt;&lt;DELIMITERtext blocktext blockDELIMITER 12345678910# 文本块中的任何以制表符缩进的内容将会在传递给命令之前去除这些制表符缩进command &lt;&lt;-EOF# 带单/双引号后，将不在shell中进行变量替换和命令替换NAME=&quot;World&quot;CURRENT_DATE=$(date)tee /path/to/greeting.txt &lt;&lt;EOFHello, $NAME!Today is $CURRENT_DATE.EOF","tags":[{"name":"linux","slug":"linux","permalink":"https://stu-yue.github.io/tags/linux/"},{"name":"shell","slug":"shell","permalink":"https://stu-yue.github.io/tags/shell/"}],"categories":[{"name":"languages","slug":"languages","permalink":"https://stu-yue.github.io/categories/languages/"},{"name":"shell","slug":"languages/shell","permalink":"https://stu-yue.github.io/categories/languages/shell/"}]},{"title":"","date":"2023-09-26T06:40:27.727Z","path":"jottings/tidbits/quantization/","text":"在线量化：指量化感知训练(Quantization-Aware Training)，在网络模型训练阶段采用量化方案进行量化； 量化感知训练本质上是一种伪量化的过程，它是在可识别的某些操作内嵌入伪量化节点（fake quantization op），并参与模型训练的前向推理过程模拟引入，但模型的反向传播过程依旧使用全精度浮点数进行； 伪量化节点，是指量化感知训练中插入的节点，用以寻找网络数据分布，并反馈损失精度： 找到输入、权重等待量化数据的分布，找到待量化数据的最大和最小值； 模拟低比特量化带来的精度损失，把该损失作用到网络模型中，传递给损失函数，让优化器在训练过程中对该损失值进行优化， 尽可能减少由于伪量化操作而引起的精度下降； 先饱和截断处理：$clamp(x,x_{min}, x_{max}) &#x3D; \\min(\\max(x,x_{min}), x_{max})$； 再Float-&gt;Int-&gt;Float： 离线量化：指训练后量化(Post-Training Quantization)： 动态离线量化(PTQ, Dynamic)： 动态离线量化仅将模型中特定算子的权重从FP32类型映射成 INT8&#x2F;16 类型，bias和激活函数 在推理过程中动态量化。但是对于不同的输入值来说，其缩放因子是动态计算的（“动态”的由来）。动态量化是几种量化方法中性能最差的。动态量化常用于非常大的模型。 静态离线量化(PTQ, Static)： 静态离线量化使用少量无标签校准数据，采用 KL 散度等方法计算量化比例因子。静态量化（Static quantization）与动态量化的区别在于其输入的缩放因子计算方法不同，静态量化的模型在使用前有“calibrate”的过程（校准缩放因子）：准备部分输入（对于图像分类模型就是准备一些图片，其他任务类似），使用静态量化后的模型进行预测，在此过程中量化模型的缩放因子会根据输入数据的分布进行调整。一旦校准完成后，权重和输入的缩放因子都固定（“静态”的由来）。静态量化的性能一般比动态量化好，常用于中等模型和大模型。因此实际中基本都是在用静态量化。 静态离线量化的目标是求取量化比例因子，主要通过对称量化、非对称量化方式来求，而找最大值或者阈值的方法又有MinMax、KLD、ADMM、EQ等方法。 对称量化与非对称量化：对于weight权重的量化使用对称量化[-INT_MAX, INT_MAX]，对于activate激活的量化使用非对称量化[0, INT_MAX]；","tags":[],"categories":[{"name":"tidbits","slug":"tidbits","permalink":"https://stu-yue.github.io/categories/tidbits/"}]},{"title":"Memo | Package and Device","date":"2023-09-19T14:56:53.000Z","path":"jottings/languages/shell/memo_package_device/","text":"Package Package System Package Management System Linux Release Version Debian Style (.deb) Debian, Ubuntu Red Hat Style (.rpm) Fedora, CentOS A package file is a compressed collection of files that make up a software package and may contain a large number of programs and the data files that support those programs. Metadata for the packet is included, such as a text description of the package and its contents. Other included are pre-installation and post-installation scripts (which perform configuration tasks before and after installation) Upper Tools and Underlying Tools: Linux Release Version Underlying Tool (install and delete packages) Upper Tool (search for metadata and resolve dependencies) Debian-Style dpkg apt, aptitude Fedora, CentOS rpm yum Search for metadata in the resource repository Style Command Debian apt update; apt search search_string Red Hat yum search search_string (e.g. yum search emacs) Install a package via resource repository Style Command Debian apt update; apt install package_name Red Hat yum install package_name Install a package via raw package files Style Command Debian dpkg –install &#x2F; -i package_file Red Hat rpm -i package_file (rpm -i emacs-22.1-7.fc7-i386.rpm) NOTE: Due to this command is executed by rpm, not analyzing dependencies for package_file, so if a dependency is missing, rpm will report error and exit. Uninstall a package Style Command Debian apt remove package_name &#x2F; dpkg -r package_name Red Hat yum erase package_name Upgrade package via resource repository Style Command Debian apt update; apt upgrade Red Hat yum update Upgrade package via package_file Style Command Debian dpkg –install package_file Red Hat rpm -U package_file List all package installed Style Command Debian dpkg –list &#x2F; -l Red Hat rpm -qa Determine whether a package is installed Style Command Debian dpkg –status package_name Red Hat rpm -q package_name Show the info for the installed package Style Command Debian apt show package_name Red Hat yum info package_name apt useful arguments: -y : default set yes in interactive shell; -f: solve the package dependencies; Device","tags":[{"name":"linux","slug":"linux","permalink":"https://stu-yue.github.io/tags/linux/"},{"name":"shell","slug":"shell","permalink":"https://stu-yue.github.io/tags/shell/"}],"categories":[{"name":"languages","slug":"languages","permalink":"https://stu-yue.github.io/categories/languages/"},{"name":"shell","slug":"languages/shell","permalink":"https://stu-yue.github.io/categories/languages/shell/"}]},{"title":"Summa |Networks Tidbits","date":"2023-09-07T10:35:53.000Z","path":"jottings/networks/memo_network_tidbits/","text":"Tunneling: The basic principle is to create a virtual channel between the source and the target, through which the original packets is encapsulated in the packet of another protocol, and then transmitted between the source and the target. At the target end, the receiver unwarps the encapsulated packet, reverts it to the original packet, and gives it to the target application for processing. For example, VPN (Virtual Private Network), SSH Tunneling, GRE (Generic Routing Encapsulation, like IPv6 over IPv4).","tags":[{"name":"networks","slug":"networks","permalink":"https://stu-yue.github.io/tags/networks/"}],"categories":[{"name":"networks","slug":"networks","permalink":"https://stu-yue.github.io/categories/networks/"}]},{"title":"Memo | Olds and Ends","date":"2023-09-06T07:56:53.000Z","path":"jottings/languages/shell/memo_others/","text":"Difference between sh and bash: sh is usually symbolic link for dash; dash is a more lightweight shell, POSIX, designed to replace sh and provide faster startup and executions speeds; bash is an extended version of sh, and most scripts that conform to sh syntax should work fine in bash; In a nutshell, sh is bash, which enables the POSIX standard. POSIX, Portable Operating System Interface of UNIX In accordance with the POSIX specification, “When a line of code encounters an error, it does not continue to interpret subsequent lines.” However, in bash, even if an error occurs, it will continue to execute subsequent lines. To view cpu information: lscpu, or cat /proc/cpuinfo;","tags":[{"name":"linux","slug":"linux","permalink":"https://stu-yue.github.io/tags/linux/"},{"name":"shell","slug":"shell","permalink":"https://stu-yue.github.io/tags/shell/"}],"categories":[{"name":"languages","slug":"languages","permalink":"https://stu-yue.github.io/categories/languages/"},{"name":"shell","slug":"languages/shell","permalink":"https://stu-yue.github.io/categories/languages/shell/"}]},{"title":"Memo | Permission, Progress and Shell Environment","date":"2023-08-31T14:56:53.000Z","path":"jottings/languages/shell/memo_permission_progress_env/","text":"PermissionFirst of all, let’s take a quick look at the permissions-related commands. id: To show the id number of the user. id username: chmod: To change the mode of files. symbolic examples: 123u (user), g (group), o (other), a (all)+, -, =u+x u-x +x[=a+x] o-rw, go=rw u+x,go=rw umask: To set default file permissions (before creating). an example: 1234# umask is 0002Original file mode | --- rw- rw- rw-Mask | 000 000 000 010Result | --- rw- rw- r-- su: To run the shell as another user. su - username, - can enter the home directory. sudo: To execute the command as another user. chown: To change the file owner. chown [owner[:group]] file..., here are some examples: Example Explanation bob change the file owner to bob bob:users change the file owner to bob, group to users :admins change the group to admins, file owner remains the same bob: change the file owner to bob, group to bob’s login group chgrp: To change the group ownership of files. chgrp groupname file... passwd: To change the user’s password. passwd username: set user’s password. User information is stored in /etc/passwd, group information is stored in /etc/group; use the command cat /etc/passwd to have a quick look. 123456789101112131415161718192021cat /etc/passwd | grep ubuntu # username:passwd:uid:gid:comment:home_dir:shell# passwd (x) denotes that password is protected in /etc/passwd# comment store some useful comments (like username)ubuntu:x:1000:1000:ubuntu:/home/ubuntu:/bin/bash sudo cat /etc/shadow | grep ubuntu #username:passwd(encrypted):last_modify_time:min_interval:max_intervalubuntu:$1$oQIzlBrL$MErhwMGkTzqaeWkJNzpUh1:19132:0:99999:7::: cat /etc/group | grep cdrom # groupname:passwd:gid:group_membercdrom:x:24:ubuntu,yue sudo cat /etc/gshadow | grep test # groupname:passwd:group_manager:group_membertest:*:yue:ubuntu Permission Overview: r w x File readable writable executable Directory readable writable to files in the directory enterable to the directory Home directory default permission: user &#x3D;rwx, go&#x3D;r-x ; setuid (s/S &lt;-&gt; x/-, 4000/u+s) : It set valid user ID from the running user’s ID to the file owner’s ID; setgid (g/G &lt;-&gt; x/-, 2000/g+s): Run not with the permissions of the group to which the user who started it belongs, but with the group that owns the file. In other words, the process gid is the same as the file gid. sticky (t/T &lt;-&gt; x/-, 1000/o+t): It has no effect on files, but when it is used on a directory, all files in the directory can only be deleted or moved by their owner. File Type: Tag Type - a normal file d a directory l a symbolic link (real file property is the file property that the symbolic link points to) c a character device file (process bytes stream, like terminal) b a block device file (process blocks, like hard-disk or CD-ROM) User and GroupBecause the permission is too large, you can even delete system files and crash the system. Therefore, you are not recommended to directly use root account. On Linux systems, sudo is used as the default root identity for standard users. Next, we have some commands for managing users and groups. groupadd: create a new work group, whose info is added to /etc/group, /etc/gshadow and so on. [-g gid] (specify the id of new group), -r (create system working groups) groupdel: delete a group gpasswd [options] groupname: management tool the /etc/group and /etc/gshadow -a/d username (add&#x2F;delete user to group) -A (specify the manager) -r/R (cancel the password for the group, then only group member can newgrp to access the group) -M user1,user2... add users to group groupmod: change the group information -g gid (change group id) -n new_name old_name (change group name) newgrp groupname: It’s using the same account another group name, to log into the system again. useradd: create a new user. -m/M (automatically &#x2F; not create a user home directory), -g (specify the login group), -G grp1,grp2... (specify the supplementary groups) -d (specify the starting directory for the user to log in to), -r (create a system account) -s (specify the login shell) -n (cancel creating a group with the user name) -p (specify the password, or later run the command passwd to set) useradd -m -g root username, useradd -d /home/test username userdel: -r (recursively delete) Initial Login Group, is a group that a user owns immediately upon login. It’s usually specified with -g when creating a user. The GID in the user info (/etc/passwd) is login group. A user’s additional group is to assign additional permission to the specified user. (There can be only one login group and multiple supplementary group) usermod: modify the settings of the user account. -d: set login directory of the user account -e: set validity period … -g: set the login group … -G: set the supplementary group … -s: set the shell used after login … -l new_name old_name: set the new username … -L/U: lock&#x2F;unlock the account … -p: set the new password … ProgressWhen the system starts, the kernel initializes some of its own activities as Init Process (PID 1). In turn, a series of shell scripts called Init Scripts (located in /etc) are run, which can start all system services. Many of these system services are implemented in the form of daemons, which run only in the background without any user interface (inaccessible). Here are some of the command-line tools available: ps: To view the snapshot of process status; common parameter aux (show all processes) 1234567ubuntu@VM-16-17-ubuntu:/etc$ ps auxUSER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMAND# TTY: ? denotes terminal running locally, Pts/n denotes terminal running remotely# VSZ: virtual memory size# RSS: physical memory size occupied by process# STAT: R(running), S(sleeping), D(uninterruptible sleeping), T(terminal), Z(zombie) &lt;(high priority) N(low priority) Ctrl-C: send a SIGIN Ctrl-D: send a EOF Ctrl-Z: send a SIGSTP, pause ongoing process on the terminal so as to be resumed when necessary. top: To displays a continuously updated list of system process in order of process activity (default, refresh per 3s); jobs: bg: fg: kill: To send signals to programs (kill [-signal] PID...); NO. NAME EXPLANATION 1 HUP Hang up, this signal is used to tell the program that the control terminal has “hung up.” You can show what this signal does by closing a terminal session. Foreground programs running on the current terminal will receive this signal and terminate. Many daemons also use this signal to re-initialize. 2 INT like Ctrl-c 9 KILL The KILL signal is never sent to the target program. Instead, the kernel immediately terminates the process. When a process is terminated in this way, it has no opportunity to do any “cleaning” or saving work. 15 TERM Terminal, this is default signal sent by KILL 18 CONT Continue, after getting a stop signal, program will be resumed by CONT 19 STOP like KILL, STOP is not sent to the target process, so it cannot be ignored 其他常用信号： NO. NAME EXPLANATION 3 QUIT 11 SEGV If a program uses memory illegally, this signal will be sent 20 TSTP Ctrl-z will trigger this signal to cause terminal stop, unlike STOP, it can be ignored killall: To send signals to multiple processes that match a particular program or username (killall [-u user] [-signal] name...); shutdown: To shutdown the machine or reboot; Shell Environment printenv: set: To display existing shell variables in the system and set new variable values for shell variables. When user log in to the system, the bash program starts and reads a series of configuration scripts (startup files that define a default environment for all users), then it reads the startup files in the home directory that define the user’s personal shell environment. The exact startup order depends on the type of shell session you want to run. There are two types, one is login shell session (need username and password), the other is non-login shell session (start under the GUI). Login shell startup order: File Usage &#x2F;etc&#x2F;profile global conf script applying to all users ~&#x2F;.bash_profile user’s personal startup file, used to extend or override settings in global conf script ~&#x2F;.bash_login if ~&#x2F;.bash_profile is not found, bash will try to read this script Non-login shell startup order: File Usage &#x2F;etc&#x2F;bash.bashrc global conf script applying to all users ~&#x2F;.bashrc user’s personal startup file, used to extend or override settings in global conf script In addition to reading the startup files above, non-login shell also inherit the environment settings of their parent process, usually a login shell. In general users’ points, the file ~&#x2F;.bashrc is probably the most important startup file because it’s almost always read. Non-login shells read it by default, and most startup files for login shells are written in such a way that they can read ~&#x2F;.bashrc . The below is a typical .bash_profile file (From CentOS 4): 12345678# .bash_profile# Get the aliaes and functionsif [ -f ~/.bashrc ]; then. ~/.bashrcfi# User specific environment and startup programsPATH=$PATH:$HOME/binexport PATH export: export environment variables; alias: create alias for command; Refref1","tags":[{"name":"linux","slug":"linux","permalink":"https://stu-yue.github.io/tags/linux/"},{"name":"shell","slug":"shell","permalink":"https://stu-yue.github.io/tags/shell/"}],"categories":[{"name":"languages","slug":"languages","permalink":"https://stu-yue.github.io/categories/languages/"},{"name":"shell","slug":"languages/shell","permalink":"https://stu-yue.github.io/categories/languages/shell/"}]},{"title":"Memo | Parameter Estimation","date":"2023-08-30T05:56:53.000Z","path":"jottings/mathematics/memo_mle_mae_bayes/","text":"Through this note, I hope to deepen my understanding of probability distribution and inference. Preface Probability: to predict results obtained in the next observation when parameters are known; Statistic&#x2F;Likelihood: to estimate parameters about properties when the result of observations are known; Parametric Method: assume that the learned distribution has a specific functional form (like Gaussian distribution or exponential p.d.f), we only estimate the parameters in these functions. Nonparametric Method: use the training samples to estimate the density of any point in the domain. Nonparametric methods also have parameters, we just don’t assume any specific functional form for distribution; Actually, nonparametric methods treat all training samples as parameters; example: kernel density estimation; Under the joint distribution $p_{X,Y}(X,Y)$: When the effect of $Y$ is removed from the joint distribution $p_{X,Y}(X,Y)$, marginal distribution $p_X(X)$ is called marginal likelihood; When $X$ has not yet been observed, marginal distribution $p_Y(Y)$ is called prior distribution; Posterior Distribution: $p(\\theta|\\mathcal{D}) &#x3D; \\frac{p(\\mathcal{D},\\theta)}{p(\\mathcal{D})}$, concentrating Population Info, Sample Info and Prior Info; $p(\\mathcal{D})$ is marginal likelihood; $\\text{Posterior} &#x3D; \\frac{\\text{Likelihood}\\times \\text{Prior}}{\\text{Marginal Likelihood}}$, in terms of $p(Y|X) &#x3D; \\frac{1}{Z}p(X|Y)p(Y)$, $Z&#x3D;p(X)&gt;0$ is a normalized constant such that $p(Y|X)$ is a valid probability distribution. The Views Frequentist: Data are repeatable random sample - there is a frequency; Underlying parameters remain constant during this repeatable process; Parameters are fixed value; statistical inference: Population Info + Sample Info MLE, MAP; Bayesian: Data are observed from the realized sample; Parameters are unknown (random variable) and described probabilistically (prior distribution); statistical inference: Population Info + Sample Info + Prior Info (Main Diff); Bayesian Estimation; Notations Training Data: $\\mathcal{D} &#x3D; { (\\mathbf{x_1}, y_1),\\cdots,(\\mathbf{x}_n, y_n) }$; Model Parameter: $\\theta$; New Data: $x^*$; Maximum Likelihood Estimation Objective is$$\\theta_{MLE}^* &#x3D; \\arg\\max_\\theta p(\\mathcal{D}|\\theta)$$ $p(\\mathcal{D}|\\theta)$ is likelihood, not conditional probability; Usually, we define$$\\mathscr{l}(\\theta) &#x3D; p(\\mathcal{D}|\\theta) \\\\mathscr{ll}(\\theta) &#x3D; \\ln \\mathscr{l}(\\theta)$$So, objective is equivalent to$$\\theta_{MLE}^* &#x3D; \\arg\\max_\\theta \\mathscr{ll}(\\theta)$$That is, we seek those values for the parameters in $\\theta$ which maximize $p(\\mathcal{D}|\\theta)$. The MLE solution is usually obtained by setting$$\\frac{\\partial \\mathscr{ll}(\\theta)}{\\partial\\theta} &#x3D; 0$$ However, the model… Does not incorporate prior belief; Easy to overfit the data; Maximum A Posteri Estimation Objective is$$\\theta^*_{MAP} &#x3D; \\arg\\max_\\theta p(\\theta|\\mathcal{D})$$Since we have Bayes rule:$$p(\\theta | \\mathcal{D}) &#x3D; \\frac{p(\\mathcal{D})p(\\theta)}{p(\\mathcal{D})}$$Our objective is equivalent to$$\\theta^*_{MAP} &#x3D; \\arg\\max_\\theta p(\\mathcal{D}|\\theta)p(\\theta)$$Further, by taking the log$$\\theta^*_{MAP} &#x3D; \\arg\\max_\\theta {\\ln p(\\mathcal{D}|\\theta) + lnp(\\theta) } \\\\theta^*_{MAP} &#x3D; \\arg\\max_\\theta { \\mathscr{ll}(\\theta) + \\ln p(\\theta) }$$Thus, our final goal is to find$$\\theta^*_{MAP} &#x3D; \\arg\\max_\\theta { \\mathscr{ll}(\\theta) + \\ln p(\\theta) }$$The difference between MAP and MLE is the “extra” term - $p(\\theta)$. The term is: our prior (belief) also can be seen as penalty (regularization) - to reduce the overfitting. For $p(\\theta|\\mathcal{D})$, in terms of point estimation for $\\theta$, using the maximum value is called Maximum A Posterior Estimation; using the median value is called Posteriror Median Estimation; using the expectation value is called Posterior Expectation Estimation; Bayesian Estimation $p(\\theta|\\mathcal{D})$ (it’s the result of adjustments to prior $p(\\theta)$ by population and sample); Equal Ignorance: If there’s no information about the prior distribution, assume $\\theta \\sim U(0,1)$ ; Learning: Computing the posterior $p(\\theta|\\mathcal{D})$ ; Prediction: $p(\\hat y | x^*,\\mathcal{D}) &#x3D; \\int_\\theta p(\\hat y | x^*,\\theta)p(\\theta|\\mathcal{D})d\\theta$ ; Both MLE and MAP return only single and specific values for the paramter $\\theta$; Bayesian estimation, by contrast, calculates fully the posterior distribution $p(\\theta|\\mathcal{D})$, and making prediction by considering all possible $\\theta$. Thus, for Bayesian methods: The prediction is optimal Avoid the overfitting Bayesian is powerful, but… We need to compute posterior distribution $p(\\theta|\\mathcal{D})$, and$$p(\\theta|\\mathcal{D}) &#x3D; \\frac{p(\\mathcal{D}|\\theta)p(\\theta)}{\\int p(\\mathcal{D},\\theta)d\\theta}$$In practice, evaluating this posterior is usually intractable due to the complex integrals… ReferenceMainly Ref Ref2 Ref3","tags":[{"name":"math","slug":"math","permalink":"https://stu-yue.github.io/tags/math/"}],"categories":[{"name":"mathematics","slug":"mathematics","permalink":"https://stu-yue.github.io/categories/mathematics/"}]},{"title":"Summa | Intro of Statistical ML","date":"2023-08-27T07:56:53.000Z","path":"jottings/statistics_ml/summa_intro/","text":"If a system is able to improve its performance by executing a certain process, it is called learning. 1 Basis Hypothesis Space: The set of functions that includes all possible models to be learned. Common Steps: Attain a finite training data set. Determine the set of learning models. Determine the criteria for model selection (learning strategies). Develop algorithms for solving the optimal model (learning algorithm). Select optimal model according to criteria. Use the learned optimal model to predict or analyze new data. Problem Types: Classification&#x2F;Tagging (outputs take a finite number of discrete values), Regression (function fitting, real values) Model Types: Discriminative Model: directly model $p(Y|X)$; Cons: easy to learn, high classification accuracy; Generative Model: model $p(X, Y)$, usually translating to modeling the prior distribution $p(Y)$ and class conditional distribution $p(X|Y)$, due to ($p(X, Y)&#x3D; P(X|Y)p(Y)$); Cons: add the prior distribution [main diff], model the data generation process;","tags":[{"name":"ml","slug":"ml","permalink":"https://stu-yue.github.io/tags/ml/"},{"name":"statistic","slug":"statistic","permalink":"https://stu-yue.github.io/tags/statistic/"}],"categories":[{"name":"statistics_ml","slug":"statistics-ml","permalink":"https://stu-yue.github.io/categories/statistics-ml/"}]},{"title":"Memo | Algebra Basis","date":"2023-08-27T07:56:53.000Z","path":"jottings/mathematics/memo_algebra_basis/","text":"**Positive Definiteness of the Norm: ** According to the usual definition, a norm should satisfy the property of positive definiteness which means that a norm must have a non-negative value for non-zero vectors and only attain zero when the vector is the zero. **Norm and Distance Evaluation: ** Norm Distance $L_0\\ \\text{norm}$ : $ $L_1\\ \\text{norm}$ : $ $L_2\\ \\text{norm}$ : $ $L_p\\ \\text{norm}$ : $ $\\text{Infinite norm}$ : $ Normed Space: Normed space introduces a norm (or length, modulus) concept on the basis of linear space.","tags":[{"name":"algebra","slug":"algebra","permalink":"https://stu-yue.github.io/tags/algebra/"}],"categories":[{"name":"mathematics","slug":"mathematics","permalink":"https://stu-yue.github.io/categories/mathematics/"}]},{"title":"Memo | See the World through Shell","date":"2023-08-27T07:56:53.000Z","path":"jottings/languages/shell/memo_see_the_world_through_shell/","text":"In this post, there’re other commands we should be familiar with: echo: To display a line text. -e (explain the escape sequences) clear: history: To display the history list content. list history commands: sequence conduct !! repeat the last command executed !number repeat the number line command in the history !string repeat the command starting with this string !?string repeat the command containing this string Expansions in Shell Mathematical Expression: Format: $((expression)), for example, echo $((2 + 2)); Operators: +, -, *, / (integer division), %, ** (exponentiation); Curly Brackets (or Braces): Curly braces can create multiple text characters. Generally speaking, a pattern in curly braces may contain a header and a postscript. The curly brace expression itself may contain a list of strings separated by commas, an integer interval, or a single character interval. Whitespace characters cannot be embedded in this mode. echo Number_&#123;1..5&#125;, echo &#123;Z..A&#125;, echo Front-&#123;A,B,C&#125;; mkdir &#123;2007..2009&#125;-0&#123;1..9&#125; &#123;2007..2009&#125;-&#123;10..12&#125; Furthermore, curly braces can be nested: 12echo a&#123;A&#123;1,2&#125;,B&#123;3,4&#125;&#125;baA1b aA2b aB3b aB4b Parameter Expansion: Command Substitution: Use the output of a command as expansion mode: ls -l $(which cp), echo$(ls), file $(ls /usr/bin* | grep zip); In the old shell, [&#96;&#96;] can replace [$()]: 1ls -l `which cp` Double Quotation: In double quotes, the special characters used by the shell lose their special meaning and are treated as ordinary characters. Mathematical expansion, parameter expansion and command substitution are still performed. (Double quotation can retain whitespace) ls -l &quot;two words.txt&quot;, echo &quot;$USER $((2+2)) $(cal)&quot;; echo &quot;$(cal)&quot; is different with echo $(cal), try it. Single Quotation: Single quotation disables all mode expansions. 123456ubuntu@VM-16-17-ubuntu:~$ echo text ~/*.txt &#123;a,b&#125; $(echo foo) $((2+2)) $USERubuntu@VM-16-17-ubuntu:~$ echo &quot;text ~/*.txt &#123;a,b&#125; $(echo foo) $((2+2)) $USER&quot;ubuntu@VM-16-17-ubuntu:~$ echo &#x27;text ~/*.txt &#123;a,b&#125; $(echo foo) $((2+2)) $USER&#x27;text /home/ubuntu/lazy_dog.txt /home/ubuntu/ls.txt a b foo 4 ubuntutext ~/*.txt &#123;a,b&#125; foo 4 ubuntutext ~/*.txt &#123;a,b&#125; $(echo foo) $((2+2)) $USER **Backslash: ** \\ can escape special characters in shell, but not in single quotes. 12echo &quot;The balance for user $USER is: \\$5.00&quot;mv bad\\&amp;filename good_filename NOTE: 1$&quot;abc&quot;, $&quot;\\n&quot; The above is a special string conversion syntax used to localize strings (with Settings such as the environment variable LANG or LC_MESSAGES). 1$&#x27;abc&#x27;, $&#x27;\\n&#x27;, $&#x27;ab\\tc&#x27; When you use the form $’abc’ in the Shell, the Shell extends the string and replaces the special characters with the corresponding escape sequence. Supplement $()仅在Bash Shell中有效，反引号可在多种Shell中使用；它俩作用都是执行命令 $()是新用法，``是老用法；$()支持嵌套——如$(wc -l $(ls | sed -n &#39;1p&#39;)); 单引号（’）所见即所得，直接显示单引号里的内容。即单引号里的任何字符都会原样输出，单引号字符串中的变量是无效的； 双引号（“）先把变量解析之后，再输出；双引号括起来的字符中（$，\\，`)，主要是区分空格 $ 代表引用变量的值； \\ 是转义字符； ` 代表引用命令； echo -e可以识别双引号中的转义字符； 反引号用于命令替换，即先执行反引号中的语句，再把结果加入到原命令中； $&#123;a&#125;和$a一样，数组则需要$&#123;array[0]&#125;； https://blog.csdn.net/qq_39852676/article/details/90228973 Vim1 替换字符串1234567891011:[range]s/&#123;pattern&#125;/&#123;string&#125;/[flags] [count]range: .,$s/foo/bar/ .表示当前行 3,10s/foo/bar/ 3-10行 %s/foo/bar/ 整个文本 flags: i:忽略大小写 g:出现的所有模式 c:确认每次替换 https://cloud.tencent.com/developer/article/2015348","tags":[{"name":"linux","slug":"linux","permalink":"https://stu-yue.github.io/tags/linux/"},{"name":"shell","slug":"shell","permalink":"https://stu-yue.github.io/tags/shell/"}],"categories":[{"name":"languages","slug":"languages","permalink":"https://stu-yue.github.io/categories/languages/"},{"name":"shell","slug":"languages/shell","permalink":"https://stu-yue.github.io/categories/languages/shell/"}]},{"title":"Memo | Redirection","date":"2023-08-25T06:56:53.000Z","path":"jottings/languages/shell/memo_redirection/","text":"Input&#x2F;output redirection is achieved by modifying file pointers. When redirection occurs, file descriptors themselves are not changed, instead, it’s the file pointers associated with the file descriptors that are altered. Manipulation of File Descriptors in Shell In redirection, &amp; is used to indicates the following numbers is file descriptor rather than a filename. Output Redirection: stdiout redirection: command &gt;file: [overwrite], stdout overwrites the file. &gt; file: clear the content. command &gt;&gt;file: [append], stdout appends to the file. stderr redirection: command 2&gt;file: [overwrite], stderr overwrites the file. command 2&gt;&gt;file: [append], stderr appends to the file. Both stdout and stderr: command &gt;file 2&gt;&amp;1: [overwrite], both stdout and stderr overwrite the file. command &gt;&gt;file 2&gt;&amp;1: [append], both stdout and stderr append to the file. command &gt;file1 2&gt;file2: stdout overwrites the file1, stderr overwrites the file2. command &gt;&gt;file1 2&gt;&gt;file2: stdout appends to file1, stderr appends to file2. command &gt;file 2&gt;file: [not recommend] file is opened twice, leading to resource competition. command &gt;&gt;file 2&gt;&gt;file: [not recommend] file is opened twice, leading to resource competition. Input Redirection: command &lt;file: take the contents of the file as the input to command. 1234#!/bin/bashwhile read str; do echo $strdone &lt;readme.txt command &lt;&lt;END: read data from standard input (keyboard) until meeting the delimiter END (defined by the user). command &lt;file1 &gt;file2: input by file1, and output to file2 &amp;&gt;: redirect both stdout and stderr to the same location (usually a file). &gt;&amp;: redirect the output of one file descriptor to another &gt;&amp;-: equal to redirection to /dev/null PipelinePipeline is used to link the stdout of the previous instruction as the stdin of the next instruction. Pipeline is often used to perform complex operations on data. It’s possible to put several commands together to form a pipeline (usually called filter). For example: 1ls /bin /usr/bin | sort | uniq | less CommandThere’s some useful command in this memo: cat: To link file. cat can accept not one parameter, so it can concatenate the file: cat movie.mpeg.0* &gt; movie.mpeg; sort: To sort the text lines. uniq: To report&#x2F;omit the repetitive lines. grep: To print the matching rows. -i (ignore upper&#x2F;lower), -v (reverse find, print mismatching lines), -n (show the matching lines), -r (recursively find), -l (only print matching filename), -c (only print the number of matching line) wc: To print the LF, word, bytes of the text. -c (bytes), -l (lines), -w (words) head&#x2F;tail: To print first&#x2F;last part of text. -n (lines), -c (bytes) tail -f filename: continue to monitor this file, when the new is added to the file, they appear immediately on the screen. tee: read from stdin, and write to stdout and file. -a (append mode), -i (ignore-interrupts) ls -l | tee -a ls.log : print the content both in the stdout and file. ls /usr/bin | tee ls.log | grep zip","tags":[{"name":"linux","slug":"linux","permalink":"https://stu-yue.github.io/tags/linux/"},{"name":"shell","slug":"shell","permalink":"https://stu-yue.github.io/tags/shell/"}],"categories":[{"name":"languages","slug":"languages","permalink":"https://stu-yue.github.io/categories/languages/"},{"name":"shell","slug":"languages/shell","permalink":"https://stu-yue.github.io/categories/languages/shell/"}]},{"title":"Memo | GIL and Coroutine","date":"2023-08-10T08:01:53.000Z","path":"jottings/languages/python/memo_GIL_and_coroutine/","text":"GIL (Global Interpreter Lock) GIL is not a characteristic of Python itself, but rather a characteristic to CPython, the reference implementation of Python. In CPython, GIL is a mutex lock used to ensure that only on thread is executing at a time within a process. In the absence of the GIL, it’s possible for multiple threads executing the same code simultaneously to cause incorrect reference counting of variables, leading to the garbage collector directly reclaiming the variables involved in the executed code. This can result in errors when other threads attempt to use those reclaimed variables. How to work: Each thread acquires the GIL at the beginning of its execution to prevent other threads from preempting. Similarly, after each thread completes a segment of code (or before system calls that may cause blocking, such as IO operations), it releases the GIL to allow other threads to utilize resources. In CPython, there’s another mechanism called check_interval, where the interpreter periodically checks the status of the GIL lock for threads. After a certain interval, the interpreter forces the current thread to release the GIL, allowing other threads to have opportunity to execute. Overall, each Python thread is encapsulated in a similar loop structure. Let’s take a look at the following code: 12345678910111213for (;;) &#123; if (--ticker &lt; 0) &#123; ticker = check_interval; /* Give another thread a chance */ PyThread_release_lock(interpreter_lock); /* Other thread may run now */ PyThread_acquire_lock(interpreter_lock); &#125; bytecode = *next_instr++; switch(bytecode) &#123; /* execute the next instruction ... */ &#125;&#125; The above example represents instruction counting, while the current approach is mostly based on time slicing. Here’s another example that demonstrates GIL’s working principle: 123456789101112131415161718# coding=utf-8from threading import Threadfrom multiprocessing import Processdef test(): # endless loop for full CPU utilization while True: pass# multi-thread version:# t1 = Thread(target=test)# multi-process version:t1 = Process(target=test)t1.start()# main thread executionwhile True: pass When the multi-thread code running, we can observe the CPU utilization from htop as shown below: 121 [50%]2 [50%] According to the above, we can know that each thread spends their a half time waiting for GIL. Additionally, the multi-process version as shown below: 121 [100%]2 [100%] The other way to utilize multi-core CPU is Multi-Process or Coroutine GeneratorGenerator contains the yield keyword to produce values. It has the following characteristics: Laziness: Generators are lazy in nature, meaning they produce values on-demand as requested by the caller, rather than generating all the values at once. This lazy evaluation allows for efficient memory usage, especially when dealing with large or infinite sequences. Memory efficiency: Due to their lazy evaluation, generators are memory-efficient. Iterability: Generators are iterable objects, which means they can be looped over using a for loop or consumed by other iterable functions like list() or sum(). They provide a convenient way to iterate over a sequence of values without the need to store the entire sequence in memory. State persistence: Generators maintain their internal state between successive calls. When a generator function is paused at a yield statement, the local variables’ values are preserved. This allows the generator to resume execution from where it left off, retaining the necessary information to generate the next value. Infinite sequences: Generators can be used to represent infinite sequences since they generate values on-the-fly. Function-like behavior: Generators are defined using the def keyword like regular functions, and they can have parameters, return values, and other function features. However, they differ in their execution behavior, as they can be paused and resumed. When the generator function finishes executing (no more yield statements or returns), the generator object raises a StopIteration exception. yield from is a syntactic sugar that allows delegation of generator execution within a generator function. It provides a concise way to call a sub-generator from a parent generator and directly pass the values yielded by the sub-generator to the parent generator. 123456789101112def sub_generator(): yield &#x27;A&#x27; yield &#x27;B&#x27; yield &#x27;C&#x27; def parent_generator(): yield &#x27;START&#x27; yield from sub_generator() yield &#x27;END&#x27; for item in parent_generator(): print(item) yield and send are used together to allow the generator function to receive values during each iteration and send values back into the generator function for processing. Here is an example of how yield and send are used: 1234567891011121314151617def generator_function(): result = yield # First call, receives a value sent by send() while True: print(&#x27;gen&#x27;, result) result = yield resultgen = generator_function()# Start the generatornext(gen) # or gen.send(None)output = gen.send(&quot;Hello&quot;)print(&#x27;out&#x27;, output)output = gen.send(&quot;World&quot;)print(&#x27;out&#x27;, output)# print result:# gen Hello# out Hello# gen World# out World generator.throw(AnyException) allows generator to throw an Exception, and generator.close() can stop the generator. CoroutineCoroutines are implemented using generator functions and the yield statement. The principle of coroutines is as follows: Coroutine Function Definition: Define a generator function as a coroutine function. This function can use the yield statement to specify suspension points, where it pauses execution and returns a value to the caller. Coroutine Initialization: Create a coroutine object by calling the coroutine function. Coroutine Iteration: Use the next() function or the .send() method of the coroutine object to iterate and execute the coroutine. When the coroutine encounters a yield statement, it pauses execution and returns the result to the caller. Coroutine Resumption: When the caller sends a value to the coroutine (using the .send() method), the coroutine resumes execution from the last paused position and uses the sent value as the result of the yield expression. Coroutine Termination: When the coroutine reaches the end of the function or encounters a StopIteration exception, the coroutine terminates. Further calls to the .send() method on the coroutine object will raise a StopIteration exception. Coroutines allow achieving concurrent execution without the need for multiple threads or processes. Coroutines can switch between different execution paths, enabling efficient asynchronous programming. Coroutines can also delegate to other coroutines using the yield from statement, enabling more complex cooperation and task decomposition. Python provides the asyncio module to support coroutine programming, where the async and await keywords offer a more concise syntax for defining and managing coroutines. With asyncio, it becomes easy to write asynchronous programs and handle tasks like I&#x2F;O operations, network communication, and more. It’s important to note that coroutines run in a single thread, so their performance may not be as good as multi-threading or multiprocessing when it comes to CPU-bound tasks. However, in I&#x2F;O-bound tasks, coroutines shine because they can effectively utilize the waiting time for I&#x2F;O to execute other tasks. Async&#x2F;Await async def is a keyword combination used in Python to define asynchronous functions. An asynchronous function is a special type of function that can contain await expressions, which suspend the execution of the function and wait for the completion of asynchronous operations. Here is a example: 12345678910111213141516import asyncio# Define an asynchronous functionasync def async_func(): print(&quot;Start&quot;) await asyncio.sleep(1) # Suspend function execution using await, waiting for the completion of an asynchronous operation print(&quot;End&quot;)# Run the asynchronous function in an event loopasync def main(): await async_func()# Create an event loop and run the main functionloop = asyncio.get_event_loop()loop.run_until_complete(main())loop.close() await is a keyword used to suspend the execution of an asynchronous function and wait for the completion of an asynchronous operation. await can only be used within an asynchronous context and is typically used in conjunction with async def. The general usage of await is as follows: Use await within an asynchronous function or coroutine to suspend its execution and wait for the completion of an asynchronous operation. For example: 123async def async_func(): result = await async_operation() # 等待 async_operation() 异步操作的完成 # 继续异步操作，使用异步操作的结果 result Note that await can only be used within asynchronous functions or coroutines. It is not valid to use await in synchronous code. Typically, await is followed by an awaitable object, such as an asynchronous function, coroutine, asynchronous iterator, etc. These awaitable objects must implement specific protocols, which include methods like __await__() or __aiter__(). The await expression invokes these methods to obtain an iterator or a context manager from the awaitable object and waits for its completion. Here are some common awaitable objects: Asynchronous functions or coroutines: Use await to wait for the execution of an asynchronous function or coroutine to complete. Asynchronous generators: Use await to iterate over asynchronous generators and wait for each generated value. Asynchronous iterators: Use await to iterate over asynchronous iterators and wait for each iteration value. Asynchronous context managers: Use await to enter and exit the context of an asynchronous context manager. For example: 123456async def async_func(): async with async_context_manager() as cm: await cm.do_something() # Wait for the completion of the asynchronous context manager async for item in async_iterator(): await process_item(item) # Wait for the completion of each item generated by the asynchronous iterator","tags":[{"name":"python","slug":"python","permalink":"https://stu-yue.github.io/tags/python/"}],"categories":[{"name":"languages","slug":"languages","permalink":"https://stu-yue.github.io/categories/languages/"},{"name":"python","slug":"languages/python","permalink":"https://stu-yue.github.io/categories/languages/python/"}]},{"title":"Memo | Optimization Algorithm in Machine Learning","date":"2023-08-05T14:56:53.000Z","path":"jottings/mathematics/memo_optimization_alg/","text":"Reproduced in The Summary of Optimization Algorithm in ML For machine learning algorithms with diverse forms and characteristics, we have various optimization algorithms suitable for optimizing their objective functions. Apart from a few problems that can be solved using Brute Force Search to obtain the optimal solution, we can categorize the optimization algorithms used in machine learning into two types: Analytical Solutions: These algorithms aim to find the optimal solution to the objective function by solving mathematical equations or performing algebraic manipulations. They often involve setting derivatives or gradients to zero and solving the resulting equations. Analytical solutions are typically used for linear regression, logistic regression, and certain types of optimization problems with closed-form solutions. Numerical Optimization: These algorithms iteratively search for the optimal solution by evaluating the objective function at different points in the search space. They do not rely on explicit mathematical equations or derivatives. Numerical optimization methods include gradient-based algorithms like gradient descent and its variants, Newton’s method, stochastic gradient descent, and quasi-Newton methods. Global Optimization Methods: Heuristic Algorithm, Simulated Annealing, Particle Swarm Optimization, etc. Local Optimization Methods: Gradient Based: First Order Derivative: (Jacobian) Gradient Descent: $\\theta &#x3D; \\theta - \\eta \\cdot\\nabla J(\\theta)$ whereas Standard Gradient Descent will only converge to the minimum of the basin as mentioned above. SGD: $\\theta &#x3D; \\theta - \\eta \\cdot \\nabla J(\\theta;x_i;y_i)$; frequent updates, parameters updates have high variance and causes the Loss function to fluctuate to different intensities; helps us discover new and possibly better local minima; it ultimately complicates the convergence to the exact minimum and will keep overshooting due to the frequent fluctuations; Mini-Batch GD: ultimately lead us to a much better and stable convergence; make use of highly optimized matrix optimizations common to state-of-the-art deep learning libraries that make computing the gradient w.r.t. a mini-batch very efficient. Challenges in GD: Choosing a proper learning rate can be difficult; Additionally, the same learning rate applies to all parameter updates. If our data is sparse and our features have very different frequencies, we might not want to update all of them to the same extent, but perform a larger update for rarely occurring features. avoiding getting trapped in their numerous sub-optimal local minima; Actually, Difficulty arises in fact not from local minima but from *saddle points*, i.e. points where one dimension slopes up and another slopes down. These saddle points are usually surrounded by a plateau of the same error, which makes it notoriously hard for SGD to escape, as the gradient is close to zero in all dimensions. Momentum: $V(t) &#x3D; \\gamma\\cdot V(t-1)+\\eta \\cdot\\nabla J(\\theta)$, then $\\theta &#x3D; \\theta - V(t)$; leads to faster and stable convergence; reduced oscillations Problem: What actually happens is that as we reach the minima i.e the lowest point on the curve ,the momentum is pretty high and it doesn’t knows to *slow* down at that point due to the high momentum which could cause it to miss the minima entirely and continue movie up. Nesterov accelerated gradient (NAG): $V(t) &#x3D; \\gamma\\cdot V(t-1)+\\eta\\cdot\\nabla J(\\theta - \\gamma\\cdot V(t-1))$, then $\\theta &#x3D; \\theta - V(t)$； In the method he suggested we first make a big jump based on out previous momentum then calculate the Gradient and then make an correction which results in an parameter update. Now this anticipatory update prevents us to go too fast and not miss the minima and makes it more responsive to changes. We know that we will use our momentum term γV(t−1) to move the parameters θ. Computing θ−γV(t−1) thus gives us an approximation of the next position of the parameters which gives us a rough idea where our parameters are going to be. We can now effectively look ahead by calculating the gradient not w.r.t. to our current parameters θ but w.r.t. the approximate future position of our parameters AdaGrad: $\\theta_{t+1,i} &#x3D; \\theta_{t,i}-\\frac{\\eta}{\\sqrt{G_{t,i}+\\epsilon}}\\cdot g_{t,i}$, $G_{t,i} &#x3D; G_{t,i}+\\nabla_{\\theta_{t,i}}J(\\theta)$; allows the learning Rate $-\\eta$ to adapt based on the parameters. So it makes big updates for infrequent parameters and small updates for frequent parameters. For this reason, it is well-suited for dealing with sparse data. At the beginning, AdaGrad has an incentive effect on convergence, and then slowly becomes penalty convergence, and the updating speed is getting slower and slower Problem: its learning rate $-\\eta$ is always Decreasing and decaying. Due to: the accumulation of each squared Gradients in the denominator , since every added term is positive. This in turn causes the learning rate to shrink and eventually become so small, that the model just stops learning entirely and stops acquiring new additional knowledge. This problem of Decaying learning Rate is Rectified in another algorithm called AdaDelta. AdaDelta: $g(t)$ is gradient of mini-batch; RMSprop: $E[g^2]t &#x3D; \\gamma E[g^2]{t-1}+(1-\\gamma)g^2_t$, $\\Delta\\theta_t &#x3D; -\\frac{\\eta}{\\sqrt{E[g^2]_t+\\epsilon}}\\odot g_t &#x3D; -\\frac{\\eta}{RMS[g]_t}g_t$, $\\theta_{t+1} &#x3D; \\theta_t + \\Delta\\theta_t$ The authors note that the units in this update (as well as in SGD, Momentum, or Adagrad) do not match, i.e. the update should have the same hypothetical units as the parameter. To realize this, they first define another exponentially decaying average, this time not of squared gradients but of squared parameter updates; Thus, they use $RMS[\\Delta\\theta]_{t-1}$ instead of hyperparameter $\\eta$ ; $RMS[\\Delta\\theta]t &#x3D; \\sqrt{E[\\Delta\\theta^2]{t-1}+\\epsilon}$ $E[\\Delta\\theta^2]t &#x3D; \\gamma E[\\Delta\\theta^2]{t-1}+(1-\\gamma)\\Delta\\theta^2_t$ Instead of accumulating all previous squared gradients, *AdaDelta* limits the window of accumulated past gradients to some fixed size w. Another thing with AdaDelta is that we don’t even need to set a default learning rate. What improvements we have done so far—— We are calculating different learning Rates for each parameter. We are also calculating momentum. Preventing Vanishing(decaying) learning Rates. Since we are calculating individual learning rates for each parameter , why not calculate individual *momentum* changes for each parameter and store them separately. This is where a new modified technique and improvement comes into play called as *Adam.* Adam: Adaptive Moment Estimation (Adam) is another method that computes adaptive learning rates for each parameter. In addition to storing an exponentially decaying average of past squared gradients like AdaDelta ,Adam *also keeps an exponentially decaying average of past gradients M(t), similar to momentum*: $m_t &#x3D; \\beta_1m_{t-1}+(1-\\beta_1)g_t, \\quad v_t &#x3D; \\beta_2v_{t-1}+(1-\\beta_2)g^2_t$ $m_t &#x3D; (1-\\beta_1)\\sum_{i&#x3D;1}^{t}\\beta^{t-i}_{1}g_i$, sum all weights of $g_i$ is $(1-\\beta_1)\\sum_{i&#x3D;1}^{t}\\beta_1^{t-i} &#x3D; 1-\\beta^t_1$ To rectify the bias to 1, divide $(1-\\beta_1^t)$ respectively, $\\hat m_t &#x3D; \\frac{m_t}{1-\\beta_1^t}, \\quad \\hat v_t &#x3D; \\frac{v_t}{1-\\beta_2^t}$, $\\theta_{t+1} &#x3D; \\theta_t - \\frac{\\eta}{\\sqrt{\\hat v_t}+\\epsilon}\\hat m_t$ 指数加权移动平均$m_t$，按元素平方的指数加权移动平均$v_t$ Second Order Derivative: (Hessian) Newton Method: Divide and Conquer: Coordinate Descent: SMO Algorithm: Staged Optimization: Dynamic Programming: The following picture illustrates the organization of this memorandum: Reference Types of Optimization Algorithms used in Neural Networks and Ways to Optimize Gradient Descent","tags":[{"name":"ml","slug":"ml","permalink":"https://stu-yue.github.io/tags/ml/"},{"name":"optimization","slug":"optimization","permalink":"https://stu-yue.github.io/tags/optimization/"}],"categories":[{"name":"mathematics","slug":"mathematics","permalink":"https://stu-yue.github.io/categories/mathematics/"}]},{"title":"Memo | Exploring Linux Files and Directories","date":"2023-08-05T02:56:53.000Z","path":"jottings/languages/shell/memo_exploring_os/","text":"Let’s start by learning some commands that are helpful for researching Linux systems. type: To explain how to interpret a command name. which: To show which executable program will be executed. man: To show command manual. apropos: To display a list of appropriate commands. info: To display the command info. alias: To create an alias for the command unalias: To cancel a alias for the command. ls: To list the files and directories in the current directory. -a, -d, -h, -r/--reverse, -l, -S[sort by size], -t[sort by modification time] file: To determine the file type. file filename is OK. less: To browse the content of a file, specifically, less is an improved version of more. less filename is OK. Commands Behavior Page UP or b Backward one window Page Down or space Forward one window UP Arrow Backward one line Down Arrow Forward one line [N]G Go to last line in file (or N lines) [N]g Go to first line in file (or N lines) &#x2F;characters Search forward for matching line n Repeat previous search h Display help. cp： Options Implication -a, –archive Copy files and directories, along with their attributes, including ownership and permissions -i, –interactive Prompt the user for confirmation before overwriting an existing file (default disable) -r, –recursive -u, –update Update the content not already present in the original -v, –verbose Display detailed command operation information mv: Options Implication -i, –interactive Prompt the user for confirmation before overwriting an existing file (default disable) -u, –update Update the content not already present in the original -v, –verbose Display detailed command operation information rm: Options Implication -f, –force Directly delete the file even if its attributes are read-only, without requiring individual confirmation -i, –interactive Prompt the user for confirmation before overwriting an existing file (default disable) -r, –recursive -v, –verbose Display detailed command operation information ln: ln file hard_link creates a hard link, and ln -s item soft_link creates a soft link. Hard Links: A hard link is a direct reference to the same physical location of a file on the storage device. It creates an additional entry in the file system that points to the same inode (data structure representing a file) as the original file. Changes made to either the original file or the hard link are reflected in both, as they refer to the same underlying data. Hard links cannot reference directories or files on different file systems or partitions. Soft Links: A symbolic link is a special file that contains a path pointing to another file or directory. It acts as a pointer or shortcut to the target file or directory. Symbolic links are independent files and have their own inodes. Modifying the original file or symbolic link does not affect each other, as they are separate entities. Symbolic links can reference directories or files on different file systems or partitions. Soft links can be created to a non-existent filename (of course, if you use vi on this soft link file, Linux will automatically create a new file named “filename”). Hard links cannot be created in such cases since the file must already exist, and the inode must exist as well. Using ls -li can view the inodes of the files. drwxrwxr-x 2 ‘s 2 is the number of hard links to a file. Besides, the usual number of inodes for a directory is 2 (include parent directory and itself .) tar: To create archive files (usually with the .tar extension) and archive files. create archive file: tar -cvf archive.tar file1 file2 directory/ -c (create archive file), -v (view details), -f (specify archive file) extract archive file: tar -xvf archive.tar -x (extract the content of archive files) compress archive file: tar -czvf archive.tar.gz directory/ -z (use gzip to compress) decompress archive file: tar -xzvf archive.tar.gz, tar -xjvf archive.tar.bz2 list the content of archive files: tar -tvf archive.tar tar -tf test.tar: view package structure tar -C dest_dir/ -x[z]vf test.tar[.gz] specified_dir_or_file...[path in -tf shows]: unzip the specified file&#x2F;directory in the package; unzip archive.zip -d dest_dir split and cat: split the tar.gz into small files, split: split -6/-b 3M -d -a 2 cud_test.tar.gz[dst_filename] cud_test.tar.gz_[out_filename] 12345cud_test.tar.gz_00cud_test.tar.gz_01cud_test.tar.gz_02cud_test.tar.gz_03cud_test.tar.gz_04 -&lt;line_N&gt; : split into a file every N lines -b &lt;bytes&gt; : split into a file every N bytes -d : specify the generated split file suffix in numeric form -a x : set the length of the sequence (suffix digits) cat cud_test.tar.gz_* &gt; cud_test.tar.gz : to concatenate split files; Before starting using commands, let’s introduce wildcards that provide special characters to help you quickly specify a group of filenames. Wildcard Implication * match any sequence of characters, including zero characters. ? match any single character (excluding zero character) [character] match any single character within the specified character set [!character] match any single character without the specified character set [[:class:]] match any single character within the specified character class The following table lists the most commonly used character classes. Character Class Implication [:alnum:] match any single letter or digit [:alpha:] match any single letter [:digit:] match any single digit [:lower:] match any single lower letter [:upper:] match any single upper letter There are some examples constructed with wildcard: *, g*, b*.txt, Data???, [abc]*, BACKUP.[0-9][0-9][0-9], [[:upper:]], [![:digit:]]*, *[[:lower:]123] Service and Systemctl systemctl is a system service manager that actually combines the service and chkconfig commands together. 123456789systemctl is-enabled httpd.servicesystemctl status httpd.servicesystemctl list-unit-files --type=servicesystemctl restart httpd.servicesystemctl stop httpd.servicesystemctl reload httpd.servicesystemctl restart httpd.servicesystemctl enable httpd # bootstrapsystemctl disable httpd # not bootstrap service 12345service ssh startservice ssh stopservice ssh restartservice ssh statusservice --status-all","tags":[{"name":"linux","slug":"linux","permalink":"https://stu-yue.github.io/tags/linux/"},{"name":"shell","slug":"shell","permalink":"https://stu-yue.github.io/tags/shell/"}],"categories":[{"name":"languages","slug":"languages","permalink":"https://stu-yue.github.io/categories/languages/"},{"name":"shell","slug":"languages/shell","permalink":"https://stu-yue.github.io/categories/languages/shell/"}]},{"title":"Design Mode","date":"2023-07-22T12:00:53.000Z","path":"jottings/interview/memo_design_mode/","text":"笔记参考主要自 https://refactoringguru.cn/design-patterns/catalog 0 UML类图关系 依赖关系：一种使用关系，一个类的实现需要另一个类的协助 代码表现：局部变量、方法的参数或静态方法的调用； 箭头方向：带虚线箭头，箭头从使用类指向被使用类； 123456classDiagram Programmer ..&gt; Computer class Programmer &#123; -name: String +coding(Computer c) &#125; 关联关系：一种引用关系，使一个类知道另一个类的属性和方法；如老师与学生； 代码表现：成员变量； 箭头方向：单向关联（箭头实线指向被拥有者），双向关联（可双箭头&#x2F;也可都没有） 12345678910classDiagram Teacher--Student Student--&gt;Course class Teacher &#123; -students: List&amp;ltStudent&amp;gt &#125; class Student &#123; -teacher: Teacher -courses: List&amp;ltCourse&amp;gt &#125; 聚合关系：关联关系的一种，整体与与部分的关系，且部分可以离开整体而单独存在。 代码表现：成员变量 箭头方向：带空心菱形的实心线，菱形指向整体 12345classDiagram School o-- Teacher class School &#123; -teacher: List&amp;ltTeacher&amp;gt &#125; 组合关系：联关系的一种，整体与与部分的关系，且部分不可以离开整体而单独存在。 代码表现：成员变量 箭头方向：带实心菱形的实线，菱形指向整体 12345classDiagram Body *-- Brain class Body &#123; -brain: Brain &#125; 实现关系：接口和实现类，带三角箭头的实线指向接口； 泛化关系：父子类之间的继承关系，带三角箭头的虚线指向父类； 12345classDiagram University &lt;|.. Tsinghua University &lt;|.. Peking Animal&lt;|--Bird Animal&lt;|--Lion 1 单例模式 核心思想：保证一个类只有一个实例，并提供一个全局访问点来访问这个实例。 优点： 全局控制：可以严格的控制客户怎样访问&#x2F;何时访问，即对唯一实例的受控访问。 节省资源：单例避免多次创建了相同的对象，节省了系统资源，且多个模块可以通过单例实例共享数据。 懒加载：在需要时才实例化，可以提高程序的性能。 基本组成：私有的构造函数、静态实例变量，公有的静态方法； 123456classDiagram class Singleton&#123; -instance: Singleton - Singleton() + GetInstance() &#125; 实现： 饿汉模式：在类加载时就已经完成了创建，可能造成资源浪费，但一般不存在多个线程同时尝试初始化实例的问题。 123456789101112// hppclass Singleton &#123; public: static Singleton&amp; getInstance() &#123; return s_instance; &#125; Singleton(const Singleton&amp;) = delete; Singleton&amp; operator=(const Singleton&amp;) = delete; private: Singleton() = default; static Singleton s_instance;&#125;// cppSingleton Singleton::s_instance; 懒汉模式：首次调用才被创建，避免资源浪费（如果Instance类很大） 12345678910111213141516171819202122// Double-Checked LockingClass Singleton &#123; public: static Singleton* getInstance() &#123; if (!p_instance) &#123; // 减少对同步锁的竞争，提高效率 lock_guard&lt;mutex&gt; lck(mtx); if (!p_instance) &#123; p_instance = new Singleton(); &#125; &#125; return p_instance; &#125; Singleton(const Singleton&amp;) = delete; Singleton&amp; operator=(const Singleton&amp;) = delete; private: Singleton() = default; static Singleton* p_singleton; static mutex mtx;&#125;;// cppSingleton* Singleton::p_singleton = nullptr;mutex Singleton::mtx; C++11标准规定了局部静态变量的初始化必须是线程安全的，因此可利用局部静态变量来实现单例。 当多个线程同时到达局部静态变量的初始化语句时，保证只有一个线程将执行初始化代码，而其他线程将等待这个初始化完成。 123456789101112// Meyer&#x27;s Singletonclass Singleton &#123; public: static Singleton&amp; getInstance() &#123; static Singleton s_instance; return s_instance; &#125; Singleton(const Singleton&amp;) = delete; Singleton&amp; operator=(const Singleton&amp;) = delete; private: Singleton() = default;&#125;; 2 工厂模式2.1 简单工厂模式 核心思想：将各种产品对象的创建过程封装在一个工厂类中。 基本组成：工厂、抽象产品（接口）、具体产品； 缺点：对修改不封闭，新增产品需要修改工厂，违反了OCP原则； 2.2 工厂模式 核心思想：引入抽象工厂和具体工厂的概念，每个具体工厂负责创建对应的具体产品，新增产品秩只需添加新的工厂类而无需修改原来的代码； 基本组成：抽象工厂（接口）、具体工厂、抽象产品（接口）、具体产品； 优点：新增产品更灵活、支持扩展，符合开闭原则； 应用场景： Spring 框架中的 Bean 工厂：通过配置文件或注解，Spring 可以根据配置信息动态地创建和管理对象。 JDBC 中的 Connection 工厂：在 Java 数据库连接中， DriverManager 使用工厂方法模式来创建数据库连接。不同的数据库驱动（如 MySQL、PostgreSQL 等）都有对应的工厂来创建连接。 2.3 抽象工厂模式 核心思想： 简单工厂，一个工厂创建所有具体产品； 工厂方法，一个工厂创建一个具体产品； 抽象工厂，一个工厂创建一类具体产品； 基本组成： 123456789101112131415161718// Simple Factoryfactory.create(&#x27;A&#x27;); // product Afactory.create(&#x27;B&#x27;); // product B// Factory Method// factory_ab &lt;|-- factory_a// factory_ab &lt;|-- factory_bfactory_a.create(); // product Afactory_b.create(); // product B// Abstract Factory// factory &lt;|-- factory_1/2// product_a &lt;|-- product_a_1/2// product_b &lt;|-- product_b_1/2factory_1.create_a(); // product A1factory_1.create_b(); // product B1factory_2.create_a(); // product A2factory_2.create_b(); // product B2 3 建造者模式 核心思想：将对象的构建过程分为多个步骤，并为每个步骤定义一个抽象的接口。具体的构建过程由实现了这些接口的具体建造者类来完成。 而指导者类负责按照一定的顺序来执行构建步骤，最终生成产品。 基本组成： 1234567891011121314151617181920212223classDiagram AbstractBuilder&lt;|--Builder1 AbstractBuilder&lt;|--Builder2 AbstractProduct&lt;|--Product1 AbstractProduct&lt;|--Product2 Director o--AbstractBuilder Builder1 ..&gt; Product1 Builder2 ..&gt; Product2 class AbstractBuilder&#123; - product: AbstractProduct +BuildPartA() +BuildPartB() +GetProuct() &#125; class Director&#123; -builder: AbstractBuilder +construct() &#125; class AbstractProduct &#123; -partA -partB &#125; 12345678910111213141516// Directorclass Director &#123; public: explicit Director(Builder* builder): builder_(builder) &#123;&#125; void construct() &#123; builder_-&gt;BuildPartA(); builder_-&gt;BuildPartB(); &#125; private: Builder* builder_;&#125;// ClientBuilder* builder = new ConcreteBuilder();Director* director = new Director(builder);director-&gt;construct();Product product = builder-&gt;GetProduct(); 优点： 封装性好，构建和表示分离； 扩展性好，各个具体的建造者相互独立，利于系统解耦； 控制风险细节，客户端无需知道细节，建造者细化创建过程； 缺点： 产品的组成部分必须相同，限制了其使用范围； 产品内部发生变化，建造者需同步修改，后期维护成本较大； 4 原型模式💬 4.1 意图基于现有的对象创建新的对象，将原型对象的成员变量复制到新生成的对象中，而不需使代码依赖它们所属的类。 🙁 4.2 问题 直接复制对象（“从外部” 复制对象并非总是可行），必须知道对象所属的类才能创建复制品， 所以代码必须依赖该类； 此外，有时只知道对象所实现的接口， 而不知道其所属的具体类，比如向方法的某个参数传入实现了某个接口的任何对象； 😊 4.3 解决方案原型模式将克隆过程委派给被克隆的实际对象。 模式为所有支持克隆的对象声明了一个通用接口， 该接口让你能够克隆对象， 同时又无需将代码和对象所属类耦合。 所有的类对 clone方法的实现都非常相似。 该方法会创建一个当前类的对象， 然后将原始对象所有的成员变量值复制到新建的类中。 支持克隆的对象即为原型。 当你的对象有几十个成员变量和几百种类型时， 对其预生成原型可以代替子类的构造（避免子类实例化的复杂，无需关心该实例是如何通过子类化构造出来的）。 其运作方式如下： 创建一系列不同类型的对象并不同的方式对其进行配置。 如果所需对象与预先配置的对象相同， 那么你只需克隆原型即可， 无需新建一个对象。 🚀 4.4 原型模式结构基本实现 原型 （Prototype） 接口将对克隆方法进行声明。 在绝大多数情况下， 其中只会有一个名为 clone克隆的方法。 具体原型 （Concrete Prototype） 类将实现克隆方法。 除了将原始对象的数据复制到克隆体中之外， 该方法有时还需处理克隆过程中的极端情况， 例如克隆关联对象和梳理递归依赖等等。 克隆方法通常只有一行代码： 使用 new运算符调用原型版本的构造函数。 客户端 （Client） 可以复制实现了原型接口的任何对象。 原型注册表实现 原型注册表 （Prototype Registry） 提供了一种访问常用原型的简单方法， 其中存储了一系列可供随时复制的预生成对象。 💡 4.5 适用场景 如果你需要复制一些对象， 同时又希望代码独立于这些对象所属的具体类； 动态生成对象：原型模式允许在运行时动态生成对象的副本。 例如，在某些需要根据外部输入或者配置文件来创建对象的应用中，使用预生成原型可以简化对象的生成过程，因为你可以在运行时选择合适的原型进行复制，而不是靠硬编码的方式来决定使用哪个子类。 对象状态的复用：对于一些初始化资源消耗较大的对象，如那些需要从数据库加载数据的对象，新创建的对象可以自动继承其原型对象的状态； 🎲 4.6 优缺点✔ 可以克隆对象，无需与它们所属的类相耦合；动态生成对象副本； ✔ 可以克隆预生成原型，避免反复运行初始化代码；对象状态的复用； ❌ 克隆包含循环引用的复杂对象可能会非常麻烦。 📜 4.7 示例代码12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697using std::string;enum Type &#123; PROTOTYPE_1 = 0, PROTOTYPE_2&#125;;class Prototype &#123; protected: string prototype_name_; float prototype_field_; public: Prototype() &#123;&#125; Prototype(string prototype_name) : prototype_name_(prototype_name) &#123; &#125; virtual ~Prototype() &#123;&#125; virtual Prototype *Clone() const = 0; virtual void Method(float prototype_field) &#123; this-&gt;prototype_field_ = prototype_field; std::cout &lt;&lt; &quot;Call Method from &quot; &lt;&lt; prototype_name_ &lt;&lt; &quot; with field : &quot; &lt;&lt; prototype_field &lt;&lt; std::endl; &#125;&#125;;class ConcretePrototype1 : public Prototype &#123; private: float concrete_prototype_field1_; public: ConcretePrototype1(string prototype_name, float concrete_prototype_field) : Prototype(prototype_name), concrete_prototype_field1_(concrete_prototype_field) &#123; &#125; Prototype *Clone() const override &#123; return new ConcretePrototype1(*this); &#125;&#125;;class ConcretePrototype2 : public Prototype &#123; private: float concrete_prototype_field2_; public: ConcretePrototype2(string prototype_name, float concrete_prototype_field) : Prototype(prototype_name), concrete_prototype_field2_(concrete_prototype_field) &#123; &#125; Prototype *Clone() const override &#123; return new ConcretePrototype2(*this); &#125;&#125;;class PrototypeFactory &#123; private: std::unordered_map&lt;Type, Prototype *, std::hash&lt;int&gt;&gt; prototypes_; public: PrototypeFactory() &#123; prototypes_[Type::PROTOTYPE_1] = new ConcretePrototype1(&quot;PROTOTYPE_1 &quot;, 50.f); prototypes_[Type::PROTOTYPE_2] = new ConcretePrototype2(&quot;PROTOTYPE_2 &quot;, 60.f); &#125; ~PrototypeFactory() &#123; delete prototypes_[Type::PROTOTYPE_1]; delete prototypes_[Type::PROTOTYPE_2]; &#125; Prototype *CreatePrototype(Type type) &#123; return prototypes_[type]-&gt;Clone(); &#125;&#125;;void Client(PrototypeFactory &amp;prototype_factory) &#123; std::cout &lt;&lt; &quot;Let&#x27;s create a Prototype 1\\n&quot;; Prototype *prototype = prototype_factory.CreatePrototype(Type::PROTOTYPE_1); prototype-&gt;Method(90); delete prototype; std::cout &lt;&lt; &quot;\\n&quot;; std::cout &lt;&lt; &quot;Let&#x27;s create a Prototype 2 \\n&quot;; prototype = prototype_factory.CreatePrototype(Type::PROTOTYPE_2); prototype-&gt;Method(10); delete prototype;&#125;int main() &#123; PrototypeFactory *prototype_factory = new PrototypeFactory(); Client(*prototype_factory); delete prototype_factory; return 0;&#125; 设计原则 OCP (Open&#x2F;Closed Principle): 软件实体（类&#x2F;模块&#x2F;函数等）应该是可扩展的，但是不可修改的。 RAII (Resource Acquisition Is Initialization): 确保所有资源（如指针、文件句柄、网络连接等）都封装在类对象里，让对象的生命周期管理资源。 资源的获取（Allocation）应在对象构造时完成 资源的释放（Deallocation）自动在对象的析构时执行 异常安全：RAII保证无论对象是因为作用域结束还是因为异常而离开作用域，资源的释放将总是自动发生。 当C++程序中抛出一个异常时，程序控制流会从抛出异常的点跳转到能够处理该异常的捕捉点（catch block）。在这一过程中，程序会展开堆栈，退出当前作用域及其父作用域，直到找到合适的异常处理代码。在展开堆栈的过程中，局部对象会被销毁，其析构函数按照创建的逆序被调用。因此，RAII类的析构函数将负责释放其管理的资源，即使是在异常发生时。 优点： 代码更加简洁明了、易于维护； 自动资源管理、减少了资源泄露的风险； 提高了代码的异常安全性；","tags":[{"name":"interview","slug":"interview","permalink":"https://stu-yue.github.io/tags/interview/"}],"categories":[{"name":"interview","slug":"interview","permalink":"https://stu-yue.github.io/categories/interview/"}]},{"title":"Hot Points","date":"2023-07-22T12:00:53.000Z","path":"jottings/tidbits/hot_points/","text":"Generate random numbers by reading thermal noise on CPU pins. Hot Plug, refers to the operation of inserting&#x2F;removing a device while it is running. In contrast, Cold Plug means do above operation while the device is powered off. Software version’s explanation: Version Description Snapshot Development version Alpha Internal beta Beta Public beta Pre (M) Similar to Alpha, sometimes subdivided into M_(Milestone) version RC(Release Candidate) During Beta stage, new features will continue to be added, but once the RC version is reached, there will mainly be on debugging and issue resolution. GA(General Availability) Some software may be labeled as “stable” or “production”. Release&#x2F;Stable Current The latest version, but no necessarily a stable one. Eval There may be a monthly or fixed time limit for usage. Declarative Programming expresses the logic of a computation without describing its control flow. Many languages that apply this style attempt to minimize or eliminate side effects by describing what the program must accomplish in terms of the problem domain (what to do), rather than specifying all the details of how the program should achieve the result (how to do). Imperative Programming uses statements that change a program’s state. Imperative Programming focuses on describing how a program operates step by step, rather than on high-level descriptions of its expected results. KVM: Kernel-based Virtual Machine, is a virtualization infrastructure used in the Linux kernel to turn the Linux kernel into a hypervisor; Hypervisor: creates and runs virtual machines, sometimes called a virtual machine monitor (VMM), like a meta-OS; IT resource pooled, OS and hardware decoupled, allocated according to needs; A computer on which a hypervisor runs one or more virtual machines is called a host machine, and each virtual machine is called a guest machine; (Type 1) Native or bare mental hypervisor: runs directly on the host’s hardware to manage guest operating systems. It takes the place of a host operating system and VM resources are scheduled directly to the hardware by the hypervisor. This type of hypervisor is most common in an enterprise data center or other server-based environments. KVM, Microsoft Hyper-V, and VMware vSphere are examples of a type 1 hypervisor. KVM was merged into the Linux kernel in 2007, so if you’re using a modern version of Linux, you already have access to KVM. (Type 2) Hosted hypervisor: run on a conventional operating system as a software layer or application. It works by abstracting guest operating systems from the host operating system. VM resources are scheduled against a host operating system, which is then executed against the hardware. A type 2 hypervisor is better for individual users who want to run multiple operating systems on a personal computer. VMware Workstation and Oracle VirtualBox are examples of a type 2 hypervisor. (Emulator) The difference with the Hypervisor is that the Hypervisor runs the same VM and host CPU architecture, while the Emulator can be used to run systems or programs on other hardware platforms (arm, mips, x86). When used as a machine emulator, QEMU can run OSes and programs made for one machine (e.g. an ARM board) on a different machine (e.g. your x86 PC). By using dynamic translation, it achieves very good performance; QEMU can use other hypervisors like Xen or KVM to use CPU extensions (HVM) for virtualization. When used as a virtualizer, QEMU achieves near native performances by executing the guest code directly on the host CPU. When QEMU is used in conjunction with KVM, KVM provides hardware virtualization support, while QEMU is responsible for virtual machine simulation and management. In this configuration, KVM functions as a Hypervisor to directly interact with hardware and provide high-performance virtualization support, while QEMU runs on the upper layer of KVM and is responsible for VM simulation and management, including device simulation, VM creation, start, and stop. So while QEMU is not a traditional Hypervisor on its own, when used in conjunction with KVM, QEMU can work with KVM to provide a complete virtualization solution and act as part of a Hypervisor","tags":[{"name":"points","slug":"points","permalink":"https://stu-yue.github.io/tags/points/"}],"categories":[{"name":"tidbits","slug":"tidbits","permalink":"https://stu-yue.github.io/categories/tidbits/"}]},{"title":"TBR_compiler","date":"2023-07-22T12:00:53.000Z","path":"jottings/tidbits/tbr_compiler/","text":"《编译原理》课件搬运 ——&gt; 源地址 1.1 编译器作用 编译器（Compiler） 读入以某种语言（源语言）编写的程序； 输出等价的用另一种语言（目标语言）编写的程序； 通常目标程序是可执行的； 解释器（Interpreter） 不生成目标程序，直接根据源程序的语义执行源程序中指定的操作； Java语言的处理结合了编译(.class字节码)和解释，Python会将编译的字节码(pyc)存放于_pycache_(可跨平台部署，一定程度防源码泄露，import的py不变pyc就不变[检查py和pyc的修改时间是否一致]) 1.2 编译器的结构 编译器可以分为分析部分和综合部分 分析（analysis）部分 &#x2F; 前端（front end） 把源程序分解成组成要素，以及相应的语法结构； 使用这个结构创建源程序的中间表示； 同时收集和源程序相关的信息，存放到符号表； 综合（synthesis）部分 &#x2F; 后端（back end） 根据中间表示和符号表信息构造目标程序； 前端部分是机器无关的，后端部分是机器相关的； 编译器分成执行顺序的一组步骤： 1.3 词法分析 词法分析&#x2F;扫描（lexical analysis&#x2F;scanning） 读入源程序的字符流，输出为有意义的词素（lexeme） &lt;token-name, attribute-value&gt; token-name：由语法分析步骤使用； attribute-value：指向相应的符号表条目，由语义分析&#x2F;代码生成步骤使用； 例子 position &#x3D; initial + rate * 60 &lt;id, 1&gt; &lt;&#x3D;, &gt; &lt;id, 2&gt;, &lt;+, &gt; &lt;id, 3&gt; &lt;*, &gt; &lt;number, 4&gt; 1.4 语法分析 语法分析&#x2F;解析（syntax analysis&#x2F;parsing） 根据各个词法单元的第一个分量来创建树型的中间表示形式，通常是语法树（syntax tree） 中间表示形式指出了词法单元流的语法结构； 1.5 语义分析 语义分析（semantic analysis） 使用语法树和符号表中的信息，检查源程序是否满足语言定义的语义约束； 同时收集类型信息，用于代码生成、类型检查、类型转换； 1.6 中间代码生成 根据语义分析输出，生成类机器语言的中间表示； 三地址代码（three-address code） 每个指令最多包含三个运算分量；很容易生成机器语言指令； 1.7 中间代码优化 通过对中间代码分析，改进中间代码的质量；（更快、更短、能耗更低） 1.8 代码生成 把中间表示形式映射成目标语言；（寄存器分配、指令选择） 1.9 其他 符号表管理：记录源程序中使用的变量的名字，收集各种属性； 编译器构造工具：扫描器（Lex）、语法分析器（Yacc）、语法制导的翻译引擎； 程序设计语言的新发展向编译器设计者提出新的要求 设计相应的算法和表示方法来翻译和支持新的语言特征，如多态、动态绑定、类、类属 (模板) 、… 编译器设计者还需要更好地利用新硬件的能力 RISC技术、多核技术、大规模并行技术 1.10 程序设计语言的基础概念 静态&#x2F;动态： 静态：支持编译器静态决定某个问题； 动态：只允许在程序运行时刻作出决定； 作用域： x的作用域指程序文本的一个区域，其中对x的使用都指向这个声明； 静态作用域（static scope）: 通过静态阅读程序可决定； 动态作用域（dynamic scope）: 运行时确定x的指向； 环境与状态： 环境（environment）: 是从名字到存储位置的映射； 状态（state）: 从存储位置到它们值的映射；","tags":[{"name":"points","slug":"points","permalink":"https://stu-yue.github.io/tags/points/"}],"categories":[{"name":"tidbits","slug":"tidbits","permalink":"https://stu-yue.github.io/categories/tidbits/"}]},{"title":"TBR_Net_Info","date":"2023-07-22T12:00:53.000Z","path":"jottings/tidbits/tbr_net_info/","text":"1 IPTV 和 OTTIPTV（Internet Protocol Television）是一种通过Internet Protocol（IP）在专用网络上传输视频和音频内容的技术。它使用LAN（局域网）或WAN（广域网）等封闭网络生态系统进行操作。IPTV需要专门的设备（如机顶盒）和宽带连接，用户可以通过这些设备访问传统的电视频道，如CBS、CNN、FOX、HBO等。IPTV的优势在于提供高质量的图像和声音，因为它使用专用网络进行传输，不会占用用户的互联网带宽[2]。 OTT（Over-The-Top）是一种通过公共互联网传输视频内容的技术。OTT服务基于视频点播（VOD）的分发，用户只需订阅服务，就可以通过连接到互联网的设备播放内容。OTT服务的优势在于成本较低、安装简便，并且提供各种各样的节目选择。然而，OTT的视频质量取决于用户的互联网连接速度，可能会出现缓冲导致内容中断的情况[2]。 以下是IPTV和OTT之间的主要区别： 内容传递和网络结构： IPTV使用封闭网络（LAN、WAN），提供平稳的内容传递，不依赖于互联网带宽。 OTT使用公共互联网连接，任何人都可以访问，但服务质量取决于用户的互联网速度。 设备需求： OTT不需要额外的硬件设备，只需使用专用应用程序即可。 IPTV需要专门的机顶盒和路由器。 视频质量： OTT的视频质量取决于互联网速度。 IPTV提供更流畅、更好的用户体验，具有更高的视频质量和音频效果。 价格： OTT服务通常是免费的，但可能需要付费订阅以去除广告。 IPTV的价格较高，平均每月费用在15美元至30美元之间。 内容类型： OTT提供免费或付费的内容，也可以是混合内容。 IPTV提供独家的付费内容，如订阅视频点播（SVOD）。 OTT+直播？ 2 运营商、ISP、ICP、骨干网 电信运营商：指提供基础的电信业务，包括固网（有线），移动网络（无线）的运营商（电信&#x2F;联通&#x2F;移动是一级&#x2F;基础运营商）。 ISP：多指互联网接入服务提供商，长城宽带等二级运营商（通过购买电信运营商的带宽来提供互联网接入服务） ICP：内容提供商指提供互联网上各种内容的服务提供商，例如视频、音乐、新闻、社交媒体等。在中国，一些知名的内容提供商包括爱奇异、优酷等。 内容提供商通常将其内容托管在电信运营商的机房内，用户通过购买ISP的服务来访问这些内容。 局域网：几台计算机互联（计算机组）（光纤【很细的光缆】&#x2F;wifi）； 四种技术类型&#x2F;标准：以太网、令牌环、令牌总线、光纤分布式数据接口(FDDI)； 城域网：城市范围内的计算机通信网（大型LAN）（光缆）； 单独列出，由于有一个独有标准：分布式队列双总线DQDB（Distributed Queue Dual Bus） 骨干网：通常是一个国家或地区范围内的网络，用于连接不同城市或地区的网络； 广域网：跨越多个城市、国家或洲际，连接不同的局域网和城域网（可包含多个骨干网） 中国主要的骨干网： 中国电信 CHINANET：中国公用计算机互联网，原邮电部 中国移动 CMNET：中国移动互联网 中国联通 UNINET：中国联通计算机互联网，还有子公司网通的中国网通公用互联网（CNCNET） 中科院 CSTNET：中国科技网 信息产业部 CHINAGBN：中国金桥信息网 教育部 CERNET：中国教育和科研计算机网，国家公用经济信息通信网 12345移动 铁通 电信 联通(CDMA) 卫通 联通(GSM) 网通 | | | | / \\ | | \\ / \\ | / \\ \\ / \\ / \\ | / | \\ / 移动 电信 航天科技 联通 SDN是典型的网络技术，NFV是典型的计算技术；SDN主要应用于承载网，NFV主要应用于核心网和接入网；SDN是转发与控制的解耦，NFV是软件与硬件的解耦。 NFV，全称Network Function Virtualization，网络功能虚拟化。所谓的网络功能，是指网络设备的功能；而虚拟化，是云计算的核心技术。 从前的移动通信网络，尤其是核心网，是由很多专用硬件设备组成的，在专用设备运行对应的软件服务，保证功能实现。但是在5G核心网中，已经没有任何专用硬件，完全采用X86通用服务器，软件服务运行在虚拟机和容器上。 这就是采用了NFV技术，将通信设备网元云化，实现软件和硬件的解耦，运行商不需要再购买专用硬件设备，减少了硬件资金的投入。 另外，NFV还具备自动部署、弹性伸缩、故障隔离和自愈性高等优点，可以大幅提升网络运维效率、降低故障风险和能耗。 SDN，全称是Software Defined Networking，软件定义网络。 它的核心思想就是将控制平面与转发平面解耦，引入网络可编程能力，来提升系统灵活性。 控制功能全部集中到控制器中，路由器交换机只负责按照Flowtable转发即可。承载网引入了SDN技术，路由器只转发，SDN控制器建立在云平台上，向上层用户提供接口服务。 类型 SDN NFV 目的 转发和控制分离，控制面集中，网络可编程 将网络功能从原来的专用服务器转移到通用设备 针对场景 数据中心、园区网、运营商网络 运营商网络 针对设备 商用交换机等数通设备 专用服务器 ISO层级 2-3层 4-7层 接入网（接入层）：无线接入、有线接入 传输网（骨干网）：主要为OTN（光传输网络）","tags":[{"name":"points","slug":"points","permalink":"https://stu-yue.github.io/tags/points/"}],"categories":[{"name":"tidbits","slug":"tidbits","permalink":"https://stu-yue.github.io/categories/tidbits/"}]},{"title":"Summa | The Majority of Memory Categories","date":"2023-07-20T08:41:53.000Z","path":"jottings/architecture/summa_memory_categories/","text":"Reproduced in 8 types of memory every embedded engineer should know about! The majority of memory can be categorized as shown in the following picture: The real difference between primary and secondary memories is the speed&#x2F;volatility(without power) tradeoffs. Primary MemoryPrimary memory is very fast, but it cannot hold data without power. The popular name for Primary Memory is RAM, which has 2 most important types namely SRAM and DRAM. Bistable Circuit usually consists of two complementary transistors or other switching devices, one used to hold the circuit in one stable state and the other to switch the circuit to another stable state. The switch between these two states is triggered by the input signal. Bistable Circuit can store data. Latches and registers are bistable devices. SRAM is the use of bistable flip-flop to save information, as long as the power is not lost, the information is not lost. DRAM uses capacitors to store charge to store information, so data stored in the DRAM must be constantly refreshed every few milliseconds or else it will end up being erased. The action is taken care of by a special device named DRAM controllers. SRAM DRAM Construction Principle It uses a cross-coupled flip flop configuration of transistors It uses a capacitor transistor circuit to hold data Cost Relatively more expensive, it needs more transistors per bit of data it can store Relatively less expensive, as fewer transistors per bit of storage are needed Speed Faster Slower (capacitor charge and discharge time) Volatility As long as power is ON, it can store data since it uses no capacitors Data needs to be continuously refreshed (usually in the order of 4 times a second) since the capacitors leak power. Power consumption Less More Density Less dense (6 transistors, more area needed) Highly dense (1 pass transistor and 1 capacitor, easy to integrate) Addition components needed None DRAM controllers are needed to make it work like an SRAM. This controller offloads the data refreshing duties of a microprocessor and hence a DRAM coupled with a DRAM controller behaves more like an SRAM from the processor’s perspective. Application areas Cache memory (Ls2) Main memory (memory chips) NVRAM or Non Volatile Random Access Memory, is a special type of RAM that can store data permanently. It’s basically an SRAM with a power supply Secondary MemoryROM MASK ROM: The main characteristic of this device is the fact that the data is written onto the device as it gets manufactured and it is impossible to change them. This is done by designing the chip in such a manner so that it already contains the necessary data. In order to mass production, the manufacturer makes a ROM or EPROM with original data as a sample in advance, and then mass-produces the same ROM as the sample. This kind of ROM sample for mass production is the MASK ROM, and the data burned in the MASK ROM can never be modified. PROM stands for Programmable Read-Only Memory. These are programmable chips for user needs, the main characteristic being it can only be programmed one time. That is it cannot be erased or reprogrammed. They are also known as One Time Programmable devices or OTPs for short. EPROM stands for Erasable Programmable Read-Only Memory. These chips usually have a small glass window on top and if you expose them to direct sunlight(UV, ultraviolet) that will erase the chip’s data. They can then be programmed again with fresh data. Cons: Inconvenient, instability, can not be exposed to the light source otherwise easy to lead to data loss EEPROM stands for Electrically Erasable Programmable Read-Only Memory. These chips can be erased and reprogrammed using electricity as opposed to exposing them to UV rays as EPROMs. EEPROM can be erased and reprogrammed on a computer or dedicated device, generally plugging and playing. FLASH MEMORY The basic storage unit of flash memory is the transistor-based storage unit, and each storage unit can store 1 bit of data. Storage units are usually organized into a block, and each block contains thousands of storage units. Each storage unit has a floating gate to store electric charges. The state of a storage unit can be determined as “1” or “0” based on the amount of electric charges stored in the floating gate. The state of a storage unit is changed by injecting or extracting electrons into&#x2F;from the floating gate to modify the amount of electric charges stored in it. Flash memory uses Hot carrier injection(HCI) mechanism to write data. In simple terms, a certain storage unit is grounded at the source, a positive voltage is applied to its control gate, and a positive voltage is applied to the drain to generate a strong electric field between the source and drain. This will give electrons enough energy (hot carriers) to be attracted by the voltage at the control gate and injected into the floating gate. Afterwards, as the insulating material on the top and bottom of the floating gate is not conductive, these electrons are trapped in the floating gate and cannot escape. (Every time electrons enter and exit the surrounding silicon dioxide on the floating gate, it will cause aging[1]) To save cost, flash memory adopts page programming mode. Each page contains a certain number of storage units, and all units in a page are written at the same time. Cons: Flash memory has the issue of wear-out (write&#x2F;erase endurance limits), which is usually mitigated by disabling bad blocks, and then reducing usable capacity. USB flash disk, namely “U disk”, is a new generation of storage devices based on USB interface and flash memory chip as storage medium. It is basically composed of five parts: USB port, main control chip, flash memory chip, PCB backboard, outer package. USB flash drives, SD cards, and SSDs are a type of storage device that uses flash memory chips as the storage medium. They are primarily composed of a controller (main control) and flash memory chips, and have no mechanical structure, consisting purely of electronic circuitry. They are resistant to physical shocks and impacts. The controller manages data storage and other functions. Even after power loss, data remains stored in the memory cells. USB flash drives generally have a cache on them to prevent the loss of data copies from being quickly plugged in&#x2F;out. OTHER Hard Disk Drive: The disk reads data according to the polarity of the magnetic particle and writes data according to the polarity of the magnetic head. Compact Disk: CD-ROM can only be read and not written to because after being burned once, each unit has a fixed different reflectivity (the reading probe emits laser and the reflected laser is read as “1”, or non-reflected laser is read as “0”). Floppy Disk: A type of magnetic disk, less capacity, slower speed.","tags":[{"name":"memory","slug":"memory","permalink":"https://stu-yue.github.io/tags/memory/"}],"categories":[{"name":"architecture","slug":"architecture","permalink":"https://stu-yue.github.io/categories/architecture/"}]},{"title":"Welcome to Yue's Jotter","date":"2023-07-13T08:23:57.000Z","path":"jottings/intro/","text":"","tags":[],"categories":[]},{"title":"Summa | Firmware and Drivers","date":"2023-07-13T06:21:53.000Z","path":"jottings/architecture/summa_firmware_and_drivers/","text":"Reproduced in Firmware vs Device Drivers: Explained with Examples! Firmware vs. Device Drivers Firmware Device Drivers Firmware is a class of software that is written for specific custom hardware. Device drivers are software that is needed to make a given hardware accessory Firmware lives and runs directly on the hardware. Device drivers live on hard-disk and run on the CPU. Firmware is independent of an operating system, i.e., you can run any operating system on top of a given motherboard’s firmware. Device drivers are highly dependent on the operating system on which they are used. For example, for the same hardware device, you need different device drivers for using that on Windows vs Linux. Firmware cannot be updated through an operating system, we need to go into the BIOS&#x2F;UEFI to update the device drivers. Drivers can be updated from within the operating system. Firmware engineers do not need any knowledge of operating systems. But they need core knowledge on processors and the latest RAMs, PCIe standards, and so on to write firmware that complies with the latest standards Device driver engineers need knowledge about the specific device that they are using, the communication standard the device uses to talk to the computer (like Bluetooth, USB, etc), and the operating system the device driver is written for. Firmware is written by motherboard manufacturers Drivers are written by engineers in companies that produce hardware accessories that connect to your computer Examples include the BIOS&#x2F;UEFI interface that comes with the computer’s motherboard Examples include special software you install to handle the extra buttons on your mouse, software that comes with any non-standard hardware like special game controllers, also the software that helps us use all the standard hardware like USB storage devices, keyboards, mice, headphones, etc. Layers of software on a typical computer are shown in the following figure: Firmware​ Firmware is a computer program that is written to work directly on specific custom hardware and it lives in non-volatile memory such as a flash chip and it is executed directly from it. The job of the firmware is to make the hardware accessible to the operating system. Firmware can be thought of as the glasses through which the operating system can see the actual hardware! ​ Originally Firmware is written on Masked ROMs, which is a special type of memory that can be programmed&#x2F;written-data-to only once. The products were then shipped with these unchangeable programs called firmware and they run for ages till the device goes out of use. ​ The first replacement of Masked ROMs came in the form of EPROM which can be erased by exposure to UV light and then reprogrammed as required. Then came EEPROMs which used electricity to change the contents. Nowadays the Masked ROMs have been replaced with Flash memory, which is cheaper and serves the purpose. Device DriversDevice drivers are programs that can control a given hardware and provide a software interface to it. Other programs like Operating Systems can interact with the hardware through this software interface without needing to know the actual underlying implementation of the software interface. The relation graph between firmware and drivers is also shown below: ​ Generally speaking, drivers and firmware together form the module that operates hardware. But why not make the firmware perfect so that it doesn’t require driver support? ​ The answer to the above question is, there are different operating systems which have completely different ways of operating hardware. So, on the one hand, hardware manufactures need to write firmware to make their hardware easier to use with software, but on the other hand, they cannot make the firmware too rigid in order to be compatible with various operating systems. They must leave enough room for software to freely operate —— and that’s where drivers come in.","tags":[{"name":"firmware","slug":"firmware","permalink":"https://stu-yue.github.io/tags/firmware/"},{"name":"driver","slug":"driver","permalink":"https://stu-yue.github.io/tags/driver/"}],"categories":[{"name":"architecture","slug":"architecture","permalink":"https://stu-yue.github.io/categories/architecture/"}]}],"categories":[{"name":"interview","slug":"interview","permalink":"https://stu-yue.github.io/categories/interview/"},{"name":"Hot100","slug":"interview/Hot100","permalink":"https://stu-yue.github.io/categories/interview/Hot100/"},{"name":"mathematics","slug":"mathematics","permalink":"https://stu-yue.github.io/categories/mathematics/"},{"name":"statistics_ml","slug":"statistics-ml","permalink":"https://stu-yue.github.io/categories/statistics-ml/"},{"name":"architecture","slug":"architecture","permalink":"https://stu-yue.github.io/categories/architecture/"},{"name":"tools","slug":"tools","permalink":"https://stu-yue.github.io/categories/tools/"},{"name":"docker","slug":"tools/docker","permalink":"https://stu-yue.github.io/categories/tools/docker/"},{"name":"languages","slug":"languages","permalink":"https://stu-yue.github.io/categories/languages/"},{"name":"shell","slug":"languages/shell","permalink":"https://stu-yue.github.io/categories/languages/shell/"},{"name":"tidbits","slug":"tidbits","permalink":"https://stu-yue.github.io/categories/tidbits/"},{"name":"networks","slug":"networks","permalink":"https://stu-yue.github.io/categories/networks/"},{"name":"python","slug":"languages/python","permalink":"https://stu-yue.github.io/categories/languages/python/"}],"tags":[{"name":"math","slug":"math","permalink":"https://stu-yue.github.io/tags/math/"},{"name":"ml","slug":"ml","permalink":"https://stu-yue.github.io/tags/ml/"},{"name":"instruction set arch","slug":"instruction-set-arch","permalink":"https://stu-yue.github.io/tags/instruction-set-arch/"},{"name":"docker","slug":"docker","permalink":"https://stu-yue.github.io/tags/docker/"},{"name":"linux","slug":"linux","permalink":"https://stu-yue.github.io/tags/linux/"},{"name":"shell","slug":"shell","permalink":"https://stu-yue.github.io/tags/shell/"},{"name":"networks","slug":"networks","permalink":"https://stu-yue.github.io/tags/networks/"},{"name":"statistic","slug":"statistic","permalink":"https://stu-yue.github.io/tags/statistic/"},{"name":"algebra","slug":"algebra","permalink":"https://stu-yue.github.io/tags/algebra/"},{"name":"python","slug":"python","permalink":"https://stu-yue.github.io/tags/python/"},{"name":"optimization","slug":"optimization","permalink":"https://stu-yue.github.io/tags/optimization/"},{"name":"interview","slug":"interview","permalink":"https://stu-yue.github.io/tags/interview/"},{"name":"points","slug":"points","permalink":"https://stu-yue.github.io/tags/points/"},{"name":"memory","slug":"memory","permalink":"https://stu-yue.github.io/tags/memory/"},{"name":"firmware","slug":"firmware","permalink":"https://stu-yue.github.io/tags/firmware/"},{"name":"driver","slug":"driver","permalink":"https://stu-yue.github.io/tags/driver/"}]}