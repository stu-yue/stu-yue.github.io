{"pages":[{"title":"ABOUT","date":"2023-10-04T06:07:04.840Z","path":"about/index.html","text":"👩‍💻: A CODER WITH GREEN HANDS 💡: THINK TWICE, CODE ONCE! 🎯: GOTO NIPS, MSRA AND BE REASSURED 💌: BEST WISHES TO LEE XIAOMAO 12345// life mottoif (sad() == true) &#123; sad().stop(); beAwesome();&#125;"},{"title":"TAGS","date":"2023-07-13T06:48:03.646Z","path":"tags/index.html","text":""},{"title":"CATEGORIES","date":"2023-07-13T06:47:46.496Z","path":"categories/index.html","text":""}],"posts":[{"title":"Memo | Law of Large Number","date":"2023-10-21T05:56:53.000Z","path":"jottings/mathematics/memo_law_of_probability/","text":"1 Law of Large Number1.1 Folk UnderstandingIn simple terms, the large numbers theorem refers to that a random event may or may not occur in a single experiment, but in a large number of repeated experiments, it often shows obvious regularity, that is, the frequency of the random event will converge to a constant value, which is the probability of the event. Another way to express it is that when the sample data is infinite, the sample mean tends to population mean. Because in real life, we can not run an infinite number of experiments, and it is difficult to estimate the parameters of the population. The law of large numbers connects mean values, which belong to mathematical statistics, with expectations, which belong to probability theory. 1.2 Convergence in Probability Weak Law: convergence in probability Strong Law: almost sure convergence (outlier can be negligible in measure) 1.3 Bernoulli’s LawFrom the perspective of defining probability, reveals the relationship between probability and frequency.$$\\lim\\limits_{n\\rightarrow \\infin} P{ |\\frac{f_A}{n} - p| &lt; \\epsilon } &#x3D; 1$$ 1.4 Khinchin’s LawPriori Condition: Independent Identically Distributed, $\\lim\\limits_{n\\rightarrow \\infin} P{ |\\frac{1}{n}\\sum\\limits_{i&#x3D;1}^nX_i - \\frac{1}{n}\\sum\\limits_{i&#x3D;1}^{n}E(X_i)| &lt; \\epsilon } &#x3D; 1$ 1.5 Chebyshev’s LawPriori Condition: Independent Distributed, $\\lim\\limits_{n\\rightarrow \\infin} P{ |\\frac{1}{n}\\sum\\limits_{i&#x3D;1}^nX_i - \\frac{1}{n}\\sum\\limits_{i&#x3D;1}^{n}\\mu_i| &lt; \\epsilon } &#x3D; 1$ 2 Heavy-tailed Distribution Ref: 1, 2 Pareto Distribution:$$P(X&gt;x) &#x3D; \\left{\\begin{align}(\\frac{x_{min}}{x})^\\alpha,&amp;\\quad x\\ge x_{min} \\1, &amp;\\quad x&lt;x_{min}\\end{align}\\right.$$ $$f(x) &#x3D; \\left{\\begin{align}\\frac{1}{x^\\alpha}\\cdot\\frac{\\alpha x_{min}^\\alpha}{x},&amp;\\quad x\\ge x_{min} \\0, &amp;\\quad x&lt;x_{min}\\end{align}\\right.$$ Pareto Principle: states that for many outcomes, roughly 80% of consequences com from 20% of causes (the “vital few”). Other names —— 80&#x2F;20 rule, the law of the vital few (states the imbalance phenomenon); Mathematically, the 80&#x2F;20 rule is roughly described by a power law distribution (also known as a Pareto distribution) for a particular set of parameters. Zipf Distribution:$$f(x) &#x3D; \\frac{1}{x^\\alpha\\sum_{i&#x3D;1}^{n}(1&#x2F;i)^\\alpha}, \\ x &#x3D; 1,2,\\cdots,n$$Zipf’s law states that the value of the nth entry is inversely proportional to n, when a list of measured values is sorted in decreasing order. Zeta Distribution: when $n\\rightarrow \\infty$, $\\text{Zipf}(\\alpha, n)\\rightarrow\\text{Zeta}(\\alpha)$ ; Zeta is regraded as a type of pareto distribution in the discrete distribution.$$f(x) &#x3D; \\frac{1}{x^\\alpha\\sum_{i&#x3D;1}^{\\infty}(1&#x2F;i)^\\alpha}, \\ x &#x3D; 1,2,\\cdots,n,\\text{and}\\ \\alpha &gt; 1$$","tags":[{"name":"math","slug":"math","permalink":"https://stu-yue.github.io/tags/math/"}],"categories":[{"name":"mathematics","slug":"mathematics","permalink":"https://stu-yue.github.io/categories/mathematics/"}]},{"title":"Memo | CTC Introduction","date":"2023-10-17T07:56:53.000Z","path":"jottings/statistics_ml/memo_ctc/","text":"1 Problem DescriptionIn seq2seq structure, given input sequence $X&#x3D;[x_1,\\cdots,x_T]$ with corresponding label $Y &#x3D; [y_1,\\cdots,y_N]$, such as speech recognition. Our job is to find a map, and this algorithm for classifying time series data is called Temporal Classification. Compared with traditional classification, temporal classification has the following difficulties: The lengths of $X$ and $Y$ are variable. The lengths of $X$ and $Y$ are not equal. For an end-to-end model, we don’t want manual design the alignment between $X$ and $Y$. The CTC provides the solution, that for a given input sequence $X$, CTC gives the output distribution of all possible $Y$. Based on this distribution, we can output the most likely outcome or give the probability of a certain output. Loss Function: Given the input sequence $X$, we want to maximize the posterior probability $P(Y|X)$ of $Y$, and $P(Y|X)$ should be derivable so that we can perform the gradient-descent algorithm; Test: Given a trained model and input sequence $X$, we want to output $Y$ with the highest probability:$$Y^* &#x3D; \\arg\\max_YP(Y|X)$$Of course, when testing, we want Y to be searched as soon as possible (greedy, beam, prefix-beam, LM). CTC Procedure REF: https://zhuanlan.zhihu.com/p/42719047 CTC Traits Conditional independence: A very unreasonable assumption of the CTC is its assumption that each time slice is independent of each other, which is a very bad assumption. In OCR or speech recognition, there is some semantic information between each time slice, so the effect should be improved if the language model can be added to the CTC. Monotonic alignment: Another constraint of CTC is the monotonic alignment between input $X$ and output $Y$, which holds true in OCR and speech recognition. However, in some scenarios, such as machine translation, this constraint is not valid. Many-to-one mapping: Another constraint of CTC is that the length of the input sequence $X$ is greater than the length of the label data $Y$, but for scenarios where the length of $Y$ is greater than the length of $X$, CTC is invalid.","tags":[{"name":"ml","slug":"ml","permalink":"https://stu-yue.github.io/tags/ml/"}],"categories":[{"name":"statistics_ml","slug":"statistics-ml","permalink":"https://stu-yue.github.io/categories/statistics-ml/"}]},{"title":"Memo | LM and Word Representation","date":"2023-10-17T07:56:53.000Z","path":"jottings/statistics_ml/memo_lm_and_word_vector/","text":"Language Model 语言模型是衡量一句话出现在自然语言中的概率的模型； 数学形式上，给定一句话 $s &#x3D; { w_1,\\cdots,w_n }$，它对应的概率为：$$\\begin{align*}P(s) &amp;&#x3D; P(w_1,\\cdots,w_n) \\&amp;&#x3D; P(w_1)\\times P(w_2|w_1) \\times \\cdots \\times P(w_n|w_1,\\cdots,w_{n-1})\\&amp;&#x3D; \\prod\\limits_{i&#x3D;1}^{n}P(w_i|w_1,\\cdots,w_{i-1})\\end{align*}$$ 语言模型的核心在于根据前文预测下一个词出现的概率； $P(w_i|w_1,\\cdots,w_{i-1}),\\ w_i \\in V,\\ V&#x3D;{ w_1,\\cdots,w_{|V|} }$ 马尔可夫假设 (Markov Assumption)：当前词出现的概率只和它前面的k个词相关；$$\\begin{align*}P(w_i | w_1,\\cdots,w_{i-1}) &amp;&#x3D; P(w_i | w_{i-k},\\cdots,w_{i-1}) \\&amp; &#x3D; P(w_i) \\qquad\\qquad\\qquad \\rightarrow\\quad\\text{k&#x3D;0, Unigram Model} \\&amp; &#x3D; P(w_i | w_{i-1}) \\qquad\\qquad \\rightarrow\\quad\\text{k&#x3D;1, Bigram Model} \\&amp; &#x3D; P(w_i | w_{i-2}, w_{i-1})\\qquad\\rightarrow\\quad\\text{k&#x3D;2, Trigram Model}\\\\end{align*}$$ 用频率估计概率（大数定理——伯努利）$$\\begin{align*}P(w_i | w_1,\\cdots,w_{i-1}) &amp;&#x3D; P(w_i | w_{i-k},\\cdots,w_{i-1}) \\&amp; &#x3D; \\frac{P(w_{i-k},\\cdots,w_{i-1},w_i)}{P(w_{i-k},\\cdots,w_{i-1})} \\&amp; \\approx \\frac{\\text{count}(w_{i-k},\\cdots,w_{i-1},w_i)}{\\text{count}(w_{i-k},\\cdots,w_{i-1})} \\\\end{align*}$$ Zipf Law, also known as the rank-size rule or Zipf distribution, is an empirical observation about the frequency distribution of words or other items in a given corpus of natural language. It states that the frequency of any word is inversely proportional to its rank in the frequency table. 隐藏信息，排位靠后的词的频率非常低，甚至未出现在语料中； 数据稀疏，对于未出现在语料中的词或n-gram，无法估计其概率； 平滑技术 （拉普拉斯平滑、古德-图灵平滑、插值平滑、Katz平滑） $$\\begin{align*}P(w_i | w_1,\\cdots,w_{i-1}) &amp;&#x3D; P(w_i | w_{i-k},\\cdots,w_{i-1}) \\&amp; &#x3D; \\frac{P(w_{i-k},\\cdots,w_{i-1},w_i)}{P(w_{i-k},\\cdots,w_{i-1})} \\&amp; \\approx \\frac{\\text{count}(w_{i-k},\\cdots,w_{i-1},w_i)+1}{\\text{count}(w_{i-k},\\cdots,w_{i-1})+|V|} \\\\end{align*}$$ 回退策略$$\\begin{align*}P(w_i | w_1,\\cdots,w_{i-1}) &amp;&#x3D; P(w_i | w_{i-k},\\cdots,w_{i-1}) \\&amp; &#x3D; \\frac{P(w_{i-k},\\cdots,w_{i-1},w_i)}{P(w_{i-k},\\cdots,w_{i-1})} \\&amp; \\approx \\frac{\\text{count}(w_{i-k},\\cdots,w_{i-1},w_i)}{\\text{count}(w_{i-k},\\cdots,w_{i-1})}\\qquad \\rightarrow\\quad\\text{students opened their} \\&amp; \\approx \\frac{\\text{count}(w_{i-k+j},\\cdots,w_{i-1},w_i)}{\\text{count}(w_{i-k+j},\\cdots,w_{i-1})}\\quad \\rightarrow\\quad\\text{opened their} \\\\end{align*}$$ 参数规模问题：随着k的增大，参数数目呈指数增长，无法存储； k&#x3D;1，参数量&#x3D;$|V|^2$；k&#x3D;2，参数量&#x3D;$|V|^3$；k&#x3D;n-1，参数量&#x3D;$|V|^n$； 困惑度（Perplexity） 用来衡量一个概率分布或概率模型预测样本的好坏程度； 可以用来比较两个概率模型，低困惑度的概率模型能更好地预测样本；$$\\text{Perplexity}(s) &#x3D; 2^{H(s)} &#x3D; \\sqrt[n]{1&#x2F;P(w_1,\\cdots,w_n)}$$ Word Representation 词库： WordNet：一个包含同义词（ synonym ）和上位词（ hypernyms ）的知识库； 词库的问题： 缺少差异性 (proficient也被视为good的同义词)，不够精确； 缺少新词，无法及时更新 主观性，人工标注； 离散词表示： One-hot表示： 123456单词表示 motel = [0 0 0 0 0 1 0] hotel = [0 0 0 1 0 0 0]文本表示 The students opened their books [0 0 1 1 0 0 1 0] ⬆ 次数、频率、逆文档频率、TF-IDF、... 词袋模型（Bag of Word）：词袋模型用于文本表示，如果每个词为One-hot表示，那么把每个词的One-hot向量相加，得到的向量就是该文本基于BOW得到的表示； 词频（Term Frequency，TF）：在文档中出现频率越高的词对当前文档可能越重要；$$f_{ij} &#x3D; \\frac{\\text{count}(\\text{term}\\ i)\\text{in doc} \\ j}{\\text{count}(\\text{all term})\\text{in doc} \\ j}, \\tf_{ij} &#x3D; \\frac{f_{ij}}{\\max_k(f_{kj})}$$ 逆文档频率（Inverse Document Frequency，IDF）：在很多文档中都出现的词可能不重要（如虚词）；$$df_i &#x3D; \\text{doc frequency of term}\\ i &#x3D; \\text{numbers of doc containing term} \\ i, \\idf_i &#x3D; \\log_2\\frac{N}{df_i} \\ \\text{（N为文档总数）}$$ TF-IDF：综合一个词在当前文档中的频率和所有文档中出现的次数来度量这个词对当前文档重要性；$$tf_{ij}-idf_i &#x3D; tf_{ij}*idf_i &#x3D; tf_{ij}*\\log_2\\frac{N}{df_i}$$ N-gram：N元组提取局部的上下文你信息； 离散词表示问题：语义鸿沟、维度爆炸； 分布式词表示（词嵌入） 用一个低维稠密的向量表示单词的整体含义； 核心思想：一个词的含义能被该词所在的上下文反映； Co-occurrence： 基于窗口的共现矩阵： 统计窗口内单词之间的共现信息； 类似于word2vec； 能够捕获一些句法和语义信息（局部信息）； 基于文档的共现矩阵： 统计文档和单词之间的共现信息； Latent Semantics Analysis (LSA)； 能够捕获话题信息（全局信息）； Word2vec[Mikolov et al. 2013] 是一套学习词向量的算法框架 算法思想：大量的自然语言文本（训练语料） 为词表中的每个词随机初始化一个向量表示 遍历文本中的每个单词 $c$，其上下文单词为 $o$ 使用单词 $c$ 的上下文 $o$ 预测单词 $c$ 的概率分布（核心思想） 更新词向量的表示使得单词 $c$ 的预测概率最大化 连续词袋模型（CBOW，Continuous Bag of Words） 目标：通过局部语言模型的优化，获得词向量 优化目标：固定上下文词向量表示，计算中心词的似然函数，最大化其似然（负对数） 计算优化：负采样（Negative Sampling） 为避免softmax计算整个庞大的词表，通常采用负采样的方法，将多分类问题转换为二分类问题；大大减少了训练时间和计算成本； Skip-gram：中心词 $c$ 预测上下文 $o$； 优点 缺点 代表方法 共现矩阵法 速度快，有效利用统计数据 过分依赖单词共现性和数据量 LSA, HAL 直接学习法 能够捕获语法和语义信息 速度和数据规模相关，未有效利用统计数据 Skip-gram, CBOW 基于计数的和基于预测的都探究了语料库的潜在共现统计 GloVe：集两家之长 共现概率矩阵$X_{ij}$； 单词 $w_i$，$w_j$ 的词向量 $v_i$，$v_j$； 以学习的方式，用词向量之间的语义关系来拟合共现概率矩阵；$$J &#x3D; \\sum\\limits_{i,j&#x3D;1}^{|V|}f(X_{ij})(v_i^Tv_j+b_i+b_j-\\log X_{ij})^2 \\v_i^Tv_j \\quad\\rightarrow\\quad\\text{局部信息} \\\\log X_{ij}\\quad\\rightarrow\\quad\\text{全局统计信息} \\$$ 训练快，适应于大规模数据，在小规模数据上性能优秀；","tags":[{"name":"ml","slug":"ml","permalink":"https://stu-yue.github.io/tags/ml/"}],"categories":[{"name":"statistics_ml","slug":"statistics-ml","permalink":"https://stu-yue.github.io/categories/statistics-ml/"}]},{"title":"Memo | ISA and Micro-architecture","date":"2023-10-14T10:21:53.000Z","path":"jottings/architecture/memo_isa_and_micro_architecture/","text":"What is CPU? CPUs are a general purpose, flexible architecture that take in a stream of instructions from all types of workloads, and compute or process information based on those instructions. Simply put, CPUs do what we tell them or program them to do. This ability to continue shrinking transistors is based on a famous law&#x2F;observation that we in the industry refer to as Moore’s Law, that is, we can double the number of transistors per unit area about every two year. Bug Aside: Operators traced an error on the computers to a moth trapped in a relay, recoining the term “bug”. CPU Architecture: **ENIAC: ** In early period, computer programs are hardware-based. Computers with data in memory and programs embedded in the hardware are computationally inefficient and less flexible. Von Neumann Machine: Programs are encoded as data and stored in memory (Principle of Stored Program Control). Harvard Machine: A memory structure that separates program instruction storage from data storage. CPU can access instructions and read&#x2F;write data at the same time. Use two independent memory modules to store instructions and data respectively, and each storage module does not allow instructions and data to coexist; Use two independent buses as a dedicated communication path between the CPU and each memory, and these two buses are unrelated. In fact, the vast majority of modern computers use “Modified Harvard Architecture,” where instructions and data share the same address space, but the cache is separate. As it stands, von Neumann for large-scale processing, and Harvard for small-scale processing. CPU workflow architecture: Instruction Set ArchitectureThe ISA is the dictionary of instructions, data types, and the formats that the CPU adhering to that ISA must execute. The ISA is used as a design spec (specification) that tells the engineer what operations it needs to execute. Because of this layer of abstraction, the instructions in the ISA are implementation independent. Micro-architecture is the concrete implementation of ISA in the hardware. CISC (Complex Instruction Set Computers): Early CPUs all used CISC, which was designed to perform the required computational tasks with minimal machine language instructions. In order to achieve complex operations, microprocessors provide programmers with functions similar to various registers and machine instructions, but also through microprograms stored in read-only memory (ROM) to achieve its extremely powerful functions. RISC (Reduced Instruction Set Computers): In CISC, many complex instructions require extremely complex operations, and most of these instructions are direct copies of some high-level language, so the universality is poor. Because of the secondary microcode execution, it also slows down the operation of simple instruction systems that are frequently invoked. Summary: The complex instructions are converted into a microprogram, which is stored in the microservice memory when the CPU is manufactured. A microprogram contains several microinstructions (also known as microcode), and when executing complex instructions, it is actually executing a microprogram. This also brings a difference between the two instruction sets, the execution of microprograms cannot be interrupted, while RISC instructions can be interrupted between each other, so in theory RISC can respond faster to interrupts. Command Capability: The instruction capability of CISC is strong, but the usage rate of most instructions is low, which increases the complexity of CPU. Instructions are variable length format, which must be divided into different length instructions, so more processing work is needed when executing a single instruction. Most RISC instructions are single-cycle instructions, the length of instructions is fixed, and the CPU is fast and stable when executing instructions. Addressing Mode: CISC supports a variety of addressing methods. RISC supports few addressing methods. Implementation Mode: CISC is implemented through microprogrammed control technology (microcode). RISC adds a general register, hard-wired logic control is the main, suitable for pipelined execution. RISC can optimize compilation and effectively support high-level languages. R&amp;D Cycle: CISC has a long development cycle. RISC hardware is simple, so its manufacturing process is simple and low cost.","tags":[{"name":"instruction set arch","slug":"instruction-set-arch","permalink":"https://stu-yue.github.io/tags/instruction-set-arch/"}],"categories":[{"name":"architecture","slug":"architecture","permalink":"https://stu-yue.github.io/categories/architecture/"}]},{"title":"Memo | Tmux","date":"2023-10-04T07:56:53.000Z","path":"jottings/languages/shell/memo_tmux/","text":"What is tmux? A typical use of the command line is to open a terminal window (session), whose important feature is that window is connected to the process started in it (window closed, session ends, vice versa). Tmux, Terminal multiplexer, is the session and window “unbind” tool. It allows: simultaneous access to multiple sessions in a single window. (useful for running multiple terminal simultaneously) a new window to access an existing session; each session having multiple connection window (multiple people sharing sessions in real time) arbitrary vertical and horizontal splitting of windows; Basic conception: session: Basic Usage Start —tmux, Quit—exit/Ctrl-d, Prefix Key—Ctrl+b; A status bar is located at the bottom: 12[name/id] [list of ][0] 0:bash 1:test3* 2:test4- &quot;VM-16-17-ubuntu&quot; 15:28 04-Oct-23 Session Management New a session: tmux new -s &lt;session-name&gt;; Split sessions: tmux detach/Ctrl+b d, after the command is executed, the current Tmux window exits, but the session and the processes inside it still run in the background; View all current tmux sessions: tmux ls 1230: 1 windows (created Tue Sep 19 20:42:24 2023)1: 1 windows (created Tue Oct 3 19:57:48 2023)test2: 1 windows (created Wed Oct 4 14:49:04 2023) (attached) Attach a session: tmux attach -t id/&lt;session-name&gt;; Kill a session: tmux kill-session -t id/&lt;session-name&gt;; Switch a session: tmux switch -t id/&lt;session-name&gt;; Rename a session: tmux rename-session -t id/&lt;session-name&gt; &lt;new-name&gt;; Shortcuts: Ctrl+bd: Split current session; Ctrl+b s list all session; Ctrl+b $: rename current session; Simple workflow of tmux: new a session: tmux new -s my_session; run program in tmux window; Ctrl+b d splits the session; Attach the last session tmux attach-session -t my_session; Pane OperationTmux can split the window into panes, which can execute different commands. tmux splilt-window splits into vertical layout; tmux split-window -h splits into horizontal layout; tmux select-pane moves the cursor in different panes: 1234567891011# 光标切换到上方窗格$ tmux select-pane -U# 光标切换到下方窗格$ tmux select-pane -D# 光标切换到左边窗格$ tmux select-pane -L# 光标切换到右边窗格$ tmux select-pane -R tmux swap-pane exchanges the positions of panes: 12345# 当前窗格上移$ tmux swap-pane -U# 当前窗格下移$ tmux swap-pane -D Shortcuts in pane operations: 1234567891011121314Ctrl+b % ：划分左右两个窗格。Ctrl+b &quot; ：划分上下两个窗格。Ctrl+b &lt;arrow key&gt; ：光标切换到其他窗格。&lt;arrow key&gt;是指向要切换到的窗格的方向键，比如切换到下方窗格，就按方向键↓。Ctrl+b ; ：光标切换到上一个窗格。Ctrl+b o ：光标切换到下一个窗格。Ctrl+b &#123; ：当前窗格与上一个窗格交换位置。Ctrl+b &#125; ：当前窗格与下一个窗格交换位置。Ctrl+b Ctrl+o ：所有窗格向前移动一个位置，第一个窗格变成最后一个窗格。Ctrl+b Alt+o ：所有窗格向后移动一个位置，最后一个窗格变成第一个窗格。Ctrl+b x ：关闭当前窗格。Ctrl+b ! ：将当前窗格拆分为一个独立窗口。Ctrl+b z ：当前窗格全屏显示，再使用一次会变回原来大小。Ctrl+b Ctrl+&lt;arrow key&gt; ：按箭头方向调整窗格大小。Ctrl+b q ：显示窗格编号。 Window Operation tmux new-window -n &lt;window-name&gt;: new a window; tmux select-window -t &lt;window-number/name&gt;: switch window tmux rename-window Shortcuts in window operation: 123456Ctrl+b c：创建一个新窗口，状态栏会显示多个窗口的信息。Ctrl+b p：切换到上一个窗口（按照状态栏上的顺序）。Ctrl+b n：切换到下一个窗口。Ctrl+b &lt;number&gt;：切换到指定编号的窗口，其中的&lt;number&gt;是状态栏上的窗口编号。Ctrl+b w：从列表中选择窗口。Ctrl+b ,：窗口重命名。 Other Commands123456789101112131415# 列出所有快捷键，及其对应的 Tmux 命令$ tmux list-keys# 列出所有 Tmux 命令及其参数$ tmux list-commands# 列出当前所有 Tmux 会话的信息$ tmux info# 重新加载当前的 Tmux 配置$ tmux source-file ~/.tmux.confctrl+b, : //按完前缀ctrl+B后，再按分号：进入命令行模式set -g mouse on //命令行中输入这句命令，回车就行了","tags":[{"name":"shell","slug":"shell","permalink":"https://stu-yue.github.io/tags/shell/"},{"name":"linux","slug":"linux","permalink":"https://stu-yue.github.io/tags/linux/"}],"categories":[{"name":"languages","slug":"languages","permalink":"https://stu-yue.github.io/categories/languages/"},{"name":"shell","slug":"languages/shell","permalink":"https://stu-yue.github.io/categories/languages/shell/"}]},{"title":"","date":"2023-09-26T06:40:27.727Z","path":"jottings/tidbits/quantization/","text":"在线量化：指量化感知训练(Quantization-Aware Training)，在网络模型训练阶段采用量化方案进行量化； 量化感知训练本质上是一种伪量化的过程，它是在可识别的某些操作内嵌入伪量化节点（fake quantization op），并参与模型训练的前向推理过程模拟引入，但模型的反向传播过程依旧使用全精度浮点数进行； 伪量化节点，是指量化感知训练中插入的节点，用以寻找网络数据分布，并反馈损失精度： 找到输入、权重等待量化数据的分布，找到待量化数据的最大和最小值； 模拟低比特量化带来的精度损失，把该损失作用到网络模型中，传递给损失函数，让优化器在训练过程中对该损失值进行优化， 尽可能减少由于伪量化操作而引起的精度下降； 先饱和截断处理：$clamp(x,x_{min}, x_{max}) &#x3D; \\min(\\max(x,x_{min}), x_{max})$； 再Float-&gt;Int-&gt;Float： 离线量化：指训练后量化(Post-Training Quantization)： 动态离线量化(PTQ, Dynamic)： 动态离线量化仅将模型中特定算子的权重从FP32类型映射成 INT8&#x2F;16 类型，bias和激活函数 在推理过程中动态量化。但是对于不同的输入值来说，其缩放因子是动态计算的（“动态”的由来）。动态量化是几种量化方法中性能最差的。动态量化常用于非常大的模型。 静态离线量化(PTQ, Static)： 静态离线量化使用少量无标签校准数据，采用 KL 散度等方法计算量化比例因子。静态量化（Static quantization）与动态量化的区别在于其输入的缩放因子计算方法不同，静态量化的模型在使用前有“calibrate”的过程（校准缩放因子）：准备部分输入（对于图像分类模型就是准备一些图片，其他任务类似），使用静态量化后的模型进行预测，在此过程中量化模型的缩放因子会根据输入数据的分布进行调整。一旦校准完成后，权重和输入的缩放因子都固定（“静态”的由来）。静态量化的性能一般比动态量化好，常用于中等模型和大模型。因此实际中基本都是在用静态量化。 静态离线量化的目标是求取量化比例因子，主要通过对称量化、非对称量化方式来求，而找最大值或者阈值的方法又有MinMax、KLD、ADMM、EQ等方法。 对称量化与非对称量化：对于weight权重的量化使用对称量化[-INT_MAX, INT_MAX]，对于activate激活的量化使用非对称量化[0, INT_MAX]；","tags":[],"categories":[{"name":"tidbits","slug":"tidbits","permalink":"https://stu-yue.github.io/categories/tidbits/"}]},{"title":"Memo | Package and Device","date":"2023-09-19T14:56:53.000Z","path":"jottings/languages/shell/memo_package_device/","text":"Package Package System Package Management System Linux Release Version Debian Style (.deb) Debian, Ubuntu Red Hat Style (.rpm) Fedora, CentOS A package file is a compressed collection of files that make up a software package and may contain a large number of programs and the data files that support those programs. Metadata for the packet is included, such as a text description of the package and its contents. Other included are pre-installation and post-installation scripts (which perform configuration tasks before and after installation) Upper Tools and Underlying Tools: Linux Release Version Underlying Tool (install and delete packages) Upper Tool (search for metadata and resolve dependencies) Debian-Style dpkg apt, aptitude Fedora, CentOS rpm yum Search for metadata in the resource repository Style Command Debian apt update; apt search search_string Red Hat yum search search_string (e.g. yum search emacs) Install a package via resource repository Style Command Debian apt update; apt install package_name Red Hat yum install package_name Install a package via raw package files Style Command Debian dpkg –install &#x2F; -i package_file Red Hat rpm -i package_file (rpm -i emacs-22.1-7.fc7-i386.rpm) NOTE: Due to this command is executed by rpm, not analyzing dependencies for package_file, so if a dependency is missing, rpm will report error and exit. Uninstall a package Style Command Debian apt remove package_name &#x2F; dpkg -r package_name Red Hat yum erase package_name Upgrade package via resource repository Style Command Debian apt update; apt upgrade Red Hat yum update Upgrade package via package_file Style Command Debian dpkg –install package_file Red Hat rpm -U package_file List all package installed Style Command Debian dpkg –list &#x2F; -l Red Hat rpm -qa Determine whether a package is installed Style Command Debian dpkg –status package_name Red Hat rpm -q package_name Show the info for the installed package Style Command Debian apt show package_name Red Hat yum info package_name apt useful arguments: -y : default set yes in interactive shell; -f: solve the package dependencies; Device","tags":[{"name":"shell","slug":"shell","permalink":"https://stu-yue.github.io/tags/shell/"},{"name":"linux","slug":"linux","permalink":"https://stu-yue.github.io/tags/linux/"}],"categories":[{"name":"languages","slug":"languages","permalink":"https://stu-yue.github.io/categories/languages/"},{"name":"shell","slug":"languages/shell","permalink":"https://stu-yue.github.io/categories/languages/shell/"}]},{"title":"Summa |Networks Tidbits","date":"2023-09-07T10:35:53.000Z","path":"jottings/networks/memo_network_tidbits/","text":"Tunneling: The basic principle is to create a virtual channel between the source and the target, through which the original packets is encapsulated in the packet of another protocol, and then transmitted between the source and the target. At the target end, the receiver unwarps the encapsulated packet, reverts it to the original packet, and gives it to the target application for processing. For example, VPN (Virtual Private Network), SSH Tunneling, GRE (Generic Routing Encapsulation, like IPv6 over IPv4).","tags":[{"name":"networks","slug":"networks","permalink":"https://stu-yue.github.io/tags/networks/"}],"categories":[{"name":"networks","slug":"networks","permalink":"https://stu-yue.github.io/categories/networks/"}]},{"title":"Memo | Olds and Ends","date":"2023-09-06T07:56:53.000Z","path":"jottings/languages/shell/memo_others/","text":"Difference between sh and bash: sh is usually symbolic link for dash; dash is a more lightweight shell, POSIX, designed to replace sh and provide faster startup and executions speeds; bash is an extended version of sh, and most scripts that conform to sh syntax should work fine in bash; In a nutshell, sh is bash, which enables the POSIX standard. POSIX, Portable Operating System Interface of UNIX In accordance with the POSIX specification, “When a line of code encounters an error, it does not continue to interpret subsequent lines.” However, in bash, even if an error occurs, it will continue to execute subsequent lines. To view cpu information: lscpu, or cat /proc/cpuinfo;","tags":[{"name":"shell","slug":"shell","permalink":"https://stu-yue.github.io/tags/shell/"},{"name":"linux","slug":"linux","permalink":"https://stu-yue.github.io/tags/linux/"}],"categories":[{"name":"languages","slug":"languages","permalink":"https://stu-yue.github.io/categories/languages/"},{"name":"shell","slug":"languages/shell","permalink":"https://stu-yue.github.io/categories/languages/shell/"}]},{"title":"Memo | Permission, Progress and Shell Environment","date":"2023-08-31T14:56:53.000Z","path":"jottings/languages/shell/memo_permission_progress_env/","text":"PermissionFirst of all, let’s take a quick look at the permissions-related commands. id: To show the id number of the user. id username: chmod: To change the mode of files. symbolic examples: 123u (user), g (group), o (other), a (all)+, -, =u+x u-x +x[=a+x] o-rw, go=rw u+x,go=rw umask: To set default file permissions (before creating). an example: 1234# umask is 0002Original file mode | --- rw- rw- rw-Mask | 000 000 000 010Result | --- rw- rw- r-- su: To run the shell as another user. su - username, - can enter the home directory. sudo: To execute the command as another user. chown: To change the file owner. chown [owner[:group]] file..., here are some examples: Example Explanation bob change the file owner to bob bob:users change the file owner to bob, group to users :admins change the group to admins, file owner remains the same bob: change the file owner to bob, group to bob’s login group chgrp: To change the group ownership of files. chgrp groupname file... passwd: To change the user’s password. passwd username: set user’s password. User information is stored in /etc/passwd, group information is stored in /etc/group; use the command cat /etc/passwd to have a quick look. 123456789101112131415161718192021cat /etc/passwd | grep ubuntu # username:passwd:uid:gid:comment:home_dir:shell# passwd (x) denotes that password is protected in /etc/passwd# comment store some useful comments (like username)ubuntu:x:1000:1000:ubuntu:/home/ubuntu:/bin/bash sudo cat /etc/shadow | grep ubuntu #username:passwd(encrypted):last_modify_time:min_interval:max_intervalubuntu:$1$oQIzlBrL$MErhwMGkTzqaeWkJNzpUh1:19132:0:99999:7::: cat /etc/group | grep cdrom # groupname:passwd:gid:group_membercdrom:x:24:ubuntu,yue sudo cat /etc/gshadow | grep test # groupname:passwd:group_manager:group_membertest:*:yue:ubuntu Permission Overview: r w x File readable writable executable Directory readable writable to files in the directory enterable to the directory Home directory default permission: user &#x3D;rwx, go&#x3D;r-x ; setuid (s/S &lt;-&gt; x/-, 4000/u+s) : It set valid user ID from the running user’s ID to the file owner’s ID; setgid (g/G &lt;-&gt; x/-, 2000/g+s): Run not with the permissions of the group to which the user who started it belongs, but with the group that owns the file. In other words, the process gid is the same as the file gid. sticky (t/T &lt;-&gt; x/-, 1000/o+t): It has no effect on files, but when it is used on a directory, all files in the directory can only be deleted or moved by their owner. File Type: Tag Type - a normal file d a directory l a symbolic link (real file property is the file property that the symbolic link points to) c a character device file (process bytes stream, like terminal) b a block device file (process blocks, like hard-disk or CD-ROM) User and GroupBecause the permission is too large, you can even delete system files and crash the system. Therefore, you are not recommended to directly use root account. On Linux systems, sudo is used as the default root identity for standard users. Next, we have some commands for managing users and groups. groupadd: create a new work group, whose info is added to /etc/group, /etc/gshadow and so on. [-g gid] (specify the id of new group), -r (create system working groups) groupdel: delete a group gpasswd [options] groupname: management tool the /etc/group and /etc/gshadow -a/d username (add&#x2F;delete user to group) -A (specify the manager) -r/R (cancel the password for the group, then only group member can newgrp to access the group) -M user1,user2... add users to group groupmod: change the group information -g gid (change group id) -n new_name old_name (change group name) newgrp groupname: It’s using the same account another group name, to log into the system again. useradd: create a new user. -m/M (automatically &#x2F; not create a user home directory), -g (specify the login group), -G grp1,grp2... (specify the supplementary groups) -d (specify the starting directory for the user to log in to), -r (create a system account) -s (specify the login shell) -n (cancel creating a group with the user name) -p (specify the password, or later run the command passwd to set) useradd -m -g root username, useradd -d /home/test username userdel: -r (recursively delete) Initial Login Group, is a group that a user owns immediately upon login. It’s usually specified with -g when creating a user. The GID in the user info (/etc/passwd) is login group. A user’s additional group is to assign additional permission to the specified user. (There can be only one login group and multiple supplementary group) usermod: modify the settings of the user account. -d: set login directory of the user account -e: set validity period … -g: set the login group … -G: set the supplementary group … -s: set the shell used after login … -l new_name old_name: set the new username … -L/U: lock&#x2F;unlock the account … -p: set the new password … ProgressWhen the system starts, the kernel initializes some of its own activities as Init Process (PID 1). In turn, a series of shell scripts called Init Scripts (located in /etc) are run, which can start all system services. Many of these system services are implemented in the form of daemons, which run only in the background without any user interface (inaccessible). Here are some of the command-line tools available: ps: To view the snapshot of process status; common parameter aux (show all processes) 1234567ubuntu@VM-16-17-ubuntu:/etc$ ps auxUSER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMAND# TTY: ? denotes terminal running locally, Pts/n denotes terminal running remotely# VSZ: virtual memory size# RSS: physical memory size occupied by process# STAT: R(running), S(sleeping), D(uninterruptible sleeping), T(terminal), Z(zombie) &lt;(high priority) N(low priority) Ctrl-C: send a SIGIN Ctrl-D: send a EOF Ctrl-Z: send a SIGSTP, pause ongoing process on the terminal so as to be resumed when necessary. top: To displays a continuously updated list of system process in order of process activity (default, refresh per 3s); jobs: bg: fg: kill: To send signals to programs (kill [-signal] PID...); NO. NAME EXPLANATION 1 HUP Hang up, this signal is used to tell the program that the control terminal has “hung up.” You can show what this signal does by closing a terminal session. Foreground programs running on the current terminal will receive this signal and terminate. Many daemons also use this signal to re-initialize. 2 INT like Ctrl-c 9 KILL The KILL signal is never sent to the target program. Instead, the kernel immediately terminates the process. When a process is terminated in this way, it has no opportunity to do any “cleaning” or saving work. 15 TERM Terminal, this is default signal sent by KILL 18 CONT Continue, after getting a stop signal, program will be resumed by CONT 19 STOP like KILL, STOP is not sent to the target process, so it cannot be ignored 其他常用信号： NO. NAME EXPLANATION 3 QUIT 11 SEGV If a program uses memory illegally, this signal will be sent 20 TSTP Ctrl-z will trigger this signal to cause terminal stop, unlike STOP, it can be ignored killall: To send signals to multiple processes that match a particular program or username (killall [-u user] [-signal] name...); shutdown: To shutdown the machine or reboot; Shell Environment printenv: set: To display existing shell variables in the system and set new variable values for shell variables. When user log in to the system, the bash program starts and reads a series of configuration scripts (startup files that define a default environment for all users), then it reads the startup files in the home directory that define the user’s personal shell environment. The exact startup order depends on the type of shell session you want to run. There are two types, one is login shell session (need username and password), the other is non-login shell session (start under the GUI). Login shell startup order: File Usage &#x2F;etc&#x2F;profile global conf script applying to all users ~&#x2F;.bash_profile user’s personal startup file, used to extend or override settings in global conf script ~&#x2F;.bash_login if ~&#x2F;.bash_profile is not found, bash will try to read this script Non-login shell startup order: File Usage &#x2F;etc&#x2F;bash.bashrc global conf script applying to all users ~&#x2F;.bashrc user’s personal startup file, used to extend or override settings in global conf script In addition to reading the startup files above, non-login shell also inherit the environment settings of their parent process, usually a login shell. In general users’ points, the file ~&#x2F;.bashrc is probably the most important startup file because it’s almost always read. Non-login shells read it by default, and most startup files for login shells are written in such a way that they can read ~&#x2F;.bashrc . The below is a typical .bash_profile file (From CentOS 4): 12345678# .bash_profile# Get the aliaes and functionsif [ -f ~/.bashrc ]; then. ~/.bashrcfi# User specific environment and startup programsPATH=$PATH:$HOME/binexport PATH export: export environment variables; alias: create alias for command; Refref1","tags":[{"name":"shell","slug":"shell","permalink":"https://stu-yue.github.io/tags/shell/"},{"name":"linux","slug":"linux","permalink":"https://stu-yue.github.io/tags/linux/"}],"categories":[{"name":"languages","slug":"languages","permalink":"https://stu-yue.github.io/categories/languages/"},{"name":"shell","slug":"languages/shell","permalink":"https://stu-yue.github.io/categories/languages/shell/"}]},{"title":"Memo | Parameter Estimation","date":"2023-08-30T05:56:53.000Z","path":"jottings/mathematics/memo_mle_mae_bayes/","text":"Through this note, I hope to deepen my understanding of probability distribution and inference. Preface Probability: to predict results obtained in the next observation when parameters are known; Statistic&#x2F;Likelihood: to estimate parameters about properties when the result of observations are known; Parametric Method: assume that the learned distribution has a specific functional form (like Gaussian distribution or exponential p.d.f), we only estimate the parameters in these functions. Nonparametric Method: use the training samples to estimate the density of any point in the domain. Nonparametric methods also have parameters, we just don’t assume any specific functional form for distribution; Actually, nonparametric methods treat all training samples as parameters; example: kernel density estimation; Under the joint distribution $p_{X,Y}(X,Y)$: When the effect of $Y$ is removed from the joint distribution $p_{X,Y}(X,Y)$, marginal distribution $p_X(X)$ is called marginal likelihood; When $X$ has not yet been observed, marginal distribution $p_Y(Y)$ is called prior distribution; Posterior Distribution: $p(\\theta|\\mathcal{D}) &#x3D; \\frac{p(\\mathcal{D},\\theta)}{p(\\mathcal{D})}$, concentrating Population Info, Sample Info and Prior Info; $p(\\mathcal{D})$ is marginal likelihood; $\\text{Posterior} &#x3D; \\frac{\\text{Likelihood}\\times \\text{Prior}}{\\text{Marginal Likelihood}}$, in terms of $p(Y|X) &#x3D; \\frac{1}{Z}p(X|Y)p(Y)$, $Z&#x3D;p(X)&gt;0$ is a normalized constant such that $p(Y|X)$ is a valid probability distribution. The Views Frequentist: Data are repeatable random sample - there is a frequency; Underlying parameters remain constant during this repeatable process; Parameters are fixed value; statistical inference: Population Info + Sample Info MLE, MAP; Bayesian: Data are observed from the realized sample; Parameters are unknown (random variable) and described probabilistically (prior distribution); statistical inference: Population Info + Sample Info + Prior Info (Main Diff); Bayesian Estimation; Notations Training Data: $\\mathcal{D} &#x3D; { (\\mathbf{x_1}, y_1),\\cdots,(\\mathbf{x}_n, y_n) }$; Model Parameter: $\\theta$; New Data: $x^*$; Maximum Likelihood Estimation Objective is$$\\theta_{MLE}^* &#x3D; \\arg\\max_\\theta p(\\mathcal{D}|\\theta)$$ $p(\\mathcal{D}|\\theta)$ is likelihood, not conditional probability; Usually, we define$$\\mathscr{l}(\\theta) &#x3D; p(\\mathcal{D}|\\theta) \\\\mathscr{ll}(\\theta) &#x3D; \\ln \\mathscr{l}(\\theta)$$So, objective is equivalent to$$\\theta_{MLE}^* &#x3D; \\arg\\max_\\theta \\mathscr{ll}(\\theta)$$That is, we seek those values for the parameters in $\\theta$ which maximize $p(\\mathcal{D}|\\theta)$. The MLE solution is usually obtained by setting$$\\frac{\\partial \\mathscr{ll}(\\theta)}{\\partial\\theta} &#x3D; 0$$ However, the model… Does not incorporate prior belief; Easy to overfit the data; Maximum A Posteri Estimation Objective is$$\\theta^*_{MAP} &#x3D; \\arg\\max_\\theta p(\\theta|\\mathcal{D})$$Since we have Bayes rule:$$p(\\theta | \\mathcal{D}) &#x3D; \\frac{p(\\mathcal{D})p(\\theta)}{p(\\mathcal{D})}$$Our objective is equivalent to$$\\theta^*_{MAP} &#x3D; \\arg\\max_\\theta p(\\mathcal{D}|\\theta)p(\\theta)$$Further, by taking the log$$\\theta^*_{MAP} &#x3D; \\arg\\max_\\theta {\\ln p(\\mathcal{D}|\\theta) + lnp(\\theta) } \\\\theta^*_{MAP} &#x3D; \\arg\\max_\\theta { \\mathscr{ll}(\\theta) + \\ln p(\\theta) }$$Thus, our final goal is to find$$\\theta^*_{MAP} &#x3D; \\arg\\max_\\theta { \\mathscr{ll}(\\theta) + \\ln p(\\theta) }$$The difference between MAP and MLE is the “extra” term - $p(\\theta)$. The term is: our prior (belief) also can be seen as penalty (regularization) - to reduce the overfitting. For $p(\\theta|\\mathcal{D})$, in terms of point estimation for $\\theta$, using the maximum value is called Maximum A Posterior Estimation; using the median value is called Posteriror Median Estimation; using the expectation value is called Posterior Expectation Estimation; Bayesian Estimation $p(\\theta|\\mathcal{D})$ (it’s the result of adjustments to prior $p(\\theta)$ by population and sample); Equal Ignorance: If there’s no information about the prior distribution, assume $\\theta \\sim U(0,1)$ ; Learning: Computing the posterior $p(\\theta|\\mathcal{D})$ ; Prediction: $p(\\hat y | x^*,\\mathcal{D}) &#x3D; \\int_\\theta p(\\hat y | x^*,\\theta)p(\\theta|\\mathcal{D})d\\theta$ ; Both MLE and MAP return only single and specific values for the paramter $\\theta$; Bayesian estimation, by contrast, calculates fully the posterior distribution $p(\\theta|\\mathcal{D})$, and making prediction by considering all possible $\\theta$. Thus, for Bayesian methods: The prediction is optimal Avoid the overfitting Bayesian is powerful, but… We need to compute posterior distribution $p(\\theta|\\mathcal{D})$, and$$p(\\theta|\\mathcal{D}) &#x3D; \\frac{p(\\mathcal{D}|\\theta)p(\\theta)}{\\int p(\\mathcal{D},\\theta)d\\theta}$$In practice, evaluating this posterior is usually intractable due to the complex integrals… ReferenceMainly Ref Ref2 Ref3","tags":[{"name":"math","slug":"math","permalink":"https://stu-yue.github.io/tags/math/"}],"categories":[{"name":"mathematics","slug":"mathematics","permalink":"https://stu-yue.github.io/categories/mathematics/"}]},{"title":"Memo | Algebra Basis","date":"2023-08-27T07:56:53.000Z","path":"jottings/mathematics/memo_algebra_basis/","text":"**Positive Definiteness of the Norm: ** According to the usual definition, a norm should satisfy the property of positive definiteness which means that a norm must have a non-negative value for non-zero vectors and only attain zero when the vector is the zero. **Norm and Distance Evaluation: ** Norm Distance $L_0\\ \\text{norm}$ : $ $L_1\\ \\text{norm}$ : $ $L_2\\ \\text{norm}$ : $ $L_p\\ \\text{norm}$ : $ $\\text{Infinite norm}$ : $ Normed Space: Normed space introduces a norm (or length, modulus) concept on the basis of linear space.","tags":[{"name":"algebra","slug":"algebra","permalink":"https://stu-yue.github.io/tags/algebra/"}],"categories":[{"name":"mathematics","slug":"mathematics","permalink":"https://stu-yue.github.io/categories/mathematics/"}]},{"title":"Summa | Intro of Statistical ML","date":"2023-08-27T07:56:53.000Z","path":"jottings/statistics_ml/summa_intro/","text":"If a system is able to improve its performance by executing a certain process, it is called learning. 1 Basis Hypothesis Space: The set of functions that includes all possible models to be learned. Common Steps: Attain a finite training data set. Determine the set of learning models. Determine the criteria for model selection (learning strategies). Develop algorithms for solving the optimal model (learning algorithm). Select optimal model according to criteria. Use the learned optimal model to predict or analyze new data. Problem Types: Classification&#x2F;Tagging (outputs take a finite number of discrete values), Regression (function fitting, real values) Model Types: Discriminative Model: directly model $p(Y|X)$; Cons: easy to learn, high classification accuracy; Generative Model: model $p(X, Y)$, usually translating to modeling the prior distribution $p(Y)$ and class conditional distribution $p(X|Y)$, due to ($p(X, Y)&#x3D; P(X|Y)p(Y)$); Cons: add the prior distribution [main diff], model the data generation process;","tags":[{"name":"ml","slug":"ml","permalink":"https://stu-yue.github.io/tags/ml/"},{"name":"statistic","slug":"statistic","permalink":"https://stu-yue.github.io/tags/statistic/"}],"categories":[{"name":"statistics_ml","slug":"statistics-ml","permalink":"https://stu-yue.github.io/categories/statistics-ml/"}]},{"title":"Memo | See the World through Shell","date":"2023-08-27T07:56:53.000Z","path":"jottings/languages/shell/memo_see_the_world_through_shell/","text":"In this post, there’re other commands we should be familiar with: echo: To display a line text. -e (explain the escape sequences) clear: history: To display the history list content. list history commands: sequence conduct !! repeat the last command executed !number repeat the number line command in the history !string repeat the command starting with this string !?string repeat the command containing this string Expansions in Shell Mathematical Expression: Format: $((expression)), for example, echo $((2 + 2)); Operators: +, -, *, / (integer division), %, ** (exponentiation); Curly Brackets (or Braces): Curly braces can create multiple text characters. Generally speaking, a pattern in curly braces may contain a header and a postscript. The curly brace expression itself may contain a list of strings separated by commas, an integer interval, or a single character interval. Whitespace characters cannot be embedded in this mode. echo Number_&#123;1..5&#125;, echo &#123;Z..A&#125;, echo Front-&#123;A,B,C&#125;; mkdir &#123;2007..2009&#125;-0&#123;1..9&#125; &#123;2007..2009&#125;-&#123;10..12&#125; Furthermore, curly braces can be nested: 12echo a&#123;A&#123;1,2&#125;,B&#123;3,4&#125;&#125;baA1b aA2b aB3b aB4b Parameter Expansion: Command Substitution: Use the output of a command as expansion mode: ls -l $(which cp), echo$(ls), file $(ls /usr/bin* | grep zip); In the old shell, [&#96;&#96;] can replace [$()]: 1ls -l `which cp` Double Quotation: In double quotes, the special characters used by the shell lose their special meaning and are treated as ordinary characters. Mathematical expansion, parameter expansion and command substitution are still performed. (Double quotation can retain whitespace) ls -l &quot;two words.txt&quot;, echo &quot;$USER $((2+2)) $(cal)&quot;; echo &quot;$(cal)&quot; is different with echo $(cal), try it. Single Quotation: Single quotation disables all mode expansions. 123456ubuntu@VM-16-17-ubuntu:~$ echo text ~/*.txt &#123;a,b&#125; $(echo foo) $((2+2)) $USERubuntu@VM-16-17-ubuntu:~$ echo &quot;text ~/*.txt &#123;a,b&#125; $(echo foo) $((2+2)) $USER&quot;ubuntu@VM-16-17-ubuntu:~$ echo &#x27;text ~/*.txt &#123;a,b&#125; $(echo foo) $((2+2)) $USER&#x27;text /home/ubuntu/lazy_dog.txt /home/ubuntu/ls.txt a b foo 4 ubuntutext ~/*.txt &#123;a,b&#125; foo 4 ubuntutext ~/*.txt &#123;a,b&#125; $(echo foo) $((2+2)) $USER **Backslash: ** \\ can escape special characters in shell, but not in single quotes. 12echo &quot;The balance for user $USER is: \\$5.00&quot;mv bad\\&amp;filename good_filename NOTE: 1$&quot;abc&quot;, $&quot;\\n&quot; The above is a special string conversion syntax used to localize strings (with Settings such as the environment variable LANG or LC_MESSAGES). 1$&#x27;abc&#x27;, $&#x27;\\n&#x27;, $&#x27;ab\\tc&#x27; When you use the form $’abc’ in the Shell, the Shell extends the string and replaces the special characters with the corresponding escape sequence.","tags":[{"name":"shell","slug":"shell","permalink":"https://stu-yue.github.io/tags/shell/"},{"name":"linux","slug":"linux","permalink":"https://stu-yue.github.io/tags/linux/"}],"categories":[{"name":"languages","slug":"languages","permalink":"https://stu-yue.github.io/categories/languages/"},{"name":"shell","slug":"languages/shell","permalink":"https://stu-yue.github.io/categories/languages/shell/"}]},{"title":"Memo | Redirection","date":"2023-08-25T06:56:53.000Z","path":"jottings/languages/shell/memo_redirection/","text":"Input&#x2F;output redirection is achieved by modifying file pointers. When redirection occurs, file descriptors themselves are not changed, instead, it’s the file pointers associated with the file descriptors that are altered. Manipulation of File Descriptors in Shell In redirection, &amp; is used to indicates the following numbers is file descriptor rather than a filename. Output Redirection: stdiout redirection: command &gt;file: [overwrite], stdout overwrites the file. &gt; file: clear the content. command &gt;&gt;file: [append], stdout appends to the file. stderr redirection: command 2&gt;file: [overwrite], stderr overwrites the file. command 2&gt;&gt;file: [append], stderr appends to the file. Both stdout and stderr: command &gt;file 2&gt;&amp;1: [overwrite], both stdout and stderr overwrite the file. command &gt;&gt;file 2&gt;&amp;1: [append], both stdout and stderr append to the file. command &gt;file1 2&gt;file2: stdout overwrites the file1, stderr overwrites the file2. command &gt;&gt;file1 2&gt;&gt;file2: stdout appends to file1, stderr appends to file2. command &gt;file 2&gt;file: [not recommend] file is opened twice, leading to resource competition. command &gt;&gt;file 2&gt;&gt;file: [not recommend] file is opened twice, leading to resource competition. Input Redirection: command &lt;file: take the contents of the file as the input to command. 1234#!/bin/bashwhile read str; do echo $strdone &lt;readme.txt command &lt;&lt;END: read data from standard input (keyboard) until meeting the delimiter END (defined by the user). command &lt;file1 &gt;file2: input by file1, and output to file2 &amp;&gt;: redirect both stdout and stderr to the same location (usually a file). &gt;&amp;: redirect the output of one file descriptor to another &gt;&amp;-: equal to redirection to /dev/null PipelinePipeline is used to link the stdout of the previous instruction as the stdin of the next instruction. Pipeline is often used to perform complex operations on data. It’s possible to put several commands together to form a pipeline (usually called filter). For example: 1ls /bin /usr/bin | sort | uniq | less CommandThere’s some useful command in this memo: cat: To link file. cat can accept not one parameter, so it can concatenate the file: cat movie.mpeg.0* &gt; movie.mpeg; sort: To sort the text lines. uniq: To report&#x2F;omit the repetitive lines. grep: To print the matching rows. -i (ignore upper&#x2F;lower), -v (reverse find, print mismatching lines), -n (show the matching lines), -r (recursively find), -l (only print matching filename), -c (only print the number of matching line) wc: To print the LF, word, bytes of the text. -c (bytes), -l (lines), -w (words) head&#x2F;tail: To print first&#x2F;last part of text. -n (lines), -c (bytes) tail -f filename: continue to monitor this file, when the new is added to the file, they appear immediately on the screen. tee: read from stdin, and write to stdout and file. -a (append mode), -i (ignore-interrupts) ls -l | tee -a ls.log : print the content both in the stdout and file. ls /usr/bin | tee ls.log | grep zip","tags":[{"name":"shell","slug":"shell","permalink":"https://stu-yue.github.io/tags/shell/"},{"name":"linux","slug":"linux","permalink":"https://stu-yue.github.io/tags/linux/"}],"categories":[{"name":"languages","slug":"languages","permalink":"https://stu-yue.github.io/categories/languages/"},{"name":"shell","slug":"languages/shell","permalink":"https://stu-yue.github.io/categories/languages/shell/"}]},{"title":"Memo | GIL and Coroutine","date":"2023-08-10T08:01:53.000Z","path":"jottings/languages/python/memo_GIL_and_coroutine/","text":"GIL (Global Interpreter Lock) GIL is not a characteristic of Python itself, but rather a characteristic to CPython, the reference implementation of Python. In CPython, GIL is a mutex lock used to ensure that only on thread is executing at a time within a process. In the absence of the GIL, it’s possible for multiple threads executing the same code simultaneously to cause incorrect reference counting of variables, leading to the garbage collector directly reclaiming the variables involved in the executed code. This can result in errors when other threads attempt to use those reclaimed variables. How to work: Each thread acquires the GIL at the beginning of its execution to prevent other threads from preempting. Similarly, after each thread completes a segment of code (or before system calls that may cause blocking, such as IO operations), it releases the GIL to allow other threads to utilize resources. In CPython, there’s another mechanism called check_interval, where the interpreter periodically checks the status of the GIL lock for threads. After a certain interval, the interpreter forces the current thread to release the GIL, allowing other threads to have opportunity to execute. Overall, each Python thread is encapsulated in a similar loop structure. Let’s take a look at the following code: 12345678910111213for (;;) &#123; if (--ticker &lt; 0) &#123; ticker = check_interval; /* Give another thread a chance */ PyThread_release_lock(interpreter_lock); /* Other thread may run now */ PyThread_acquire_lock(interpreter_lock); &#125; bytecode = *next_instr++; switch(bytecode) &#123; /* execute the next instruction ... */ &#125;&#125; The above example represents instruction counting, while the current approach is mostly based on time slicing. Here’s another example that demonstrates GIL’s working principle: 123456789101112131415161718# coding=utf-8from threading import Threadfrom multiprocessing import Processdef test(): # endless loop for full CPU utilization while True: pass# multi-thread version:# t1 = Thread(target=test)# multi-process version:t1 = Process(target=test)t1.start()# main thread executionwhile True: pass When the multi-thread code running, we can observe the CPU utilization from htop as shown below: 121 [50%]2 [50%] According to the above, we can know that each thread spends their a half time waiting for GIL. Additionally, the multi-process version as shown below: 121 [100%]2 [100%] The other way to utilize multi-core CPU is Multi-Process or Coroutine GeneratorGenerator contains the yield keyword to produce values. It has the following characteristics: Laziness: Generators are lazy in nature, meaning they produce values on-demand as requested by the caller, rather than generating all the values at once. This lazy evaluation allows for efficient memory usage, especially when dealing with large or infinite sequences. Memory efficiency: Due to their lazy evaluation, generators are memory-efficient. Iterability: Generators are iterable objects, which means they can be looped over using a for loop or consumed by other iterable functions like list() or sum(). They provide a convenient way to iterate over a sequence of values without the need to store the entire sequence in memory. State persistence: Generators maintain their internal state between successive calls. When a generator function is paused at a yield statement, the local variables’ values are preserved. This allows the generator to resume execution from where it left off, retaining the necessary information to generate the next value. Infinite sequences: Generators can be used to represent infinite sequences since they generate values on-the-fly. Function-like behavior: Generators are defined using the def keyword like regular functions, and they can have parameters, return values, and other function features. However, they differ in their execution behavior, as they can be paused and resumed. When the generator function finishes executing (no more yield statements or returns), the generator object raises a StopIteration exception. yield from is a syntactic sugar that allows delegation of generator execution within a generator function. It provides a concise way to call a sub-generator from a parent generator and directly pass the values yielded by the sub-generator to the parent generator. 123456789101112def sub_generator(): yield &#x27;A&#x27; yield &#x27;B&#x27; yield &#x27;C&#x27; def parent_generator(): yield &#x27;START&#x27; yield from sub_generator() yield &#x27;END&#x27; for item in parent_generator(): print(item) yield and send are used together to allow the generator function to receive values during each iteration and send values back into the generator function for processing. Here is an example of how yield and send are used: 1234567891011121314151617def generator_function(): result = yield # First call, receives a value sent by send() while True: print(&#x27;gen&#x27;, result) result = yield resultgen = generator_function()# Start the generatornext(gen) # or gen.send(None)output = gen.send(&quot;Hello&quot;)print(&#x27;out&#x27;, output)output = gen.send(&quot;World&quot;)print(&#x27;out&#x27;, output)# print result:# gen Hello# out Hello# gen World# out World generator.throw(AnyException) allows generator to throw an Exception, and generator.close() can stop the generator. CoroutineCoroutines are implemented using generator functions and the yield statement. The principle of coroutines is as follows: Coroutine Function Definition: Define a generator function as a coroutine function. This function can use the yield statement to specify suspension points, where it pauses execution and returns a value to the caller. Coroutine Initialization: Create a coroutine object by calling the coroutine function. Coroutine Iteration: Use the next() function or the .send() method of the coroutine object to iterate and execute the coroutine. When the coroutine encounters a yield statement, it pauses execution and returns the result to the caller. Coroutine Resumption: When the caller sends a value to the coroutine (using the .send() method), the coroutine resumes execution from the last paused position and uses the sent value as the result of the yield expression. Coroutine Termination: When the coroutine reaches the end of the function or encounters a StopIteration exception, the coroutine terminates. Further calls to the .send() method on the coroutine object will raise a StopIteration exception. Coroutines allow achieving concurrent execution without the need for multiple threads or processes. Coroutines can switch between different execution paths, enabling efficient asynchronous programming. Coroutines can also delegate to other coroutines using the yield from statement, enabling more complex cooperation and task decomposition. Python provides the asyncio module to support coroutine programming, where the async and await keywords offer a more concise syntax for defining and managing coroutines. With asyncio, it becomes easy to write asynchronous programs and handle tasks like I&#x2F;O operations, network communication, and more. It’s important to note that coroutines run in a single thread, so their performance may not be as good as multi-threading or multiprocessing when it comes to CPU-bound tasks. However, in I&#x2F;O-bound tasks, coroutines shine because they can effectively utilize the waiting time for I&#x2F;O to execute other tasks. Async&#x2F;Await async def is a keyword combination used in Python to define asynchronous functions. An asynchronous function is a special type of function that can contain await expressions, which suspend the execution of the function and wait for the completion of asynchronous operations. Here is a example: 12345678910111213141516import asyncio# Define an asynchronous functionasync def async_func(): print(&quot;Start&quot;) await asyncio.sleep(1) # Suspend function execution using await, waiting for the completion of an asynchronous operation print(&quot;End&quot;)# Run the asynchronous function in an event loopasync def main(): await async_func()# Create an event loop and run the main functionloop = asyncio.get_event_loop()loop.run_until_complete(main())loop.close() await is a keyword used to suspend the execution of an asynchronous function and wait for the completion of an asynchronous operation. await can only be used within an asynchronous context and is typically used in conjunction with async def. The general usage of await is as follows: Use await within an asynchronous function or coroutine to suspend its execution and wait for the completion of an asynchronous operation. For example: 123async def async_func(): result = await async_operation() # 等待 async_operation() 异步操作的完成 # 继续异步操作，使用异步操作的结果 result Note that await can only be used within asynchronous functions or coroutines. It is not valid to use await in synchronous code. Typically, await is followed by an awaitable object, such as an asynchronous function, coroutine, asynchronous iterator, etc. These awaitable objects must implement specific protocols, which include methods like __await__() or __aiter__(). The await expression invokes these methods to obtain an iterator or a context manager from the awaitable object and waits for its completion. Here are some common awaitable objects: Asynchronous functions or coroutines: Use await to wait for the execution of an asynchronous function or coroutine to complete. Asynchronous generators: Use await to iterate over asynchronous generators and wait for each generated value. Asynchronous iterators: Use await to iterate over asynchronous iterators and wait for each iteration value. Asynchronous context managers: Use await to enter and exit the context of an asynchronous context manager. For example: 123456async def async_func(): async with async_context_manager() as cm: await cm.do_something() # Wait for the completion of the asynchronous context manager async for item in async_iterator(): await process_item(item) # Wait for the completion of each item generated by the asynchronous iterator","tags":[{"name":"python","slug":"python","permalink":"https://stu-yue.github.io/tags/python/"}],"categories":[{"name":"languages","slug":"languages","permalink":"https://stu-yue.github.io/categories/languages/"},{"name":"python","slug":"languages/python","permalink":"https://stu-yue.github.io/categories/languages/python/"}]},{"title":"Memo | Optimization Algorithm in Machine Learning","date":"2023-08-05T14:56:53.000Z","path":"jottings/mathematics/memo_optimization_alg/","text":"Reproduced in The Summary of Optimization Algorithm in ML For machine learning algorithms with diverse forms and characteristics, we have various optimization algorithms suitable for optimizing their objective functions. Apart from a few problems that can be solved using Brute Force Search to obtain the optimal solution, we can categorize the optimization algorithms used in machine learning into two types: Analytical Solutions: These algorithms aim to find the optimal solution to the objective function by solving mathematical equations or performing algebraic manipulations. They often involve setting derivatives or gradients to zero and solving the resulting equations. Analytical solutions are typically used for linear regression, logistic regression, and certain types of optimization problems with closed-form solutions. Numerical Optimization: These algorithms iteratively search for the optimal solution by evaluating the objective function at different points in the search space. They do not rely on explicit mathematical equations or derivatives. Numerical optimization methods include gradient-based algorithms like gradient descent and its variants, Newton’s method, stochastic gradient descent, and quasi-Newton methods. Global Optimization Methods: Heuristic Algorithm, Simulated Annealing, Particle Swarm Optimization, etc. Local Optimization Methods: Gradient Based: First Order Derivative: Gradient Descent: Second Order Derivative: Newton Method: Divide and Conquer: Coordinate Descent: SMO Algorithm: Staged Optimization: Dynamic Programming: The following picture illustrates the organization of this memorandum:","tags":[{"name":"optimization","slug":"optimization","permalink":"https://stu-yue.github.io/tags/optimization/"},{"name":"ml","slug":"ml","permalink":"https://stu-yue.github.io/tags/ml/"}],"categories":[{"name":"mathematics","slug":"mathematics","permalink":"https://stu-yue.github.io/categories/mathematics/"}]},{"title":"Memo | Exploring Linux Files and Directories","date":"2023-08-05T02:56:53.000Z","path":"jottings/languages/shell/memo_exploring_os/","text":"Let’s start by learning some commands that are helpful for researching Linux systems. type: To explain how to interpret a command name. which: To show which executable program will be executed. man: To show command manual. apropos: To display a list of appropriate commands. info: To display the command info. alias: To create an alias for the command unalias: To cancel a alias for the command. ls: To list the files and directories in the current directory. -a, -d, -h, -r/--reverse, -l, -S[sort by size], -t[sort by modification time] file: To determine the file type. file filename is OK. less: To browse the content of a file, specifically, less is an improved version of more. less filename is OK. Commands Behavior Page UP or b Backward one window Page Down or space Forward one window UP Arrow Backward one line Down Arrow Forward one line [N]G Go to last line in file (or N lines) [N]g Go to first line in file (or N lines) &#x2F;characters Search forward for matching line n Repeat previous search h Display help. cp： Options Implication -a, –archive Copy files and directories, along with their attributes, including ownership and permissions -i, –interactive Prompt the user for confirmation before overwriting an existing file (default disable) -r, –recursive -u, –update Update the content not already present in the original -v, –verbose Display detailed command operation information mv: Options Implication -i, –interactive Prompt the user for confirmation before overwriting an existing file (default disable) -u, –update Update the content not already present in the original -v, –verbose Display detailed command operation information rm: Options Implication -f, –force Directly delete the file even if its attributes are read-only, without requiring individual confirmation -i, –interactive Prompt the user for confirmation before overwriting an existing file (default disable) -r, –recursive -v, –verbose Display detailed command operation information ln: ln file hard_link creates a hard link, and ln -s item soft_link creates a soft link. Hard Links: A hard link is a direct reference to the same physical location of a file on the storage device. It creates an additional entry in the file system that points to the same inode (data structure representing a file) as the original file. Changes made to either the original file or the hard link are reflected in both, as they refer to the same underlying data. Hard links cannot reference directories or files on different file systems or partitions. Soft Links: A symbolic link is a special file that contains a path pointing to another file or directory. It acts as a pointer or shortcut to the target file or directory. Symbolic links are independent files and have their own inodes. Modifying the original file or symbolic link does not affect each other, as they are separate entities. Symbolic links can reference directories or files on different file systems or partitions. Soft links can be created to a non-existent filename (of course, if you use vi on this soft link file, Linux will automatically create a new file named “filename”). Hard links cannot be created in such cases since the file must already exist, and the inode must exist as well. Using ls -li can view the inodes of the files. drwxrwxr-x 2 ‘s 2 is the number of hard links to a file. Besides, the usual number of inodes for a directory is 2 (include parent directory and itself .) tar: To create archive files (usually with the .tar extension) and archive files. create archive file: tar -cvf archive.tar file1 file2 directory/ -c (create archive file), -v (view details), -f (specify archive file) extract archive file: tar -xvf archive.tar -x (extract the content of archive files) compress archive file: tar -czvf archive.tar.gz directory/ -z (use gzip to compress) decompress archive file: tar -xzvf archive.tar.gz, tar -xjvf archive.tar.bz2 list the content of archive files: tar -tvf archive.tar tar -tf test.tar: view package structure tar -C dest_dir/ -x[z]vf test.tar[.gz] specified_dir_or_file...[path in -tf shows]: unzip the specified file&#x2F;directory in the package; unzip archive.zip -d dest_dir split and cat: split the tar.gz into small files, split: split -6/-b 3M -d -a 2 cud_test.tar.gz[dst_filename] cud_test.tar.gz_[out_filename] 12345cud_test.tar.gz_00cud_test.tar.gz_01cud_test.tar.gz_02cud_test.tar.gz_03cud_test.tar.gz_04 -&lt;line_N&gt; : split into a file every N lines -b &lt;bytes&gt; : split into a file every N bytes -d : specify the generated split file suffix in numeric form -a x : set the length of the sequence (suffix digits) cat cud_test.tar.gz_* &gt; cud_test.tar.gz : to concatenate split files; Before starting using commands, let’s introduce wildcards that provide special characters to help you quickly specify a group of filenames. Wildcard Implication * match any sequence of characters, including zero characters. ? match any single character (excluding zero character) [character] match any single character within the specified character set [!character] match any single character without the specified character set [[:class:]] match any single character within the specified character class The following table lists the most commonly used character classes. Character Class Implication [:alnum:] match any single letter or digit [:alpha:] match any single letter [:digit:] match any single digit [:lower:] match any single lower letter [:upper:] match any single upper letter There are some examples constructed with wildcard: *, g*, b*.txt, Data???, [abc]*, BACKUP.[0-9][0-9][0-9], [[:upper:]], [![:digit:]]*, *[[:lower:]123] Service and Systemctl systemctl is a system service manager that actually combines the service and chkconfig commands together. 123456789systemctl is-enabled httpd.servicesystemctl status httpd.servicesystemctl list-unit-files --type=servicesystemctl restart httpd.servicesystemctl stop httpd.servicesystemctl reload httpd.servicesystemctl restart httpd.servicesystemctl enable httpd # bootstrapsystemctl disable httpd # not bootstrap service 12345service ssh startservice ssh stopservice ssh restartservice ssh statusservice --status-all","tags":[{"name":"shell","slug":"shell","permalink":"https://stu-yue.github.io/tags/shell/"},{"name":"linux","slug":"linux","permalink":"https://stu-yue.github.io/tags/linux/"}],"categories":[{"name":"languages","slug":"languages","permalink":"https://stu-yue.github.io/categories/languages/"},{"name":"shell","slug":"languages/shell","permalink":"https://stu-yue.github.io/categories/languages/shell/"}]},{"title":"Hot Points","date":"2023-07-22T12:00:53.000Z","path":"jottings/tidbits/hot_points/","text":"Generate random numbers by reading thermal noise on CPU pins. Hot Plug, refers to the operation of inserting&#x2F;removing a device while it is running. In contrast, Cold Plug means do above operation while the device is powered off. Software version’s explanation: Version Description Snapshot Development version Alpha Internal beta Beta Public beta Pre (M) Similar to Alpha, sometimes subdivided into M_(Milestone) version RC(Release Candidate) During Beta stage, new features will continue to be added, but once the RC version is reached, there will mainly be on debugging and issue resolution. GA(General Availability) Some software may be labeled as “stable” or “production”. Release&#x2F;Stable Current The latest version, but no necessarily a stable one. Eval There may be a monthly or fixed time limit for usage. Declarative Programming expresses the logic of a computation without describing its control flow. Many languages that apply this style attempt to minimize or eliminate side effects by describing what the program must accomplish in terms of the problem domain (what to do), rather than specifying all the details of how the program should achieve the result (how to do). Imperative Programming uses statements that change a program’s state. Imperative Programming focuses on describing how a program operates step by step, rather than on high-level descriptions of its expected results. KVM: Kernel-based Virtual Machine, is a virtualization infrastructure used in the Linux kernel to turn the Linux kernel into a hypervisor; Hypervisor: creates and runs virtual machines, sometimes called a virtual machine monitor (VMM), like a meta-OS; IT resource pooled, OS and hardware decoupled, allocated according to needs; A computer on which a hypervisor runs one or more virtual machines is called a host machine, and each virtual machine is called a guest machine; (Type 1) Native or bare mental hypervisor: runs directly on the host’s hardware to manage guest operating systems. It takes the place of a host operating system and VM resources are scheduled directly to the hardware by the hypervisor. This type of hypervisor is most common in an enterprise data center or other server-based environments. KVM, Microsoft Hyper-V, and VMware vSphere are examples of a type 1 hypervisor. KVM was merged into the Linux kernel in 2007, so if you’re using a modern version of Linux, you already have access to KVM. (Type 2) Hosted hypervisor: run on a conventional operating system as a software layer or application. It works by abstracting guest operating systems from the host operating system. VM resources are scheduled against a host operating system, which is then executed against the hardware. A type 2 hypervisor is better for individual users who want to run multiple operating systems on a personal computer. VMware Workstation and Oracle VirtualBox are examples of a type 2 hypervisor. (Emulator) The difference with the Hypervisor is that the Hypervisor runs the same VM and host CPU architecture, while the Emulator can be used to run systems or programs on other hardware platforms (arm, mips, x86). When used as a machine emulator, QEMU can run OSes and programs made for one machine (e.g. an ARM board) on a different machine (e.g. your x86 PC). By using dynamic translation, it achieves very good performance; QEMU can use other hypervisors like Xen or KVM to use CPU extensions (HVM) for virtualization. When used as a virtualizer, QEMU achieves near native performances by executing the guest code directly on the host CPU. When QEMU is used in conjunction with KVM, KVM provides hardware virtualization support, while QEMU is responsible for virtual machine simulation and management. In this configuration, KVM functions as a Hypervisor to directly interact with hardware and provide high-performance virtualization support, while QEMU runs on the upper layer of KVM and is responsible for VM simulation and management, including device simulation, VM creation, start, and stop. So while QEMU is not a traditional Hypervisor on its own, when used in conjunction with KVM, QEMU can work with KVM to provide a complete virtualization solution and act as part of a Hypervisor","tags":[{"name":"points","slug":"points","permalink":"https://stu-yue.github.io/tags/points/"}],"categories":[{"name":"tidbits","slug":"tidbits","permalink":"https://stu-yue.github.io/categories/tidbits/"}]},{"title":"TBR_compiler","date":"2023-07-22T12:00:53.000Z","path":"jottings/tidbits/tbr_compiler/","text":"《编译原理》课件搬运 ——&gt; 源地址 1.1 编译器作用 编译器（Compiler） 读入以某种语言（源语言）编写的程序； 输出等价的用另一种语言（目标语言）编写的程序； 通常目标程序是可执行的； 解释器（Interpreter） 不生成目标程序，直接根据源程序的语义执行源程序中指定的操作； Java语言的处理结合了编译(.class字节码)和解释，Python会将编译的字节码(pyc)存放于_pycache_(可跨平台部署，一定程度防源码泄露，import的py不变pyc就不变[检查py和pyc的修改时间是否一致]) 1.2 编译器的结构 编译器可以分为分析部分和综合部分 分析（analysis）部分 &#x2F; 前端（front end） 把源程序分解成组成要素，以及相应的语法结构； 使用这个结构创建源程序的中间表示； 同时收集和源程序相关的信息，存放到符号表； 综合（synthesis）部分 &#x2F; 后端（back end） 根据中间表示和符号表信息构造目标程序； 前端部分是机器无关的，后端部分是机器相关的； 编译器分成执行顺序的一组步骤： 1.3 词法分析 词法分析&#x2F;扫描（lexical analysis&#x2F;scanning） 读入源程序的字符流，输出为有意义的词素（lexeme） &lt;token-name, attribute-value&gt; token-name：由语法分析步骤使用； attribute-value：指向相应的符号表条目，由语义分析&#x2F;代码生成步骤使用； 例子 position &#x3D; initial + rate * 60 &lt;id, 1&gt; &lt;&#x3D;, &gt; &lt;id, 2&gt;, &lt;+, &gt; &lt;id, 3&gt; &lt;*, &gt; &lt;number, 4&gt; 1.4 语法分析 语法分析&#x2F;解析（syntax analysis&#x2F;parsing） 根据各个词法单元的第一个分量来创建树型的中间表示形式，通常是语法树（syntax tree） 中间表示形式指出了词法单元流的语法结构； 1.5 语义分析 语义分析（semantic analysis） 使用语法树和符号表中的信息，检查源程序是否满足语言定义的语义约束； 同时收集类型信息，用于代码生成、类型检查、类型转换； 1.6 中间代码生成 根据语义分析输出，生成类机器语言的中间表示； 三地址代码（three-address code） 每个指令最多包含三个运算分量；很容易生成机器语言指令； 1.7 中间代码优化 通过对中间代码分析，改进中间代码的质量；（更快、更短、能耗更低） 1.8 代码生成 把中间表示形式映射成目标语言；（寄存器分配、指令选择） 1.9 其他 符号表管理：记录源程序中使用的变量的名字，收集各种属性； 编译器构造工具：扫描器（Lex）、语法分析器（Yacc）、语法制导的翻译引擎； 程序设计语言的新发展向编译器设计者提出新的要求 设计相应的算法和表示方法来翻译和支持新的语言特征，如多态、动态绑定、类、类属 (模板) 、… 编译器设计者还需要更好地利用新硬件的能力 RISC技术、多核技术、大规模并行技术 1.10 程序设计语言的基础概念 静态&#x2F;动态： 静态：支持编译器静态决定某个问题； 动态：只允许在程序运行时刻作出决定； 作用域： x的作用域指程序文本的一个区域，其中对x的使用都指向这个声明； 静态作用域（static scope）: 通过静态阅读程序可决定； 动态作用域（dynamic scope）: 运行时确定x的指向； 环境与状态： 环境（environment）: 是从名字到存储位置的映射； 状态（state）: 从存储位置到它们值的映射；","tags":[{"name":"points","slug":"points","permalink":"https://stu-yue.github.io/tags/points/"}],"categories":[{"name":"tidbits","slug":"tidbits","permalink":"https://stu-yue.github.io/categories/tidbits/"}]},{"title":"TBR_Net_Info","date":"2023-07-22T12:00:53.000Z","path":"jottings/tidbits/tbr_net_info/","text":"1 IPTV 和 OTTIPTV（Internet Protocol Television）是一种通过Internet Protocol（IP）在专用网络上传输视频和音频内容的技术。它使用LAN（局域网）或WAN（广域网）等封闭网络生态系统进行操作。IPTV需要专门的设备（如机顶盒）和宽带连接，用户可以通过这些设备访问传统的电视频道，如CBS、CNN、FOX、HBO等。IPTV的优势在于提供高质量的图像和声音，因为它使用专用网络进行传输，不会占用用户的互联网带宽[2]。 OTT（Over-The-Top）是一种通过公共互联网传输视频内容的技术。OTT服务基于视频点播（VOD）的分发，用户只需订阅服务，就可以通过连接到互联网的设备播放内容。OTT服务的优势在于成本较低、安装简便，并且提供各种各样的节目选择。然而，OTT的视频质量取决于用户的互联网连接速度，可能会出现缓冲导致内容中断的情况[2]。 以下是IPTV和OTT之间的主要区别： 内容传递和网络结构： IPTV使用封闭网络（LAN、WAN），提供平稳的内容传递，不依赖于互联网带宽。 OTT使用公共互联网连接，任何人都可以访问，但服务质量取决于用户的互联网速度。 设备需求： OTT不需要额外的硬件设备，只需使用专用应用程序即可。 IPTV需要专门的机顶盒和路由器。 视频质量： OTT的视频质量取决于互联网速度。 IPTV提供更流畅、更好的用户体验，具有更高的视频质量和音频效果。 价格： OTT服务通常是免费的，但可能需要付费订阅以去除广告。 IPTV的价格较高，平均每月费用在15美元至30美元之间。 内容类型： OTT提供免费或付费的内容，也可以是混合内容。 IPTV提供独家的付费内容，如订阅视频点播（SVOD）。 OTT+直播？ 2 运营商、ISP、ICP、骨干网 电信运营商：指提供基础的电信业务，包括固网（有线），移动网络（无线）的运营商（电信&#x2F;联通&#x2F;移动是一级&#x2F;基础运营商）。 ISP：多指互联网接入服务提供商，长城宽带等二级运营商（通过购买电信运营商的带宽来提供互联网接入服务） ICP：内容提供商指提供互联网上各种内容的服务提供商，例如视频、音乐、新闻、社交媒体等。在中国，一些知名的内容提供商包括爱奇异、优酷等。 内容提供商通常将其内容托管在电信运营商的机房内，用户通过购买ISP的服务来访问这些内容。 局域网：几台计算机互联（计算机组）（光纤【很细的光缆】&#x2F;wifi）； 四种技术类型&#x2F;标准：以太网、令牌环、令牌总线、光纤分布式数据接口(FDDI)； 城域网：城市范围内的计算机通信网（大型LAN）（光缆）； 单独列出，由于有一个独有标准：分布式队列双总线DQDB（Distributed Queue Dual Bus） 骨干网：通常是一个国家或地区范围内的网络，用于连接不同城市或地区的网络； 广域网：跨越多个城市、国家或洲际，连接不同的局域网和城域网（可包含多个骨干网） 中国主要的骨干网： 中国电信 CHINANET：中国公用计算机互联网，原邮电部 中国移动 CMNET：中国移动互联网 中国联通 UNINET：中国联通计算机互联网，还有子公司网通的中国网通公用互联网（CNCNET） 中科院 CSTNET：中国科技网 信息产业部 CHINAGBN：中国金桥信息网 教育部 CERNET：中国教育和科研计算机网，国家公用经济信息通信网 12345移动 铁通 电信 联通(CDMA) 卫通 联通(GSM) 网通 | | | | / \\ | | \\ / \\ | / \\ \\ / \\ / \\ | / | \\ / 移动 电信 航天科技 联通","tags":[{"name":"points","slug":"points","permalink":"https://stu-yue.github.io/tags/points/"}],"categories":[{"name":"tidbits","slug":"tidbits","permalink":"https://stu-yue.github.io/categories/tidbits/"}]},{"title":"Summa | The Majority of Memory Categories","date":"2023-07-20T08:41:53.000Z","path":"jottings/architecture/summa_memory_categories/","text":"Reproduced in 8 types of memory every embedded engineer should know about! The majority of memory can be categorized as shown in the following picture: The real difference between primary and secondary memories is the speed&#x2F;volatility(without power) tradeoffs. Primary MemoryPrimary memory is very fast, but it cannot hold data without power. The popular name for Primary Memory is RAM, which has 2 most important types namely SRAM and DRAM. Bistable Circuit usually consists of two complementary transistors or other switching devices, one used to hold the circuit in one stable state and the other to switch the circuit to another stable state. The switch between these two states is triggered by the input signal. Bistable Circuit can store data. Latches and registers are bistable devices. SRAM is the use of bistable flip-flop to save information, as long as the power is not lost, the information is not lost. DRAM uses capacitors to store charge to store information, so data stored in the DRAM must be constantly refreshed every few milliseconds or else it will end up being erased. The action is taken care of by a special device named DRAM controllers. SRAM DRAM Construction Principle It uses a cross-coupled flip flop configuration of transistors It uses a capacitor transistor circuit to hold data Cost Relatively more expensive, it needs more transistors per bit of data it can store Relatively less expensive, as fewer transistors per bit of storage are needed Speed Faster Slower (capacitor charge and discharge time) Volatility As long as power is ON, it can store data since it uses no capacitors Data needs to be continuously refreshed (usually in the order of 4 times a second) since the capacitors leak power. Power consumption Less More Density Less dense (6 transistors, more area needed) Highly dense (1 pass transistor and 1 capacitor, easy to integrate) Addition components needed None DRAM controllers are needed to make it work like an SRAM. This controller offloads the data refreshing duties of a microprocessor and hence a DRAM coupled with a DRAM controller behaves more like an SRAM from the processor’s perspective. Application areas Cache memory (Ls2) Main memory (memory chips) NVRAM or Non Volatile Random Access Memory, is a special type of RAM that can store data permanently. It’s basically an SRAM with a power supply Secondary MemoryROM MASK ROM: The main characteristic of this device is the fact that the data is written onto the device as it gets manufactured and it is impossible to change them. This is done by designing the chip in such a manner so that it already contains the necessary data. In order to mass production, the manufacturer makes a ROM or EPROM with original data as a sample in advance, and then mass-produces the same ROM as the sample. This kind of ROM sample for mass production is the MASK ROM, and the data burned in the MASK ROM can never be modified. PROM stands for Programmable Read-Only Memory. These are programmable chips for user needs, the main characteristic being it can only be programmed one time. That is it cannot be erased or reprogrammed. They are also known as One Time Programmable devices or OTPs for short. EPROM stands for Erasable Programmable Read-Only Memory. These chips usually have a small glass window on top and if you expose them to direct sunlight(UV, ultraviolet) that will erase the chip’s data. They can then be programmed again with fresh data. Cons: Inconvenient, instability, can not be exposed to the light source otherwise easy to lead to data loss EEPROM stands for Electrically Erasable Programmable Read-Only Memory. These chips can be erased and reprogrammed using electricity as opposed to exposing them to UV rays as EPROMs. EEPROM can be erased and reprogrammed on a computer or dedicated device, generally plugging and playing. FLASH MEMORY The basic storage unit of flash memory is the transistor-based storage unit, and each storage unit can store 1 bit of data. Storage units are usually organized into a block, and each block contains thousands of storage units. Each storage unit has a floating gate to store electric charges. The state of a storage unit can be determined as “1” or “0” based on the amount of electric charges stored in the floating gate. The state of a storage unit is changed by injecting or extracting electrons into&#x2F;from the floating gate to modify the amount of electric charges stored in it. Flash memory uses Hot carrier injection(HCI) mechanism to write data. In simple terms, a certain storage unit is grounded at the source, a positive voltage is applied to its control gate, and a positive voltage is applied to the drain to generate a strong electric field between the source and drain. This will give electrons enough energy (hot carriers) to be attracted by the voltage at the control gate and injected into the floating gate. Afterwards, as the insulating material on the top and bottom of the floating gate is not conductive, these electrons are trapped in the floating gate and cannot escape. (Every time electrons enter and exit the surrounding silicon dioxide on the floating gate, it will cause aging[1]) To save cost, flash memory adopts page programming mode. Each page contains a certain number of storage units, and all units in a page are written at the same time. Cons: Flash memory has the issue of wear-out (write&#x2F;erase endurance limits), which is usually mitigated by disabling bad blocks, and then reducing usable capacity. USB flash disk, namely “U disk”, is a new generation of storage devices based on USB interface and flash memory chip as storage medium. It is basically composed of five parts: USB port, main control chip, flash memory chip, PCB backboard, outer package. USB flash drives, SD cards, and SSDs are a type of storage device that uses flash memory chips as the storage medium. They are primarily composed of a controller (main control) and flash memory chips, and have no mechanical structure, consisting purely of electronic circuitry. They are resistant to physical shocks and impacts. The controller manages data storage and other functions. Even after power loss, data remains stored in the memory cells. USB flash drives generally have a cache on them to prevent the loss of data copies from being quickly plugged in&#x2F;out. OTHER Hard Disk Drive: The disk reads data according to the polarity of the magnetic particle and writes data according to the polarity of the magnetic head. Compact Disk: CD-ROM can only be read and not written to because after being burned once, each unit has a fixed different reflectivity (the reading probe emits laser and the reflected laser is read as “1”, or non-reflected laser is read as “0”). Floppy Disk: A type of magnetic disk, less capacity, slower speed.","tags":[{"name":"memory","slug":"memory","permalink":"https://stu-yue.github.io/tags/memory/"}],"categories":[{"name":"architecture","slug":"architecture","permalink":"https://stu-yue.github.io/categories/architecture/"}]},{"title":"Welcome to Yue's Jotter","date":"2023-07-13T08:23:57.000Z","path":"jottings/intro/","text":"","tags":[],"categories":[]},{"title":"Summa | Firmware and Drivers","date":"2023-07-13T06:21:53.000Z","path":"jottings/architecture/summa_firmware_and_drivers/","text":"Reproduced in Firmware vs Device Drivers: Explained with Examples! Firmware vs. Device Drivers Firmware Device Drivers Firmware is a class of software that is written for specific custom hardware. Device drivers are software that is needed to make a given hardware accessory Firmware lives and runs directly on the hardware. Device drivers live on hard-disk and run on the CPU. Firmware is independent of an operating system, i.e., you can run any operating system on top of a given motherboard’s firmware. Device drivers are highly dependent on the operating system on which they are used. For example, for the same hardware device, you need different device drivers for using that on Windows vs Linux. Firmware cannot be updated through an operating system, we need to go into the BIOS&#x2F;UEFI to update the device drivers. Drivers can be updated from within the operating system. Firmware engineers do not need any knowledge of operating systems. But they need core knowledge on processors and the latest RAMs, PCIe standards, and so on to write firmware that complies with the latest standards Device driver engineers need knowledge about the specific device that they are using, the communication standard the device uses to talk to the computer (like Bluetooth, USB, etc), and the operating system the device driver is written for. Firmware is written by motherboard manufacturers Drivers are written by engineers in companies that produce hardware accessories that connect to your computer Examples include the BIOS&#x2F;UEFI interface that comes with the computer’s motherboard Examples include special software you install to handle the extra buttons on your mouse, software that comes with any non-standard hardware like special game controllers, also the software that helps us use all the standard hardware like USB storage devices, keyboards, mice, headphones, etc. Layers of software on a typical computer are shown in the following figure: Firmware​ Firmware is a computer program that is written to work directly on specific custom hardware and it lives in non-volatile memory such as a flash chip and it is executed directly from it. The job of the firmware is to make the hardware accessible to the operating system. Firmware can be thought of as the glasses through which the operating system can see the actual hardware! ​ Originally Firmware is written on Masked ROMs, which is a special type of memory that can be programmed&#x2F;written-data-to only once. The products were then shipped with these unchangeable programs called firmware and they run for ages till the device goes out of use. ​ The first replacement of Masked ROMs came in the form of EPROM which can be erased by exposure to UV light and then reprogrammed as required. Then came EEPROMs which used electricity to change the contents. Nowadays the Masked ROMs have been replaced with Flash memory, which is cheaper and serves the purpose. Device DriversDevice drivers are programs that can control a given hardware and provide a software interface to it. Other programs like Operating Systems can interact with the hardware through this software interface without needing to know the actual underlying implementation of the software interface. The relation graph between firmware and drivers is also shown below: ​ Generally speaking, drivers and firmware together form the module that operates hardware. But why not make the firmware perfect so that it doesn’t require driver support? ​ The answer to the above question is, there are different operating systems which have completely different ways of operating hardware. So, on the one hand, hardware manufactures need to write firmware to make their hardware easier to use with software, but on the other hand, they cannot make the firmware too rigid in order to be compatible with various operating systems. They must leave enough room for software to freely operate —— and that’s where drivers come in.","tags":[{"name":"firmware","slug":"firmware","permalink":"https://stu-yue.github.io/tags/firmware/"},{"name":"driver","slug":"driver","permalink":"https://stu-yue.github.io/tags/driver/"}],"categories":[{"name":"architecture","slug":"architecture","permalink":"https://stu-yue.github.io/categories/architecture/"}]}],"categories":[{"name":"mathematics","slug":"mathematics","permalink":"https://stu-yue.github.io/categories/mathematics/"},{"name":"statistics_ml","slug":"statistics-ml","permalink":"https://stu-yue.github.io/categories/statistics-ml/"},{"name":"architecture","slug":"architecture","permalink":"https://stu-yue.github.io/categories/architecture/"},{"name":"languages","slug":"languages","permalink":"https://stu-yue.github.io/categories/languages/"},{"name":"shell","slug":"languages/shell","permalink":"https://stu-yue.github.io/categories/languages/shell/"},{"name":"tidbits","slug":"tidbits","permalink":"https://stu-yue.github.io/categories/tidbits/"},{"name":"networks","slug":"networks","permalink":"https://stu-yue.github.io/categories/networks/"},{"name":"python","slug":"languages/python","permalink":"https://stu-yue.github.io/categories/languages/python/"}],"tags":[{"name":"math","slug":"math","permalink":"https://stu-yue.github.io/tags/math/"},{"name":"ml","slug":"ml","permalink":"https://stu-yue.github.io/tags/ml/"},{"name":"instruction set arch","slug":"instruction-set-arch","permalink":"https://stu-yue.github.io/tags/instruction-set-arch/"},{"name":"shell","slug":"shell","permalink":"https://stu-yue.github.io/tags/shell/"},{"name":"linux","slug":"linux","permalink":"https://stu-yue.github.io/tags/linux/"},{"name":"networks","slug":"networks","permalink":"https://stu-yue.github.io/tags/networks/"},{"name":"algebra","slug":"algebra","permalink":"https://stu-yue.github.io/tags/algebra/"},{"name":"statistic","slug":"statistic","permalink":"https://stu-yue.github.io/tags/statistic/"},{"name":"python","slug":"python","permalink":"https://stu-yue.github.io/tags/python/"},{"name":"optimization","slug":"optimization","permalink":"https://stu-yue.github.io/tags/optimization/"},{"name":"points","slug":"points","permalink":"https://stu-yue.github.io/tags/points/"},{"name":"memory","slug":"memory","permalink":"https://stu-yue.github.io/tags/memory/"},{"name":"firmware","slug":"firmware","permalink":"https://stu-yue.github.io/tags/firmware/"},{"name":"driver","slug":"driver","permalink":"https://stu-yue.github.io/tags/driver/"}]}